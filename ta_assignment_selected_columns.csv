Question 2 Response,Question 3 Response,Question 4 Response,Question 5 Response,Question 6 Response,Question 7 Response,Question 8 Response,Question 9 Response,Question 10 Response,TA
Zhengrui Wang | wzray@seas.upenn.edu,Jiaxi Li | recardo@seas.upenn.edu,,"Jiaxi Li (ID: 81263908)
Responsibilities:
o Data Cleaning
o Feature Engineering
o Principal Component Analysis (PCA)
o Model Training (SVD, K-NN)

Zhengrui Wang (ID: 68798785)
Responsibilities:
o Cross-Validation
o Build a hybrid model by Integrating content-based and collaborative algorithms
o Backtesting
o Simulation + Hypothesis Testing
o Analysis",https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset,45000 rows,"Our project aims to develop a movie recommendation system that suggests films tailored to users' tastes. By analyzing each user's favorite movies and past ratings, we’ll predict which titles they’re likely to enjoy, enhancing their overall experience.

This project is especially relevant because recommendation systems are essential on movie platforms and have real-world applications that boost business efficiency. Gaining insight into how these systems work and their effectiveness reveals valuable connections between user behavior and personalized recommendations.","For our recommendation system, we’re using a hybrid model that combines Singular Value Decomposition (SVD) and k-Nearest Neighbors (k-NN). SVD will capture latent user-item factors, while k-NN will handle similarity-based recommendations. Unlike classification or regression, our model ranks items by predicting preference scores for each user-item pair.

To validate accuracy, we’ll conduct hypothesis testing. Our null hypothesis (H0) states that predicted ratings align with actual ratings, while the alternative (H1) suggests otherwise. Using Root Mean Squared Error (RMSE) as our metric, we’ll bootstrap test samples, build confidence intervals for RMSE, and calculate p-values to assess significance.","Feature Engineering: Choosing relevant features while avoiding multicollinearity is essential.

Model Performance: Precision and recall balance may be challenging; alternatives may be needed if K-NN underperforms.

Overfitting: PCA will reduce dimensionality, but cross-validation and backtesting are crucial to prevent overfitting.

Complexity vs. Efficiency: Combining SVD and k-NN could raise computational complexity, especially on large datasets.

Hyperparameter Tuning: Optimizing SVD and k-NN parameters requires extensive testing.

Future Improvements: Access to post-deployment user data can provide insights for model refinement.",Emily Liu
Muhammad Abdullah Goher|mgoher@seas.upenn.edu,Etienne Li|etienne9@sas.upenn.edu,Baron Ping Ye-Hseih|bpyhsieh@seas.upenn.edu,"Baron - Data Cleaning
Abdullah - Model Training
Etienne - Explatory analysis","1. Steam Games Dataset
https://www.kaggle.com/datasets/fronkongames/steam-games-dataset/data
2. Steam Reviews Dataset
https://www.kaggle.com/datasets/andrewmvd/steam-reviews
3. Steam time series / popularity over time dataset
https://www.kaggle.com/code/vinskatalita/basic-time-series-analysis-steam-monthly-players", Around 97k rows 39 Columns,"Our project aims to analyze what factors contribute to video game success on Steam, the world's largest digital game distribution platform. Using a comprehensive dataset of over 97,000 games with 39 different features, we will investigate how various factors such as pricing, platform compatibility, genres, and player engagement features (achievements, DLC) influence game success.
As gamers who have spent a lot of time playing games on Steam, we're personally invested in understanding what makes certain games thrive while others struggle. Our experience with both indie titles and AAA releases drives our curiosity about the patterns behind their success.
As the gaming industry continues to grow ($184.4B revenue in 2022), understanding these success factors becomes increasingly valuable for developers and publishers. Our analysis will focus on identifying the most significant predictors of game success through metrics such as user reviews, player counts, and metacritic scores, providing actionable insights for game development strategies.","We're exploring both classification and regression approaches to understand game success. For success metrics, we could use user review ratios, metacritic scores, player count thresholds, or even a combination of these factors.
For modeling, we're considering logistic regression or random forest to classify games as ""successful"" based on review thresholds, while MLR or XGBoost could help predict numerical metrics like metacritic scores.
Some potential hypotheses we're interested in testing:
      Do games with more achievements/DLC tend to have better reviews?
      Do games with multiplayer features perform better?
      Do certain genre combinations perform significantly better?
We're still exploring these ideas and are open to different approaches as we dig deeper into the data.
","We anticipate several challenges. Just to list a few: missing entries across our dataset's 39 columns, unreliable reviews and potential review bombing, statistical uncertainty for games with few users, class imbalance between successful and unsuccessful games, and complexity in merging our games and reviews datasets. We will also need to work out how to use our time series dataset effectively.",Emily Liu
Jiayi Chen | jiayiccc@seas.upenn.edu,Xiangwei Zheng | xwz@seas.upenn.edu,Kate Cai | jiaqicai@seas.upenn.edu,"Our team will collaborate on EDA to understand the dataset’s structure and distribution. Each member will then focus on a specific model based on their assigned problem: Kate on predicting total spending, Jiayi on predicting purchase frequency, and Xiangwei on classifying spending levels. We will individually handle data preprocessing, feature engineering, model assessment, and hyperparameter tuning for our respective models, while each contributes to the documentation and presentation. ",https://www.kaggle.com/datasets/bhavikjikadara/retail-transactional-dataset,"302,010","Objective: Our project focuses on analyzing and predicting customer spending behavior in a retail setting. We want to understand how factors like age, income, and gender, along with transaction details, affect how much customers spend and how often they make purchases. By studying these patterns, we aim to identify high-spending customer profiles, determine which factors drive frequent purchases, and classify customers based on their spending levels.
Value Proposition: The main goal of this project is to create predictive models that can help retailers better understand and target their customers. With these insights, businesses can personalize marketing efforts, improve customer satisfaction, and make data-driven decisions to boost revenue. This project is interesting because it not only explores real-world customer behavior but also provides a practical application of machine learning and data analysis techniques, making it valuable for both business strategy and hands-on learning.","We might approach this dataset with three potential problems. First, for predicting total spending, we target Total_Amount to identify high-spending customers, using Gradient Boosting and Random Forest Regressors to capture complex patterns and feature importance. We hypothesize that spending differs across customer segments, testing if average spending is consistent with an F-test and simulation approach by shuffling Total_Amount across segments to observe significant differences.
Second, to predict purchase frequency, we assess Total_Purchases, using Linear Regression for interpretability and Support Vector Regression for complex relationships. We test if age affects purchase frequency in the US by shuffling Total_Purchases and examining linear regression coefficients.
Lastly, for classifying spending levels, we focus on Customer_Segment using Logistic Regression and KNN to identify clusters for targeted marketing. We hypothesize spending differences by gender, using a t-test and simulation by shuffling gender labels and comparing t-statistics.","In our analysis, we may encounter several challenges. Data quality issues, such as duplications, missing values, and inconsistencies in customer demographics or transaction details, could impact model accuracy and require thorough cleaning. Class imbalance may also pose a problem if categories like high-spending customers are underrepresented, in which case resampling and class weighting will be necessary for balanced performance. Additionally, complex models like Random Forest and Gradient Boosting may risk overfitting, which we’ll address with cross-validation and regularization strategies. Finally, feature scaling will be essential for models like KNN and SVR, which are sensitive to distance metrics, ensuring all features contribute appropriately.",Emily Liu
"Leo Zhou | leozhou@sas.upenn.edu
","Vanshika Sriram | vansri@seas.upenn.edu
",Kevin Yang-Li | ylkevin@sas.upenn.edu,"Kevin: Perform Hypothesis Testing on whether or not a remote job offering would get more views and applications than an onsite job counterpart and write the structure of the Hypothesis Testing in the Google Collab Notebook. 
Vanshika: Predict whether a job will be remote or in person based on factors like industry, location, company size.
Leo: Predict the number of views for each job posting based on factors like job title, industry, location, company size, remote work.
",https://www.kaggle.com/datasets/arshkon/linkedin-job-postings?select=postings.csv,122124,"Our ultimate objective with this project is to understand what factors influence people to apply to certain jobs, such as remote work, salary data, and the industry itself. We also want to understand what factors affect the salary offered by these jobs. 

It is well known that many industries are experiencing hiring freezes or are generally hiring less and offering lower salaries. This project is interesting as it would be valuable to analyze how, given the state of the job market, people are approaching job applications and if companies are in fact offering lower salaries compared to industry standards. 
","1. Factors that Affect Job Posting Views: Target variable - number of views
Model - regression
Predict the number of views for each job posting based on factors like job title, industry, location, company size, remote work
2. Remote work and applications/views: Target variable - number of applications or views
We are planning on doing hypothesis testing to answer this question
3. Factors that affect remote work offering: Target variable - application rate (ratio of application to views)
Model - classification
Predict whether a job will be remote or in person based on factors like industry, location, company size

We will perform hypothesis testing to determine whether or not a remote job offering would get more views and applications than an onsite job counterpart.","-Foresight in terms of logistics: hard to predict what will happen, especially now.
-planning, and execution: follows up on foresight but executing what needs to be done will be difficult if foresight on the project is not great. 
-possibly dividing up tasks: splitting the work evenly amongst us three may pose a challenge since the tasks could be of varying difficulty/length.",Emily Liu
"Zihua Meng | zihua@seas.upenn.edu
",Jesse Ying | jkying@seas.upenn.edu,Yuanming Shao | shaoym@seas.upenn.edu,"Member 1: Zihua Meng
Duties: Model Development and Evaluation
Member 2: Jesse Ying
Duties: Data Preprocessing and Analysis
Member 3: Yuanming Shao
Duties:  Feature Engineering and Visualization",https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents,"(7728394, 46)","The primary objective of this project is to predict the severity of traffic accidents and identify high-risk areas across the United States. Using machine learning algorithms on the US Accidents dataset, we aim to develop predictive models that can assess the likelihood of severe accidents based on factors such as location, time, weather conditions, and traffic control measures. Additionally, we want to perform geospatial analysis to pinpoint regions with a high concentration of severe accidents, thereby informing targeted interventions for traffic management and urban planning.

This project is particularly interesting because it combines multiple aspects of real-world data (weather, traffic, and infrastructure) to address a critical public safety issue. By understanding and predicting accident severity, we can contribute to making roads safer and potentially saving lives through better preparation and prevention measures.","We are utilizing classification models to predict traffic accident severity, with the target variable being the severity level (ranging from 1 for minor to 4 for fatal accidents). Models such as Logistic Regression, Random Forest, and Gradient Boosting (XGBoost, LightGBM) will be employed for this purpose. Additionally, clustering techniques like K-Means and DBSCAN will be used to identify high-risk accident hotspots. We plan to test two hypotheses: (1) adverse weather conditions are associated with higher accident severity, and (2) the time of day affects both the frequency and severity of accidents. These hypotheses will be evaluated using statistical tests including Chi-Square, ANOVA, t-Tests, and Regression Analysis.","We anticipate significant challenges, including handling extensive missing data in weather and location features, transforming diverse boolean and temporal variables into meaningful predictors, and managing complex geospatial data. Additionally, ensuring computational efficiency while creating accurate spatial features and addressing inconsistencies across numerous unique locations and timezones will be critical.",Emily Liu
"Dhruv Verma | vdhruv@seas.upenn.edu
","Kruthi Muralidhara Jety | kruthimj@seas.upenn.edu
",Vismay Churiwala | vismay@seas.upenn.edu,"Vismay Churiwala (EDA and Data Pre-processing)
○ Conduct exploratory data analysis (EDA) to analyze the dataset’s structure and
distribution of key features, visualize correlations
Dhruv Verma (Modeling and Feature Engineering)
○ Implement baseline and advanced classification models and perform feature engineering.
Kruthi Muralidhara Jety (Evaluation, Hyperparameter Tuning, and Reporting)
○ Evaluate model performance using metrics like accuracy and conduct hyperparameter tuning to optimize models.",https://www.kaggle.com/datasets/yasserh/loan-default-dataset/data,"(126369, 34)","●  Objective: Develop a robust predictive model to classify new borrowers as likely to default or not based on their demographic and financial characteristics. The model should not discriminate against people based on demographic factors like age.
●  Value Proposition: This model will support financial institutions in improving risk assessment by identifying borrowers at risk of default, thereby enhancing loan approval efficiency and reducing default rates. Also, we wish to prevent banks from discriminating against providing loans to people based on demographic factors like age, thereby promoting ethical and fair behavior among banks.","Modeling Plan
● Models: We will use classification models, starting with:
○ Baseline model: Logistic Regression.
○ Advanced models: Random Forest and Gradient Boosting for potentially stronger
predictive performance.
● Target Variable: Status, where 1 represents default and 0 represents non-default.

Hypothesis Testing
       
● Null Hypothesis (H0): There is no significant relationship between age and Status (default or non-default).
● Alternative Hypothesis (H1): There is a significant relationship between age and Status.
● Significance Level: 0.01.
● Simulation Use: We will use resampling techniques like bootstrap analysis to ensure model
stability and performance consistency.","Class Imbalance: The dataset has an imbalanced distribution between default and non-default cases, which could impact model accuracy. To address this, we may employ techniques like SMOTE. 
Multicollinearity Management: Due to multicollinearity among features, especially among numerical ones, feature engineering and selection will be critical to avoid redundant data. 
Hyperparameter Tuning Constraints: With multiple models, tuning can be time-intensive. 
",Emily Liu
Aria Xingni Shi  | xish@seas.upenn.edu,Yue Wang | wayue@seas.upenn.edu,Raafae Zaki |rzaki2@seas.upenn.edu,"Yue’s Duties: Data Pre-processing and EDA
Raafae’s Duties: Modeling
Aria’s Duties: Hypothesis Testing",https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data,"500,000","We would like to study how various factors (weather, location, time of day, etc.) influence the severity of car accidents in the US, with the ultimate objective being predicting the severity of future accidents based on those factors. This project is interesting because traffic accidents are a significant public safety issue, leading to great loss of life, injuries, and economic costs in society. Understanding accident patterns can provide valuable insights for city planners, transportation agencies, and policymakers in designing better systems and roadways for traffic safety.","We are primarily considering classification models. This is because our target variable is accident severity, which is a discrete score from 1-4. For the exact models, we are planning to use random forest and XGBoost to compare the performance between the models after hyperparameter tuning.
Null Hypothesis : The time of day (e.g., morning, afternoon, evening, midnight) does not affect the severity of traffic accidents.
Test statistic: Difference between mean of severity during [morning/afternoon/evening/midnight] accident and mean of severity another part of the day.
Topics: Random sampling, permutation testing, and simulations.","Feature Selection: Given the large dataset, complex features, and possible covariates, it's important to manage multicollinearity (when features are highly correlated) as it can negatively affect model performance. 
Imbalanced Data: In classification model, imbalanced data can lead to biased predictions favoring the majority class.
Hyperparameter Tuning: Tuning the parameters of different models can significantly improve their performance.",Peter Akioyamen
Brian Ling | bling212@seas.upenn.edu,Tiffany Chang | tchang1@sas.upenn.edu,Daniel Zhao | dyzhao@seas.upenn.edu,"Tiffany: Data cleaning, Modeling
Brian: Data cleaning, Exploratory Data Analysis
Danny: Exploratory Data Analysis, Modeling","https://www.kaggle.com/competitions/nfl-big-data-bowl-2025/data. The NFL Big Data Bowl 2025 dataset on Kaggle shows football tracking data for a variety of teams, including all games throughout the season. It also includes player movement data, game context, and play outcomes for every NFL play. The dataset also includes information on the position, velocity, and acceleration metrics of each player, along with background info like game situation and play outcomes. Data Types range from numeric, text, date, and formatted data. ","8.17 GB; player_plays: 50 cols, 354727 rows; plays: 50 cols, 16124 rows; plays: 7 cols, 1697 rows","We intend to predict NFL plays, positioning, and wins based on the NFL dataset. The dataset provides us with player information, team positions, tendencies, records, clock time, etc. The objective of our project is to determine the most optimal play for each team to make in the situation based on all of these variables. This is important because it could be useful for NFL teams to use our analysis to better their practice, strategy, and overall improve their winning chances in the long run. ","We are going to be using classification models primarily because there are a finite amount of defensive schemes a team can run. Suppose there are n defensive schemes, we can one-hot encode this to an 1 by n vector and use it as our dependent variable to classify our data. Our target variable is which defensive scheme to use. We can achieve this by creating a training and testing set of only successful plays that occur (based on the number of yards gained), and seeing if our model correctly predicts the play. We will likely use a variety of models, including random forest and multivariate regression. Yes, we are considering doing hypothesis testing in order to evaluate the efficacy of our models and predictions. We can check if our model predicts the correct play more than the normal amount. Our null hypothesis is that our model is not better than a random prediction at predicting the correct play. Our test statistic is the percentage success rate of our model in predicting the right play compared to the best play. ","Our biggest obstacle is going to be cleaning the dataset, simply because of how much data is present. We also have to clean the dataset to include only correct plays and filter out unneeded traits. Additionally, we will have to encode multiple categorical variables, all of which will take an extensive amount of effort and time. Another obstacle that we might anticipate is that we don’t actually know what the best “play” is at any time. We do have access to successful plays, but we don’t know whether that play is necessarily the best play. We can circumvent this by evaluating how good a play is based on how many yards were gained by the team.",Peter Akioyamen
Monisha Krothapalli | monishak@seas.upenn.edu ,Mallika Kulkarni | mallikam@seas.upenn.edu,Arushi Agarwal | arushiga@seas.upenn.edu,"Mallika Kulkarni - Data cleansing and EDA 
Monisha Krothapalli - Model training and testing
Arushi Agarwal - Hypothesis testing and write up (description of challenges and next steps)","https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products?select=amazon_products.csv
This dataset has around 1.4 million rows with 11 columns
https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews 
This is a smaller dataset, with 500k rows and 10 columns, that will used in supplement to the first one
",1.4 million rows,The objective is to create a comprehensive model that will predict product success and the features that contribute to these successes at Amazon. Figuring out what features lead to success at one company could carry forward to any corporation. We could figure out the features that drive the most reviews as well as features that analyze price-performance relationships. The project is interesting because it could lead to key insights to be used by investors and business owners that want to take after Amazon’s strategies and existing consumer behavior.,"We would use a classification model in which the prediction target is binary (whether the product will be a bestseller or not) based on different markers such as price, list price, discount price, category id, and ratings. 
We can use Random Forest, which is an ensemble of decision trees, and would be optimal for classification. 
Some metrics to track are: recall, precision, accuracy, and feature importance. 
We are considering doing hypothesis testing, but there are multiple ideas for our null hypothesis. If we decide to analyze the price-bestseller relationship, the null hypothesis would be that price had no significant correlation with the product being a best seller. In this case we could do permutation testing with simulated price distributions. A possible test statistic could be the Pearson correlation coefficient.
This method would apply if we found the data to be normally distributed:
Another possible test would be to analyze category performance among best sellers. The null hypothesis would be that product ratings are distributed similarly across all categories. Here we could do analysis of variance (ANOVA). Bootstrap sampling could work here to compare category means.","In merging the data sets and cleaning our data, we expect to face difficulty dealing with the nonuniform list of products mentioned in each data set. Neither data set includes a comprehensive list of all the products on amazon, so we expect our merged data set to decrease significantly in size as we would only be able to use those products that are listed on the first data set about whom reviews have been written. Therefore, it may also be difficult for us to draw conclusions with a limited number of data points for products in each specific category, and it may require us to find additional data to merge into our dataset. ",Peter Akioyamen
Jack Bader | jbader14@seas.upenn.edu, Juliana Lu | ljuliana@seas.upenn.edu,Oscar Wan | oscarwzt@seas.upenn.edu,Jack Bader will handle exploratory data analysis (EDA) and be responsible for running and tuning Random Forest models. Juliana Lu will focus on data cleaning to prepare the dataset and will also manage the running and tuning of logistic regression models. Oscar Wan will conduct EDA alongside Jack and run/tune Gradient Boosting Trees for optimal performance. We will all work together to do feature engineering.,https://www.kaggle.com/datasets/oscarm524/fraud-detection-in-grocery-shopping-transactions?select=DMC-2019-realclass.csv,500000,"We would like to identify key features that would assist in predicting whether or not a self-checkout transaction is fraudulent. Our objective is to build a classification model to predict whether a transaction is fraudulent. We will train several classification models and evaluate their performance against one another. This project is interesting because through building a predictive model, we can analyze the model's parameters to see what factors contribute to its decisions.  As more and more stores are transitioning to self-checkout options, it is interesting to learn what factors can predict fraud.","We are considering classification models as we want to classify each transaction as fraud or non-fraud. The target variable is fraud or not. We will experiment with a few different models, logistics regression, SVM, random forest, xgboost. If two models have similar performance we will go with the simpler model as it's more interpretable. We could do some hypothesis testing. An interesting hypothesis is that fraudulent transactions take more time per item on average. ","The biggest challenge we anticipate is class imbalance. Only 4.7% of the transactions are labeled as fraud. To address this issue, we need to carefully consider how we split training and testing data, and what evaluation metrics to use. AUC might be a good choice as it balances both true positive and true negative.",Peter Akioyamen
Nihar Ballamudi | niharb@sas.upenn.edu,Selina Zou | slinazou@sas.upenn.edu,Ming Qi | mingq@seas.upenn.edu,"Nihar: Clean data and perform EDA
Selina: Design and train model
Ming: Integrating weather data with flight data

All tasks will be highly collaborative in nature in which each member will help with every task, however, the members listed will be in charge of ensuring that their task is completed.","Delays: https://www.kaggle.com/datasets/usdot/flight-delays?select=flights.csv
Weather Dataset: https://asmith.ucdavis.edu/data/prism-weather","Delays: (5819079, 31), Weather: (2,448,850, 9)","Understanding when and where flights will be canceled is extremely useful for airlines to maximize the number of passengers to their destinations. However, due to weather and other factors airlines will have to delay flights on occasion, for the safety of the passengers. Understanding when some flights will have to be delayed beforehand is thus a highly useful tool for airlines to use. Our objective is to create a model that when given a flight with an origin and destination will understand whether or not the flight will be delayed and how long it will be delayed.","We want to test the hypothesis that as precipitation increases so does length of delay. Consider the null hypothesis H0: rate of cancellation on rainy days = rate of cancellation of non-rainy days, where we define rainy and non-rainy with some binary threshold that we can choose based on a decision tree. Our target variable is then number of delay minutes. Since this is a continuous value, we are considering more regression instead of classification.",The data integration will not be straightforward. We cannot just simply do a join as when integrating data there is a lot of domain knowledge regarding flights and airports that is necessary. Model training and testing will likely be extremely time-consuming due to the size of our dataset.,Peter Akioyamen
Kurtis Zhang | kurtisz@seas.upenn.edu,Chuhua Yang | chukyang@sas.upenn.edu,Brandon Yan | branyan@seas.upenn.edu,"Kurtis - Data filtering and cleaning. Hypothesis testing/simulation. Figuring out questions to answer other than our null hypothesis for interdependencies between features.

Alex - Hypothesis Testing, smaller-scale modeling on select features.

Brandon - Multiple Linear Regression and other all-encompassing models, with aim of predicting Popularity Score.
","Our data source is a dataset taken from the website Kaggle, and is data about Spotify tracks. The numbers and subjective evaluations are calculated by Spotify and provided via their API. Without preprocessing, the dataset contains almost 90,000 rows and 21 columns. As our preliminary inspection shows no NULL values, we expect that our usable data, post-filtering, will be well above 50,000 rows and 7-10 columns of features.
https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/data

Our data is structured, with labeled columns, that are either categorical or numerical.

The following link contains the author’s explanation of how he obtained the data (accessing Spotify’s API with Python scripts). It also contains endorsements from other users about the authenticity and reliability of the data.
https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/discussion/374642

The following link is Spotify’s official documentation, which confirms the claims that the data source is public and was accessed properly.
https://developer.spotify.com/documentation/web-api

",~90000,"Using this dataset we intend to study what makes a song popular. Spotify’s API has its own custom measurements of various characteristics of a song. For example: Danceability, Energy, Speechiness, Acousticness, etc. There are also objective numerical measurements like Loudness, Tempo, Key, Time Signature, etc. Using a combination of these characteristics, we want to find trends and patterns across popular songs. We want to identify the most important aspects in making a popular song.

Through EDA, we can also convey insights about specific subcategories of music. For instance, we can analyze the performance, as well as various tendencies, of specific artists or genres. We can rank order musicians, collaborations, groups, and albums based on various metrics.

This project is quite interesting because it can help musicians see if the hit songs are similar and if they need to change up their songs. We can aim to predict, pre-release, a song’s potential performance on Spotify, based on its score in the categories. If this is possible to do with good accuracy, it would bring huge financial benefits to music producers, significantly improving their decision making.
","Our main approach is using a Multiple Linear Regression model to predict our target variable: the Popularity Score of a song. We want to see how the Popularity Score changes when each characteristic changes. Our target variable would be the popularity rating of the song. We will also explore various other architectures like Decision Trees and Neural Networks, with the same target variable, Popularity Score. We can do a hyperparameter search to find the model best suited for our predictions.

We can do extensive hypothesis testing by running a multiple linear regression of the various metrics of each track against the popularity of the track, then residual testing each feature. 

For permutation testing and residual testing of our MLR models, our test statistic will be the r-squared value. For permutation testing, our null hypothesis is that our MLR model has no ability to explain the variation in Popularity Score. For residual testing, we will do tests on each individual input feature in our MLR, and our null hypothesis will be that the feature being tested does not help better determine a song’s Popularity Scores.

We can also do simulation-based hypothesis testing to answer smaller questions about interdependencies between features such as “Does [song characteristic x] affect [song characteristic y]?”.

For a question such as “Does Time Signature affect danceability?”, our test statistic will be the difference in average danceability score between songs of different Time Signatures. ","Some anticipated obstacles for this project may be considering whether we should use every characteristic meaning both subjective/objective or just the purely objective ones. It is possible that Spotify’s subjective calculations from their API are not quite accurate and can be considered different by different point of views. Thus, we may consider running regressions on two sets of independent variables. Another obstacle we may run into is working with non-English data because there are non-English song titles, album titles, and artist names. It may be hard to group and access these data rows while calculating relevant numbers.
",Kyle Liao
Jana Hayaly | jhayaly@sas.upenn.edu,Joe MacDougall | joemacd@seas.upenn.edu,Grace Li | gracel98@sas.upenn.edu,"Joe MacDougall:
- Taking the lead on pre-processing and cleaning the dataset (e.g. linking userIDs for hypothesis testing).
- Using regression models to inform sentiment over time.
- Sentiment analysis and keyword algorithm implementation.
Jana Hayaly:
- Ensuring that hypothesis testing is thorough and has internal validity.
- Doing the majority of the exploratory data analysis.
- Using classification to sort negative and positive reviews.
Grace Li: 
- Leading the design and implementation of sentiment analysis techniques.
- Supporting regression modeling to examine trends in review sentiment over time.
- Collaborating on data cleaning and pre-processing efforts for consistency.
- Ensuring effective classification of diverse sentiments in the analysis.
",https://amazon-reviews-2023.github.io/,571.54M,"This project focuses on sentiment analysis with the aim of exploring how language in product reviews correlates with positive or negative emotions and how this relationship evolves over time. The goal is to use reviews to assess what language is associated with positive/negative emotions and how that vocabulary has changed over time. It speaks to general language patterns across generations: for example, the level of negativity/positivity associated with certain ratings. Moreover, it is interesting to observe how language usage varies across categories of retail, as well as general sentiment towards those products. This project was particularly interesting to us since we wanted to build off of the technique of using time as a feature. Additionally, we both have an interest in sentiment analysis and language processing, and we would like to tie that in with hypothesis testing techniques learned in this course. Ultimately, we would like to be able to make generalizations about the patterns in review crafting as influenced by industry, time, and a multitude of other factors.","We intend to use both types of modeling techniques. For classification, we are interested in developing a technique for classifying positive vs negative reviews, based on the language used in the review itself. This will help inform some of our analysis on the relative positivity of reviews across categories and trends in that overtime. On top of analyzing positivity and negativity, we plan to explore a broader range of sentiments, including emotions such as joy, anger, fear, surprise, and trust. By examining these varied emotional tones, we can gain a more nuanced understanding of the language used in reviews and how these sentiments influence perceptions across different product categories. In a related sense, we intend to use regression to model the relationship between time (by years) and review sentiment, as well as customer satisfaction in general.
We would like to test a variety of null hypotheses. Our general prediction is that customer satisfaction with retail products has declined overtime, which will be tested based on multiple null hypotheses. A couple of examples include:
- There is no significant relationship between year posted and negativity of a review.
- Purchasing patterns across industries has not changed significantly over time
- There is no significant relationship between time and average length of reviews.
- There is no significant relationship between time and number of reviews per unique user.","We anticipate that implementing effective sentiment analysis may prove difficult, as it requires selecting or developing models capable of accurately discerning the nuanced emotional tones in reviews. Though we plan to look for resources or base models that can help with our implementation. Another obstacle we may face is data consistency- there is no guarantee that we have the same amount of reviews between years or the same volume of reviews across industries. Also, the dataset size is extremely large- we would have to make choices on how to clean the data and potentially use faster frameworks, like polars, for processing. ",Kyle Liao
Darsh Khandelwal | darshk@seas.upenn.edu,Nehal Doiphode | lahen@seas.upenn.edu,Leon Kabue | kabue@seas.upenn.edu,"Darsh Khandelwal:
1) Data Preprocessing and Cleaning: Responsible for data preprocessing, including handling missing values, standardizing data, and conducting exploratory data analysis (EDA).
2) Feature Engineering: Creating relevant features based on patient demographics and medical history to aid in accurate prediction modeling.

Leon Kabue:
1) Model Development: Leading the development of machine learning models to predict health outcomes (e.g., heart disease risk, COVID-19 positivity) based on lifestyle, demographic, and medical history data.
2) Hyperparameter Tuning: Responsible for tuning models and optimizing parameters to improve model accuracy and performance.

Nehal Doiphode:
1) Statistical Analysis and Hypothesis Testing: Conducting hypothesis testing to examine correlations and causal relationships, e.g., between lifestyle factors (smoking, alcohol consumption) and risk of heart disease.
2) Evaluation and Visualization: Creating metrics to evaluate model performance and designing visualizations to display findings.
","https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease
(heart_2022_with_nans.csv)","400,000 Rows and 40 Columns","The primary objective of this project is to develop a predictive model that identifies patients at high risk for specific health outcomes, with a particular focus on predicting:

Heart Disease Risk: Achieving a prediction accuracy of at least 85% for the binary outcome of whether a patient has experienced a heart attack (HadHeartAttack).

Diabetes Prediction: Predicting diabetes occurrence with a target recall rate of at least 90% to ensure we capture as many actual cases as possible.

COVID-19 Positivity: Classifying COVID-19 positivity (CovidPos) with an F1 score of at least 0.8.

To accomplish these goals, we will quantify the influence of lifestyle and demographic factors (such as smoking status, BMI, and age category) on each outcome. 

This project is interesting because we would be able to provide healthcare professionals with reliable models that support early intervention and resource allocation for high-risk individuals, potentially enhancing patient outcomes and reducing healthcare costs.
","Target Variables:
As stated above, our primary targets are binary outcomes for:

Heart Disease (HadHeartAttack): Aiming for a prediction accuracy of at least 85%.

Diabetes (HadDiabetes): Focusing on a recall rate of 90% to maximize true positive rate.

COVID-19 Positivity (CovidPos): Targeting an F1 score of 0.8 to balance precision and recall.


Model Types:
We will use classification models to predict the likelihood of each condition, experimenting with:
Logistic Regression as a baseline for comparison across all models.

Decision Trees and Random Forests to handle high-dimensional, categorical data and for feature importance insights.

Support Vector Machines (SVM) and Gradient Boosting for more refined pattern detection, optimizing for precision and recall on specific outcomes.

Neural Networks to capture complex, non-linear relationships if they significantly improve metrics for target variables.
","Key issues include class imbalance, where fewer positive cases may bias models. Another issues may involve high dimensionality and multicollinearity, which can obscure insights due to redundant or correlated features. Further, computational complexity from large data size and complex models may slow processing and strain resources without optimization.






",Kyle Liao
Angela Nguyen | angelang@sas.upenn.edu,Maggie Schwierking | mschwier@wharton.upenn.edu,Grace Dai | daigrace@sas.upenn.edu,"Angela Nguyen - data processing/cleaning, hypothesis testing
Maggie Schwierking - model analysis
Grace Dai - hypothesis testing
","Data Source: https://openpolicing.stanford.edu/data/ 
We will be pulling the Open Policing Data for Philadelphia. The dataset includes 1,865,096 rows of data (stops that were counted) with a date range of December 2013 through April 2018. Additionally, it includes 10 columns, including stop date, stop time, stop location, driver race, driver sex, driver age, search conducted, contraband found, frisk performed, and arrest made. 
",1.9M,"Our ultimate objective is to understand whether race plays a role in how often people who get stopped by the police get searched and whether a frisk is performed. We are particularly interested in Philadelphia due to the city's relevance to ourselves, and the fact that Philadelphia’s police presence around our campus is strong. In recent years, police have been called out on social media and news for imposing their own racial biases on the frequency and nature of their arrests. This dataset will allow us to gain more insight into how these racial biases might be playing a role in car stops and searches. ","We will perform logistic regression over all the features to first look at the ones that have the most importance on search conducted and frisk performed. 
The logistic regression will allow us to perform classification predictions to see whether a search is conducted or not, as well as a classification of whether a frisk is performed or not. Our target variables are search conducted and frisk performed. In particular, we will be investigating the weight of the driver race. 
","Dataset has ~2M rows, so we expect that we may need to clean the data from irrelevant columns and rows with null values.
We also might need to choose a different city with less rows if the data takes too long to query.
If we analyze the dataset based on race, we need to find accurate data for the race distribution within the city we are looking at so that we can accurately make comparisons.",Kyle Liao
Liuyi Chen | chenliuy@seas.upenn.edu,Peiwen Hu | hup@seas.upenn.edu,Joyce Chen | chejoyce@seas.upenn.edu,"Joyce Chen will handle data preprocessing and visualization, ensuring data is cleaned and effectively represented for analysis. Liuyi Chen will focus on modeling and analysis, applying statistical techniques to derive insights. Sharon Hu will conduct hypothesis testing and compile the final report, summarizing findings and conclusions for the project.",https://www.kaggle.com/datasets/mexwell/employee-performance-and-productivity-data?select=Extended_Employee_Performance_and_Productivity_Data.csv,100000,"The primary objective of this project is to identify the key drivers of employee performance in a corporate environment, focusing on demographic factors, job characteristics, and work behaviors. By developing predictive models, we aim to quantify the impact of these factors on performance scores. This analysis will provide valuable insights for optimizing hiring criteria, refining training programs, and strategically aligning employee development initiatives. Ultimately, this project offers actionable intelligence to improve productivity and efficiency, empowering organizations to make data-driven decisions that enhance workforce effectiveness and drive organizational success.","We are considering regression models, as our target variable is the employee performance score, a continuous measure. Our modeling plan begins with a linear regression model as a baseline to establish benchmark accuracy and explainable variance (using MSE and R-squared). For advanced exploration, we plan to apply random forest and gradient boosting models, potentially adding neural networks if data complexity warrants it. Additionally, we’ll test two hypotheses: (1) Employee Satisfaction Score’s impact on Performance Score using Pearson correlation, and (2) the effect of Training Hours on Performance Score with a t-test on the regression coefficient. These analyses aim to reveal key performance drivers.","Key anticipated challenges include managing data quality issues, such as missing or inconsistent values, which may affect model reliability. Effective feature engineering will be crucial to capture meaningful predictors that improve accuracy. Additionally, we must balance model complexity and interpretability to avoid overfitting with larger datasets. Finally, we expect non-linear relationships and interactions between variables, which we plan to address using non-linear models like Random Forests or by incorporating interaction terms in regression models.",Kyle Liao
Alexander Chen | axc@seas.upenn.edu,Minghan Sun | sunmi@seas.upenn.edu,Advit Ranawade | advitr@seas.upenn.edu,"Minghan Sun: data processing, data interpretation
Advit: training and architecture
Alex Chen: hyperparameter tuning, hypothesis testing
",https://www.kaggle.com/datasets/arvindnagaonkar/flight-delay/data,500000,"We want to study the factors that affect whether a flight is delayed, including airport, airline, weather, time of year, etc. The ultimate objective is to build a model that can predict the delays of any given flight with a bunch of different parameters. Flight delays harm millions of travelers each year—they’re extremely frustrating. This project’s value proposition is that it can be used by both travelers and airlines to predict when flights will actually take off, and plan their schedules accordingly.
Our null hypotheses are that for each variable x in the dataset (e.g. weather, time of day), there is no relationship between that variable and the delay time. Our alternative hypotheses are that there does exist such relationships. We could do a t-test to see if the slope of the regression is significantly different from what we’d expect if the null hypothesis was true.
","The models we’re considering are linear regression or random forest, since the relationships should be pretty simple. We’d use a regression model since we want to predict time, which is a continuous variable. Random forests are known for classification but regression trees allow the model to be used for regression. Our target variable is the amount of time that a flight will be delayed.","We anticipate challenges both on the side of data quality and model training. On the data side, there could be various external variables such as political regulations or cascade effects affecting air traffic that may not be present in our dataset, making them difficult to account for in the model. Factors like airport location, weather, and season might also be correlated, leading to multicollinearity issues requiring insightful handling. On the model side, it might be difficult to select the right complexity for our model, since linear regression might be too simplistic for some relationships, while random forests might overfit due to capturing noise in the data. Additionally, ensuring that the model generalizes well across different airports and airlines may be difficult, as operational practices can vary widely.",Kyle Liao
Suha Memon | smemon@seas.upenn.edu,Taha Iqbal | tahai@seas.upenn.edu,Ishwari Mulay | ishwarim@seas.upenn.edu,"We plan to divide the project responsibilities across all members, allowing everyone to contribute to each key area and maximize learning.

Suha: Data preprocessing, Feature engineering, EDA, model implementation, visualization, model assessment.
Taha: Data preprocessing, Feature engineering, model implementation, hyper-parameter tuning, model assessment.
Ishwari: EDA, model implementation, hyper-parameter tuning, visualization, model assessment.",https://www.kaggle.com/datasets/joniarroba/noshowappointments/data,"110,527 rows, 14 columns","Our group intends to study how factors influence a patient’s likelihood of missing a scheduled medical appointment (resulting in a no-show). Our objective is to build a predictive model that identifies patients at higher risk of not attending their scheduled appointments, with the hopes of enabling healthcare providers to take additional proactive steps to encourage their patients to actually show up for their appointments. This would enable more efficient resource allocation, reducing the financial losses, and improve appointment turnout. We find this project compelling because it would allow us to understand the effects of demographics, location (and living conditions), health conditions, and patient engagement on no-show appointments, ultimately helping providers identify when additional interventions could improve attendance and enhance patient outcomes in the healthcare system.","We will be considering classification models to predict the likelihood of a patient missing the scheduled appointment (so our target variable is a “no-show”). We will use logistic regression as our starting point because it can handle binary outcomes and provide insights into the importance of each feature. We would use a Random Forest classifier to better capture the complex relationships in our data, as it would likely be better at predicting no shows based on our features. We would try out a Decision Tree classifier to capture the non-linear relationships, but since decision trees usually overfit to the training data, it would likely not produce very accurate predictions on our test data. However, it could help us capture the nonlinear relationships in our data. We will also use neural networks because they capture complex nonlinear relationships and might improve our predictive accuracy. 

We would use a hypothesis test to see if certain features (like age, handicap, or other health conditions) actually have predictive power and are better than the baseline value. The baseline would randomly predict no shows based on the dataset’s no show rate, and the alternative model would be trained on the features. We will observe the models based on the performance metrics we saw in class as well, like accuracy, precision, and recall. We will also use bootstrapping to create confidence intervals for our performance metric. We will also try to engineer a few additional features (perhaps determining affluence based on the neighborhood and location), along with observing the correlation matrix to see how our features are related to each other. ","We may face challenges related to dataset features and its large size. Since we have around 100K rows, using deep learning models could require a great deal of processing power, which may limit our experimentation. Additionally, our dataset might lack relevant features, such as appointment urgency, length, or type (e.g., surgery or routine check-up), which could have otherwise impacted prediction accuracy. We also expect all of our features to not be equally predictive, which might make it hard to interpret our complex models effectively. Furthermore, missing or inconsistent data in variables like neighborhood or health indicators could also negatively impact our model. Finally, if our complex models are more accurate than our simpler more explanatory models, we anticipate it to be difficult to determine the causes of no show appointments, which would in turn make it more complicated to use these insights to extract information to generalize when intervening in patient appointments would be effective. ",Michael Lu
Ben Menko | bmenko@seas.upenn.edu,Mukul Anand | mukula@sas.upenn.edu,Karthikeya Jayarama | jkarthik@seas.upenn.edu,"Mukul:
EDA
Model 1 (base model): Linear regression

Ben:
Model 2: ARMA / ARIMA model

Karthik:
Model 3: Neural network

Each person is also in charge of tuning their models, evaluating performance, discussing pros/cons of their approach, and polishing their section of the ipynb.

Group effort items:
Possibly a hypothesis test
Testing our best model compared to PJM’s model
Putting together the presentation","OpenMeteo Historical Weather API: https://open-meteo.com/en/docs/historical-weather-api#latitude=39.9523,41.4092,42.1292,38.8951,40.4406&longitude=-75.1638,-75.6649,-80.0851,-77.0364,-79.9959&start_date=2018-01-01&end_date=2024-01-01&hourly=temperature_2m,relative_humidity_2m,apparent_temperature,precipitation,surface_pressure,cloud_cover,wind_speed_10m,is_day,shortwave_radiation,direct_radiation&temperature_unit=fahrenheit&wind_speed_unit=ms&precipitation_unit=inch&timezone=America%2FNew_York

PJM Preliminary Hourly Load Data: https://dataminer2.pjm.com/feed/hrl_load_prelim/definition","52,584 rows x 30 columns after cleaning/combining data. May add/subtract some columns still","In the context of energy production, “load” refers to the total watt hours demanded by the electric grid at any given time. Power companies have a crucial duty to ensure that they can meet the demanded load 24/7/365. Otherwise, blackouts can occur, such as the Northeast blackout of 2003, which inadvertently resulted in the deaths of around 100 people. 

Therefore, load forecasting is a problem within data analytics that can literally save lives. When power companies can precisely predict how load will change over time, they can make well informed business decisions that ensure our safety while maintaining minimal costs. Marginally better load forecasts can translate to millions of dollars saved over the course of several years of operation.

While there are many variables at play, weather is a strong predictor of load demand on its own. For our project, we hope to assess several models predicting load from weather data. Additionally, we will compare our model to PJM’s state-of-the-art model and evaluate/explain the discrepancies.
","Modeling: we will start with a simple linear regression as our base model. Iterating on the base model, we will likely try an ARMA or ARIMA model and a neural network. The inputs will be weather data as well as one-hot encodings of some temporal variables. The target variable will be load demanded. The temporal resolution will be hourly.

Hypothesis testing: we are not necessarily planning on doing a hypothesis test. One option would be to test whether our best model is equivalent to PJM’s model, although we are not clear on how to formalize such a test.
","Decisions relating to augmenting the dataset (e.g. adding lagged variables)

Ensuring proper training/testing splits 

Formalizing a hypothesis test to compare our model with PJM’s model (not strictly necessary)

Deciding on an architecture and hyperparameter tuning for neural network

(Stretch goal) Implementing timeseries modeling techniques to enhance accuracy
",Michael Lu
Darren Mo | darrenmo@sas.upenn.edu,Kiwon Yang | kiwony@sas.upenn.edu,Ben Tausner | btausner@seas.upenn.edu,"Kiwon Yang - Data source acquisition and preprocessing
Darren Mo - Data modeling
Benjamin Tausner - Data analysis and preparation
",https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset,"50,000+, 6,649 different stocks","Does the performance of major sports teams significantly impact the stock prices of companies based in that city? We are trying to study if performance of sports teams (seen through regular season performance, how far they make it in the playoffs, and if they win the championship) have a statistically significant impact on the success of businesses based in the city that the team is located in. This is interesting to us as sports fans and could help quantify the wider economic impact that a sports franchise, especially a successful one, can have on a region.","We are considering using regression models to see if we can predict a company’s success as a function of their major sports team’s success, or vice versa, with explanatory variables being win percentage, how far teams get into the playoffs and whether or not they win the championship.

Yes, we are considering doing hypothesis tests. Our null hypothesis would be that sport team performance does not affect the success of companies in a city, and our test statistic would be the stock price of those companies. We are planning on using simulations to test our hypothesis to see if our observed effect is statistically significant.","Comparing data across sports teams/sources might be difficult. Also our targets might not have a high level of affiliation, so might either need to modify what we are looking at or narrow down choices of stocks to those of companies in specific industries only.
",Michael Lu
Kathy Fu | kathyfu@wharton.upenn.edu,Robert Lusardi | lusardir@wharton.upenn.edu,Victor Porsenna | porsenna@wharton.upenn.edu,"Kathy Fu
EDA and Visualization, Feature Engineering

Robert Lusardi
Modeling Execution & Hypothesis Testing

Victor Porsenna
Data acquisition, digestion, wrangling, and cleaning
Given large data set, can assist in restructuring data into an efficient form (Normal Form) and handle joins, etc.
","Data Source
Our dataset contains airline flight information that is publicly available from the FAA. This particular set was compiled by Maven Analytics and contains all US flights in 2015, totaling to over 6 million rows.  It has over 10 features and our group will add weather data either from the Aviation Weather Center or the python library Metostat.
https://mavenanalytics.io/data-playground?order=date_added%2Cdesc&search=flight",578.5MB,"The primary objective is to understand how weather and other factors (such as airline company, departure airport, departure time, etc.) impacts the likelihood and probability of flight delays. Some examples of the types of question we will explore are:
1) What is the average expected delay if it is raining? Snowing? Etc.
2) What factors are the most impactful in determining flight delays?
3) If there is thunder, what is the likelihood of delay?

Value Proposition
1) Understanding the likelihood of delays could be very beneficial to:
    - Airlines: better understanding of when a flight will be delayed, or the length a flight will be delayed, can be very helpful logistically, for things such as staffing and rescheduling/re-routing
    - Travelers: I’m sure we’ve all had nightmare travel experiences due to unforeseen delays; it could be helpful to know if a flight has a significant likelihood of a delay so you can plan accordingly
","1. Modeling Plan
We expect to use both Regression-style and Classification models

1.1 Regression
1.1.1 Would be to answer questions such as: what happens to expect delay time if there is an additional inch of rainfall? If there is an additional 10MPH of wind?
1.1.2 We expect a multivariate regression model (or perhaps Lasso or Ridge, given the many potential features) would be suitable

1.2 Classification
1.2.1 Would be to answer questions such as: Based on the weather, is it more likely than not this flight will be delayed? Based on the weather, what is the estimate of how long the flight will be delayed (classify a flight into one of [x] discrete buckets)?
1.2.2 We believe a tree-based approach, e.g. GB or RF with suitable hyperparameter tuning, would be appropriate


2. Hypothesis Testing
2.1 Yes, we plan to do hypothesis testing (the example questions above extend naturally to hypothesis tests)
2.2 Some example hypothesis tests:
a) Is [x] weather pattern significant versus the null world?
     - Can test in isolation (traditional hypothesis test), or as part of a broader multivariable regression (residual shuffle)
            -Null: X weather pattern is not significant
            -Test Statistic (single pattern in isolation): Proportion of flights that are delayed where there is [x] weather pattern; compare to how often flights are delayed if there is no effect from [x] weather patten

b) Is there a significant difference between [x] and [y] airports ability to handle inclement weather?
     - Null: There is no difference between the two (i.e. the probability of delay if there is bad weather is the same across both airports)
     - Test: Difference in proportion of their delays. Compare to range of proportions from simulations of null world.","Anticipated Obstacles & Challenges
1) The primary challenge will be wrangling and cleaning; the airline dataset is very large (~6m observations), and getting the relevant weather data for each airport (or mapping the data from a more general weather set) may be difficult
2) Further, there will be many delays that are non-weather driven (or significantly impacted by factors other than the weather), which may be hard to identify and could meaningfully affect our analyses

We also expect there will be some challenges when interpreting/building our models, or trying to predict/generalize from our model
1) For example, if it snows in Atlanta, flights will almost certainly be delayed, whereas it snows frequently in Salt Lake City, but they are an efficient airport and more prepared for snow, so they may have lower likelihood of delay",Michael Lu
Andrew Mao | am123@upenn.edu,John Otto | jwotto@upenn.edu,Didrik Wiig-Andersen | didrik@upenn.edu,"Didrik will be responsible for the EDA and visualization of findings. John will handle the binary classification task for predicting project approvals, while Andrew will focus on the regression task to estimate project processing time. Each team member responsible for a model will also conduct its evaluation. For the write-up, Didrik will be responsible for the introduction, background, conclusion, and discussion sections. Andrew will handle the approach and methods section, while John will focus on the results section.",https://www.kaggle.com/datasets/aparnashastry/building-permit-applications-data,"181,495 rows and 43 columns","Our project aims to analyze factors influencing building permit approvals in San Francisco, with the objective of developing a model to assess whether a project  proposal will be approved. We also aim to develop a model to estimate the time required for a particular project proposal to be processed. Building permits are essential as they help maintain consistent standards across construction projects, prevent unsafe practices, and ensure compliance with environmental, zoning, and safety regulations. However, permit processing is often slow, incurring high costs for project developers and home owners. In particular, San Francisco has the slowest building permit approval time of any city in the US. With the selected dataset, we aim to identify attributes that contribute to permit approval and impact processing times. This can aid developers and home owners submit proposals with a higher chance of acceptance while providing a processing time estimate for better project planning and management for construction in San Francisco.","For our project, we are considering both classification and regression models. For classification, the target variable is the feature, approval_status, which indicates whether a project was approved (1) or not (0). We plan to use models including Logistic Regression, XGBoost, Random Forest, and LightGBM for this task. For regression, the target variable is time_for_approval, calculated as the difference between the issued date and the filed date, to estimate the duration of the process. Here, we will also use Multiple Linear Regression, XGBoost, Random Forest, and LightGBM models. Hypothesis testing is not part of our approach.","Some challenges of this project include handling missing values. For example, the case of missing street names for which the permit is requested. Additionally, predicting approval likelihood could be complex due to external and confounding factors, such as shifts in the political climate, policy changes, economic events and more, which may influence approval rates but are difficult to model directly. We will outline this further in our analysis.",Michael Lu
Bowen Ying | bying24@seas.upenn.edu,Edward Liu | edliu@wharton.upenn.edu,Maya Narang | mnarang@sas.upenn.edu,"SQL Queries: Bowen
Data pre-processing: Maya, Edward
Hypothesis testing: Bowen
Model creation & tuning: Edward, Maya, Bowen",https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis,"600,000","Healthcare fraud causes billions of dollars of losses in healthcare spending every year, leading to increased insurance premiums and higher costs of healthcare overall. As such, gaining insights about when healthcare fraud occurs and how it can be detected could have profound implications for efficiency and cost reduction in the healthcare system. With this patient claims data, we plan to study amounts reimbursed for healthcare providers, instances of repeat billing, and services performed to identify if there are any trends that differ between healthcare providers that are labeled as potentially engaging in fraud, and those who are not. ","We think due to the complex relationships & many types of provider fraud, there is likely not a linear relationship between input variables and fraud detections. We are considering the following models: 
- Since this is a large dataset with lots of claim data, we are considering an Ensemble model with a Random Forest Classifier with bagging to improve bias-variance tradeoff. The benefit here is that the spurious branches from falsely identified potential provider fraud cases will be canceled out on average.
- At the same time, we are considering using gradient boosting to identify explainable relationships between risk factors and provider fraud. We will compare the results of gradient boosting vs random forest classifier to decide which model is best to use. 
- Another alternative is using neural networks to model the complex relationship between risk factors and provider fraud. However we must be careful to not overfit when developing the model. 
The target variable is determining whether a healthcare provider is fraudulent or not. 


One hypothesis test we are considering is looking at amount reimbursed for healthcare providers.

Null Hypothesis (H0): The average amount reimbursed is the same for providers with and without fraud flags.
Alternative Hypothesis (H1): The average amount reimbursed differs significantly between providers with and without fraud flags.

Some of the topics from class that we intend to use include:
1. Simulations/Bootstrapping – separate the data into two groups, healthcare providers with fraud flags, and those without fraud flags
For each group (one from providers with fraud flags and one from those without) we can calculate the mean reimbursement amount
For each pair, now we can calculate the difference between mean reimbursement amount
Repeat this process, and we will get a distribution of for the difference of mean reimbursement amounts → we can now create a confidence interval, which can lead directly into the 
2. P-value calculation: We can calculate the p-value based on how often the bootstrap sample differences are as extreme as the observed difference. 
If the observed difference lies outside the range of most bootstrap sample differences, then we can conclude that the difference is significant
3. Test statistic calculation: We will use the difference between the amount reimbursed by providers that are flagged and those that are not flagged as fraudulent, and create a test statistic using this difference","There are two main challenges that we anticipate with this project:
1. Handling queries for large dataset effectively (600,000 entries of patient data): 
Strategies to address this include caching, indexing, and query optimization.
2. Maintaining data quality during preprocessing:
Issues with inconsistent formatting, null values, and duplicate entries can distort our analysis results, so we have to be careful with our data validation steps.",Praj Chirathivat
Nitish Kaza | nkaza@sas.upenn.edu,Kevin Lu | kevinlu1@seas.upenn.edu,Aditi Shukla | aditishu@sas.upenn.edu,"Kevin will work on Data Modeling, specifically on Neural Networks. Aditi will work on data modeling as well, specifically logistic regression and XGBoost. Nitish will work on data cleaning and potentially hypothesis testing (if decided to do later). ",https://www.kaggle.com/datasets/START-UMD/gtd,182k without cleaning,"In this project, we seek to build a model capable of classifying the perpetrator/group responsible for any given terrorist attack given a slew of metadata regarding a specific attack. We believe this will be immensely helpful in holding perpetrators accountable as well as identifying key patterns or ‘signatures’ for certain terrorist groups and the way they conduct their attack.","What types of models are you considering? Classification vs. Regression? What's your target variable? We would like to use methods that consist of testing a series of models including two variations of Logistic Regression (OneVsRest and Multinomial), XGBoost, and a neural network model, TabNet. No plan on testing hypothesis yet but we might do so later. ",We feel like data cleaning would be tedious to do in this case since we have many columns that do not contribute “relevant” information for our project’s purpose.,Praj Chirathivat
Richard Zhang | richz@seas.upenn.edu,Olivia Hu | huolivia@seas.upenn.edu,Daniel Li | dli2004@seas.upenn.edu,"Richard Zhang - EDA, model training and testing, model evaluation
Olivia Hu - data wrangling, presentation and notebook
Daniel Li - EDA, presentation, visualization

All of us will also just do a little bit of everything that way we are familiar with aspects of the whole data science pipeline so these are not hard responsibilities.",https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022,1 million,"We want to see if there are any interesting insights to be found regarding bank account fraud — are there specific features (information about the customer associated with the account) that seem to be more useful than others in predicting whether a given bank account is fraudulent? It’s an interesting project to consider because the ability to accurately identify fraudulent bank accounts could be highly useful commercial banks. Financial losses, security costs, and reputational damage could all be reduced with a reliable predictive model.","We are tackling a classification problem and are considering using standard tree-based classification models such as random forest and gradient boosted trees (XGBoost). Tree based methods seem to be a good choice because they are flexible, interpretable, and are less prone to overfitting. Our target variable is fraud_bool (whether or not a given bank account is fraudulent or not). ","Yes, we are considering doing hypothesis testing. Our null hypothesis would be that our model is no better than random chance. To test this hypothesis, we can do permutation testing by shuffling the labels and finding the distribution of how accurate random chance is. Our test statistic would be the p-value.",Praj Chirathivat
Feng Jiang | fjiang45@seas.upenn.edu,Jiayi Chen | jc5ii@seas.upenn.edu,Amber Yan | amberyan@seas.upenn.edu,"Feng Jiang: Clean and preprocess the raw dataset, conduct feature engineering and dimensionality reduction.
Jiayi Chen: Build, fine tuning, and evaluate several machine learning models.
Amber Yan: Conduct hypothesis testing on key features and target variables, and interpret the final model results and drive meaningful insights.",https://www.kaggle.com/competitions/playground-series-s4e9/overview,315k,"The goal of this project is to build a model that predicts used vehicle prices based on attributes like brand, model, year, mileage, fuel type, engine specs, transmission, accident history, etc. By accurately estimating vehicle prices, we aim to assist buyers, sellers, and other stakeholders in making well-informed decisions. Specifically, buyers can use this tool to understand fair pricing, avoid overpaying, and compare options, while sellers can set competitive prices to maximize revenue and increase sales likelihood. Moreover, dealerships and analysts can track pricing trends and optimize inventory based on market insights, potentially using the model for dynamic, data-driven pricing adjustments. 
Our project is interesting because in a growing used car market, accurate, data-driven pricing fosters trust and transparency. Our model leverages structured data to improve pricing accuracy, benefiting all parties by promoting fair and informed transactions.","We will try various regression models, including linear regression, decision trees, or gradient boosting models, which will be evaluated to determine the best fit for this data. The primary target variable for this project is the price of each vehicle. 
We will conduct hypothesis testing on certain feature relationships with the target variable. For example, we hypothesize that cars with higher mileage tend to sell for lower prices. Our null hypothesis (H0) would state that mileage has no significant effect on car prices, while the alternative hypothesis (H1) posits a negative correlation between mileage and price. We’ll use correlation analysis as a preliminary step, followed by bootstrapping methods to assess the statistical significance of these relationships. Additionally, we may explore interactions between other features, such as the impact of brand and model year on price, hypothesizing that brand reputation positively influences resale value while older model years typically lower it. We aim to draw more precise inferences and validate our assumptions regarding feature interactions.","Given the diversity of car attributes, some listings may have missing or inconsistent values, particularly for features like engine power. To address this, we’ll explore imputation techniques (e.g., mean/mode imputation or K-nearest neighbors) and carefully assess any potential biases introduced by imputed data. Additionally, columns like brand and model can contain a wide range of unique values, leading to high-cardinality issues that increase model complexity. We plan to manage this by using dimensionality reduction techniques to streamline model inputs.",Praj Chirathivat
Nathan Lang | nblang@wharton.upenn.edu,Otakar Korinek | Otakar@wharton.upenn.edu,James Schnoebelen | jschnoeb@wharton.upenn.edu,"Nathan will be responsible for cleaning and augmenting the data for analysis and EDA, James will do the modeling and regression to predict fake news, and Otakar will be responsible for the data visualization in the project.",https://www.kaggle.com/datasets/ismetsemedov/transactions,7477306,"We intent to study credit card fraud through the Kaggle database with 7.7m transactions from 5000 unique individuals. The ultimate objective of the project is to study credit card fraud rates and predict fraud through a logistic regression using many variables regarding time, merchant details, etc... The project is especially interesting as our team is interested in the complex payment ecosystems an this fits perfectly within our interests.","We are considering using a logistic regression model to predict the binary outcome target ""is_fraud"". We will run various models using multiple logistic regression, random forest, and other methods to find the most suitable model for the dataset without overfitting. We also will conduct empirical simulations to verify that the overall model is significant and that each coefficient is significant. ","We anticipate that the EDA will take the most time given the breadth of variables and data within the dataset. Furthermore, the colinearity of many variables will also pose an issue to ensure that our model is not skewed by collinearity. ",Praj Chirathivat
Xianli Fu | xianlifu@sas.upenn.edu,Jocelyn Lee | jocelee@wharton.upenn.edu,Caroline Gong | czgong@wharton.upenn.edu,"Xianli: Data Cleaning (on the original CDC data in SAS format and converting it); Modeling
Caroline: Modeling; Presentation
Jocelyn: Data cleaning; EDA; Presentation",https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease,319795 Rows x 18 Columns,"The objective of our project is to build a model that predicts the likelihood for someone to develop heart disease, given a range of factors. According to the CDC, heart disease is a leading cause of death for people of most races in the U.S. About half of all Americans (47%) have at least 1 of 3 major risk factors for heart disease: high blood pressure, high cholesterol, and smoking. Other key indicators include diabetes status, obesity (high BMI), and others. Identifying the key factors that have the greatest impact on heart disease is hence very important in healthcare.","We hope to build a classification model to achieve our objective. Specifically, we intend to build and compare between the logistic regression, XGBoost, Random Forest, and potentially SVM models. A classification model is the most appropriate because whether a person has heart disease is a binary variable, and a model like logistic regression will be helpful in identifying the specific features that have the most influence on the probability of developing heart disease. Therefore, our target variable to predict is whether one has heart disease, and we intend to conduct a few hypothesis tests - with the null being that a factor has no influence on the likelihood of heart disease. ","A key challenge is that the classes are unbalanced. That is, the number of people with no heart disease is almost 10x those with heart disease. As such, the model might default to predicting “no”, so this may require us to deploy creative strategies such as giving higher class weights to the minority class, or resampling methods. Additionally, there may be multicollinearity between some of the predictor features (for example, binary variable ‘Diabetic’ is almost certainly correlated with numerical BMI), so we have to be cautious when choosing the final features in the model.",Praj Chirathivat
Dalal Ahmidouch | dalalma@seas.upenn.edu,Jay Yang | jy6618@seas.upenn.edu,Terhi Nurminen | terhi@sas.upenn.edu,"Terhi
Lead on cleaning data and creating a popularity metric
Ensuring visualization and notebook readability

Dalal
Lead on hypothesis testing, assist with classification models 

Jay
Lead on MLR models and classification models
","https://www.kaggle.com/datasets/asaniczka/top-spotify-songs-in-73-countries-daily-updated
",1.4 million rows and 25 columns,"Spotify’s market cap is estimated to be nearly $78 billion, while music streaming produces more than $17 billion in revenue per year and growing. This creates incentives to produce songs that gain popularity on streaming platforms to maximize profits for both the artist and the company. Therefore, we wanted to use this dataset to understand which features make a song streamable and try to predict song popularity from its characteristics. The dataset would also allow us to look at potential temporal effects, such as whether people prefer high-energy songs in the summer and low-energy songs in the winter, which can impact publishing schedules for singles.

We hypothesize that in addition to specific features, the region plays a large role in which types of songs gain popularity. More specifically, we assume that there are cultural preferences to song characteristics. Our goal is to test whether this is true, and identify the features that make a song popular in each geographic region, with the granularity determined by our analysis. Te test whether our model is powerful enough, we will attempt to predict the realized popularity of select songs in different regions.
","How can we predict a song’s popularity based on its attributes? And how does the weight of these attributes differ by region?
Create a custom popularity scoring mechanism that measures a song's popularity for the year, including daily/weekly movements and monthly top 50 appearances.
Create MLR predicting popularity score based off of song attributes, x1= album_release_day, x2= danceability, x3= is_explicit, x4=tempo,x_5 = energy(intensity and activity level of the song),x_6 = acousticness, x_7= valence,  y=popularity
Create regional MLRs with same attributes as above to analyze the how attribute weights vary by regions
Create a multi-classification model to predict whether a song will achieve a specific popularity category, with categories such as being in the top 10, 50, or 100. The target variable is a binary indicator on popularity level and previous features in MLR models will be used to build up this model. To evaluate the classification model, the metric such as accuracy, recall and precision will be used.
Training and Testing Data: Eighty percent of the top Spotify songs from 73 countries (covering the period from October 17, 2023, to October 17, 2024) will be used to train the machine learning models. The remaining data from this one-year dataset, along with newly updated data, will be used as the test set to validate the model's performance.


Does region affect the weight of different song attributes in determining popularity? 
H0: Regional differences do not significantly affect the attribute weights in determining song popularity. Specifically, for each attribute weight, the observed difference across regions is due to random variation.
H1: Regional differences do significantly affect the attribute weights, meaning regions do have varying levels of influence from song attributes on popularity.

Gather the coefficients from each regional MLR, create a vector showing values for each attribute across regions
For each attribute (e.g., danceability), calculate a measure of spread across the regional weights. We can use variance or standard deviation of the weights across regions, which reflects the extent of variation
On the non-region specific dataset, randomly assign the ""region"" labels to each entry. So that we can break any existing regional association with the song’s attributes while preserving the overall distribution. 
After shuffling, split the data back into the original regional groups and run an MLR for each “pseudo-region” dataset
Record the attribute coefficients from each region’s mode
Perform this random label assignment and MLR fitting process many times to build a distribution of attribute coefficients under the null hypothesis
Calculated the variance or sv of all the regions for each trial
Compare observed sv or variance to simulated, was the variance by region we got by chance or actually meaningful?

Do our regional models outperform our general model?:
H0: The regional model provides no significant improvement in prediction accuracy over the overall model for each country, meaning the overall model is sufficient for predicting song popularity
H1: The regional model provides a significant improvement in prediction accuracy for each country, indicating that regional differences matter for accurately modeling popularity.

For each song in a region’s dataset, calculate the residuals (actual popularity score - predicted score) from the overall model
Repeat step 1 but use regional models instead of model that doesn't separate region
For each region, calculate the mean squared error of residuals on both models, and then calculate difference to be our test statistic
Randomly shuffle the regional labels for each song (breaking the region structure) while keeping the same residuals. Recalculate the MSEs for the shuffled data
Repeat this shuffling process many times to build a distribution of MSE differences under the null hypothesis
Compare observed MSE difference with simulated MSE difference
","There are several key challenges that need to be addressed and ensure successful outcomes. These challenges include aspects related to data quality, model complexity and temporal analysis, as outlined below:
The lack of unique ID for artists may lead to difficulties in distinguishing artists with similar names and tracking them consistently across datasets.
Proper encoding of categorical features, such as country, is necessary to ensure meaningful contributions  to the model without introducing bias.
The large volume of the dataset makes it challenging to identify and prioritize relevant features effectively while avoiding redundancy.
The complexity of the model and dataset increases the risk of overfitting, requiring careful model design, regularization and cross validation.
A fair definition of popularity is challenging, especially for newer songs that have not had sufficient time to gain traction.
",Vickie Liu
"Lan(Rhea) Sun | lansun@sas.upenn.edu
",Haoyun Wu | wuhaoyun@seas.upenn.edu,Qiutan (Leo) Li | qiutanli@sas.upenn.edu,"Leo and Haoyun are primarily responsible for coding(data analysis and modeling). Rhea is primarily responsible for the project write-up, coding, and hypothesis testing. However, each one of us is  working on each section simultaneously.","https://www.kaggle.com/datasets/yasserh/loan-default-dataset/data
",28.48 MB,"Our goal is to predict if someone might default, or fail to pay back a loan, by building a logistic regression model. This model uses details about the borrower’s personal information—like income, credit scores, and employment—to estimate how likely they are to default. With this tool, lenders can decide which applicants might be higher risk and make safer, more informed lending choices. Our approach helps improve the loan approval process, making it easier to spot risky loans and prevent financial losses.","Our project aims to build a classification model to predict loan default risk, focusing on a binary outcome (default vs. non-default). We will start with a logistic regression model as a baseline for its interpretability and then explore advanced models like Random Forests and Convolutional Neural Networks to enhance accuracy. Hypothesis testing will assess whether features, such as employment status and income level, significantly impact default likelihood, using tests like the Chi-square and t-test/ANOVA for different variables. This approach ensures a robust prediction model that aids in identifying high-risk borrowers.","Data Imbalance:Few default cases, requiring resampling to address imbalance.
Data Cleaning:Identify and correct errors, missing values, and outliers in the dataset. 
Feature Selection & Engineering: Excess features cause overfitting, and missing features lead to underfitting.
Hyperparameter Tuning: Inappropriate hyperparameter settings may lead to underfitting or overfitting of the model.",Vickie Liu
Adithya Selvakumar | adiselv@seas.upenn.edu,Gabriel Sternberg | gster@seas.upenn.edu,Stanley Yu | stany@seas.upenn.edu,"Gabriel: Data acquisition and preprocessing, ensuring data quality and consistency.
Stanley: Exploratory data analysis (EDA) and feature engineering to identify key variables influencing election outcomes.
Adithya: Model development and evaluation, implementing and assessing predictive models to determine the impact of campaign finance on election results.","The primary dataset for this project is the Database on Ideology, Money in Politics, and Elections (DIME), which provides detailed information on political donations and expenditures. The dataset is publicly available at: https://data.stanford.edu/dime.","The DIME dataset contains over 500 million itemized political contributions made by individuals and organizations to local, state, and federal elections from 1979 to 2022.","This project aims to analyze the relationship between campaign financing and election outcomes in U.S. politics. By examining the DIME dataset, we seek to identify patterns and correlations between the amount of funding candidates receive and their success in elections. Understanding this relationship can provide insights into the influence of money in politics and inform discussions on campaign finance reform.","We plan to develop a regression model to predict election outcomes based on campaign contributions and other relevant features. The target variable will be the election result (e.g., win/loss margin). Additionally, we will conduct hypothesis testing to assess whether higher campaign contributions significantly increase the probability of electoral success. This will involve formulating null and alternative hypotheses and using statistical tests to evaluate the impact of campaign finance on election results.","Potential challenges include handling the large size of the dataset, which may require substantial computational resources, and addressing missing or inconsistent data entries. Ensuring the accuracy and reliability of the data, as well as accounting for confounding variables that may influence election outcomes beyond campaign financing, will also be critical considerations.",Vickie Liu
Shengkang Gao | gaosk@seas.upenn.edu,Anyu Pan | pananyu@seas.upenn.edu,Yuxuan Liu | liuyx02@seas.upenn.edu,"Anyu Pan: 
Data cleaning and Feature engineering, Model Implementation, Writing 
Shengkang Gao:
Model Implementation, Hypothesis testing and statistical analysis, Writing
Yuxuan Liu:
EDA, Visualization, Model Assessment and Hyperparameter Tuning, Writing",https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset,"114,000","Based on the data set of Spotify which provides many features of tracks including popularity, singer name, audio attributes etc., our project wants to figure out how to become a music star and make the tracks more popular from the perspective of big data. The ultimate objective of our project is to provide meaningful suggestion for music creators, music service provider and music industry to promote their development. This project is interesting because it belongs to an interdisciplinary area where we combine music theory with data science and offer a unique perspective on how quantifiable elements predict musical success.","We will use both Classification and Regression models.
Regression model
      Target variable: popularity score of a track.
Algorithms Considered:
      Linear Regression
      Random Forest Regression
      Gradient Boosting
Classification Model
       Target variable: genre of a track.
Algorithms Considered:
        Decision Trees
        Random Forest Classifier
Hypothesis:
        H1: Specific track genres will be more popular than others.
        H2: Towards each genre (or the popular kinds) of tracks, some features can make a track popular.
        H3: The star singers can increase the popularity of the tracks.
        H4: If a star singer tries another genre, his new tracks will attract more audience by following his/her former style.","Challenges:
Data Imbalance
Too many features may lead to overfitting.
Data Quality Issues: Missing or inconsistent data could affect the accuracy of models.

Obstacle:
What is the right target variable to measure the popularity of each genre?
How to remove the influence of other factors (whether the singer is a super star) which may influence the popularity of a song?",Vickie Liu
Lucy Chen | chen966@seas.upenn.edu,Licheng Guo | guolc@seas.upenn.edu ,Ruiyi Ji | jry1130@seas.upenn.edu,"Licheng Guo is responsible for conducting Exploratory Data Analysis (EDA) and contributing to the modeling process. Lucy Chen focuses on data preprocessing, preparing the dataset for analysis, and also works on modeling. Ruiyi Ji handles data analysis and contributes to the development and refinement of the models.","https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease
The dataset titled 'Key Indicators of Heart Disease' was sourced from the CDC's Behavioral Risk Factor Surveillance System (BRFSS), which collects annual health-related data from U.S. residents through telephone surveys. This dataset includes over 400,000 observations and was reduced to 40 variables most relevant to heart disease. The dataset is publicly available, free, and legal to use, thus meeting the project requirements. It contains structured data, which makes it suitable for the type of analysis we plan to perform.","400,000","The objective of the project is to predict the likelihood of an individual developing heart disease based on various demographic, lifestyle, and health-related factors, using data analysis and machine learning techniques. Heart disease remains one of the leading causes of death in the U.S., responsible for a significant portion of morbidity and mortality rates. This project aims to uncover patterns and risk factors that contribute to the development of heart disease, leveraging the power of large datasets and advanced predictive models.

From a data analysis perspective, the project offers an opportunity to explore a wide range of methods, including feature selection, data preprocessing, and classification algorithms, to accurately predict heart disease risk. Machine learning models such as logistic regression, decision trees, and neural networks will be employed to analyze the dataset and evaluate the performance of different approaches. By identifying the most significant predictors of heart disease, the project aims to provide actionable insights that could inform healthcare professionals and guide policy makers in developing targeted prevention strategies.

This project is particularly interesting because it applies cutting-edge analytical tools to a real-world problem with immediate public health relevance. Nearly half of all Americans have at least one major risk factor for heart disease, and the ability to predict these risks could significantly improve early intervention efforts. By understanding which factors are most predictive, we can enhance the development of personalized healthcare interventions, reduce healthcare costs, and potentially save lives. ","This project is a Classification Problem where the target variable is Had-HeartAttack, a binary variable indicating whether an individual has had heart disease (Yes) or not (No). Given the unbalanced nature of the classes, we will use techniques such as undersampling or adjusting class weights to improve model performance. We will begin with a logistic regression model as a baseline and then proceed to more advanced machine learning models such as Random For- est, Support Vector Machines (SVM), and possibly Gradient Boosting to improve predictive accuracy.

We aim to investigate whether sleep hours significantly affect the likelihood of heart disease. The null hypothesis (H0) states that sleep hours have no significant effect, while the alternative hypothesis (H1) suggests a significant effect. The test statistic is evaluated using a Wald test, dividing the estimated coefficient for sleep hours by its standard error. Additionally, a permutation test is used to non-parametrically validate the null hypothesis, and bootstrapping is applied to construct a confidence interval for the sleep hours coefficient.","Several challenges and obstacles are anticipated with this project, particularly related to data quality, imbalance, and model selection. The target variable is imbalanced, requiring resampling techniques or class weighting to improve recall for heart disease predictions. Data preprocessing will handle missing values and transform categorical variables. Feature selection is needed to reduce noise from irrelevant features, using methods like recursive elimination or Lasso. To avoid overfitting, cross-validation and hyperparameter tuning are essential. Finally, there's a trade-off between model accuracy and interpretability for clinical use, while computational resources may be strained due to the dataset's size and model complexity.",Vickie Liu
"Young Jun | yhjun@seas.upenn.edu
",Chloe Yun | chloeyun@seas.upenn.edu,Arya Sanjary | aryasa@seas.upenn.edu ,"Arya Sanjary | aryasa@seas.upenn.edu 
- Lead Data pre-processing 
- Focus on XG Boost modeling

Young Jun | yhjun@seas.upenn.edu
- Lead Exploratory Data Analysis 
- Focus on Random Forest modeling

Jinyoung (Chloe) Yun | chloeyun@seas.upenn.edu
- Lead Hypothesis Testing
- Focus on Decision Tree modeling
","Data Dictionary: https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGK

Download: https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anzr=b0-gvzr

Data Usage Rights: https://ntl.bts.gov/ntl/public-access/managing-rights
","1.5 ~ 2 gb, over 100,000 rows ","A Comparative Analysis of Machine Learning Models for Predicting Delay of Airplane Flights

Full report: https://docs.google.com/document/d/1sTtVw6aKcoQr9yIJ8a6RGchtzSLzu-kI-kXUHhdaGbI/edit?usp=sharing

Accurately predicting flight delay is crucial for both passenger satisfaction and operational efficiency. Timely arrivals contribute to a smoother travel experience and help manage airport congestion and traffic control. 

We plan to compare three machine learning models for predicting airplane delays: decision tree, XGBoost, and random forest. While many machine learning methods are currently being tested and refined in the field, our work aims to add to the existing literature by performing a comparative analysis of these models' performance. Through this analysis, we hope to lay a foundation for future research.
","This project involves a classification task, with the target variable being whether a flight is delayed.

We have chosen to use decision tree, random forest, and XGBoost models for our classification task of predicting flight delays. 

Test Statistics: Precision, Recall, F1-Score 

Testing Hypotheses: Bootstrapping 

","We expect difficulties in the initial data pre-processing stage, as our team members are relatively new to handling datasets exceeding 100,000 rows. 

Additionally, we need to closely analyze the potential correlations between data points, acknowledging that most real-world data do not consist of truly independent observations. ",Vickie Liu
Diamantoula Giannopoulos | gidia@seas.upenn.edu,Eleftherios Lazarinis | ellaz@seas.upenn.edu,John Woo | jwooiv@seas.upenn.edu,"Diamantoula: Data cleaning, defining hypotheses, identifying relevant statistics for tests, presentation and video contribution
Eleftherios: Feature selection and engineering, running simulations and testing the hypotheses, documenting modeling decisions, presentation and video contribution
John: Verification and quality checks to ensure processed data is ready for modeling, model evaluation and testing its accuracy, presentation and video contribution",https://www.kaggle.com/datasets/archie2023/https-tunneling-predictive-analysis-dataset ,"1154701 rows, 14 columns","In our project, we will attempt to develop a predictive model that identifies HTTPS tunneling traffic, distinguishing it from regular HTTPS traffic. HTTPS tunneling poses cybersecurity concerns because it masks malicious activity through encryption, making traditional content inspection techniques ineffective. By analyzing features such as packet size sequences, traffic patterns, and timing data, we intend to detect potential tunneling behavior based on network characteristics alone. This project is interesting because it addresses a growing concern in the increasingly consequential field of cybersecurity. It could offer a non-invasive detection method that preserves user privacy while enhancing network security—an approach that’s both practical and impactful for modern encrypted traffic environments.","We plan to use classification models, such as Random Forest, Support Vector Machines, or Gradient Boosting, to distinguish between HTTPS tunnel traffic and normal HTTPS traffic. The target variable is a binary label indicating ""tunnel"" or ""non-tunnel"" traffic. For hypothesis testing, we’ll examine the null hypothesis that ""packet size, timing sequences, do not significantly affect the detection of HTTPS tunneling."" By using simulation techniques, we’ll assess whether specific traffic characteristics contribute meaningfully to classification accuracy. This approach will not only improve model precision but also help validate the relevance of chosen features in identifying tunneling behavior. We could also further analyze between the various VPN services to see if any had potential differences in security measures that affected their vulnerability to such malicious activity or if it is a general problem facing all VPN services.","Some challenges that may arise include handling encrypted HTTPS data and limiting analysis to behavioral features rather than content. Additionally, the class imbalance between normal and tunneling traffic might impact model accuracy, which is similar to other security models such as screening for fraudulent credit card transactions because more than the overwhelming majority are trustworthy. Ensuring the model generalizes well to unseen data and adapting it to potential shifts in tunneling behavior are also anticipated obstacles.",Jessica Forsstrom
Corentin Favier | favierc@seas.upenn.edu,Joe Xue | joexue@seas.upenn.edu,Andrea Gonzalez | andgon@sas.upenn.edu,"Corentin: EDA, feature distribution visualization, encoding
Joe: Hypothesis testing
Andrea: Logistic regression, random forest",https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data,"253,680r x 22c (including predicted col)","Over a tenth of the American population is diagnosed with diabetes, a chronic and incurable disease that can often be prevented or delayed through lifestyle changes. Our project aims to use a dataset containing a range of self-reported health and lifestyle indicators to study diabetes risk factors. By analyzing these variables, we plan to identify patterns that predict the likelihood of diabetes. Our ultimate objective is to build a predictive model that can accurately assess diabetes risk based on health metrics, potentially guiding early intervention efforts.","For classification of our target variable diabetes status, our baseline model is logistic regression. We are considering machine learning models like decision tree, random forest, and gradient boosting to carry out this project.

We are considering hypothesis testing and plan to carry them out via simulations. The test statistic will vary for each. Some tests are  proposed below.

Null hypothesis: BMI has no significant relationship with diabetes
Alternative Hypothesis: BMI has significant relationship with diabetes
","We expect bias in our dataset, as several features are self-reported (e.g., “are you physically active?”), which could lead to overestimated health metrics. To address this, we’ll explore regularization techniques, such as Lasso, to refine feature selection and may combine additional datasets to enhance model accuracy and robustness.",Jessica Forsstrom
Rahman Mohammed | rahmanmo@seas.upenn.edu,Yi-Hsuan Cheng | yihcheng@seas.upenn.edu,Jessica Cao | jesscao@seas.upenn.edu,"Jessica is responsible for data preprocessing and Elastic Net Regression. Rahman is responsible for data visualization, feature engineering, model evaluation metrics, and Multivariate Linear Regression. Yi-Hsuan is in charge of hypothesis testing and Random Forest.",https://www.kaggle.com/datasets/brllrb/uber-and-lyft-dataset-boston-ma/data,"(693071, 57)","The primary objective of this project is to analyze and predict the pricing dynamics and demand patterns between Uber and Lyft rides in Boston, MA, factoring in both time-based (hourly and peak vs. non-peak times) and weather-related variables (temperature, wind, weather conditions, etc.). This project is interesting because it provides both riders and drivers with actionable insights. For consumers, it helps understand when and where prices are likely to surge. Drivers can identify high-demand times and locations to optimize hours and routes.","Our target variable is estimated fare prices based on variables like time, location, and weather. The models we are considering are Random Forest Regression, Multivariate Linear Regression, and Elastic Net Regression. To compute surge pricing, we will use Random Forest Classification; this model can handle multiple variables and capture complex patterns. We’re considering three hypothesis tests to explore key factors influencing pricing and surge likelihood, including the impact of weather on pricing, surge likelihood during peak hours, and differences between Uber and Lyft in price responses to weather.","The challenges with this project are:
- Missing Data: Balancing accuracy by filling or omitting gaps.
- Weather Correlations: Using VIF (Variance Inflation Factor) or PCA (Principal Component Analysis) to address overlaps.
- Model Interpretability: Relying on feature importance for clarity.
- Surge Imbalance: Testing oversampling or weighted classification.
- External Factors: Focusing on broader trends.
- Computational Constraints: Optimizing dataset and feature selection.",Jessica Forsstrom
Binghua Zhang | zbinghua@seas.upenn.edu,Xinran Zhu | xinranzh@seas.upenn.edu,Leyi (Loraine) Jiang | lorinej@seas.UPenn.edu,"Binghua Zhang: Data collection and preparation, machine learning model development, and results analysis.
Xinran Zhu: Strategy formulation, alternative data integration, and performance evaluation.
Leyi Jiang: Financial characteristics modeling, trading strategy comparison, and hypothesis testing.
","https://www.similarweb.com
https://finance.yahoo.com",50000,"Main thesis: Use predictive modeling and alternative data to generate a long/short strategy on Consumer D2C stocks to see if it adds value in investing in Consumer D2C stocks. We hypothesize that D2C companies' websites would be good proxies to the value of the businesses. 

Main steps: Select a set of consumer D2C stocks -> Collect financial & web traffic data -> Predict returns through modeling ->  Construct a long / short strategy -> compare against the benchmark

Value proposition: As the e-commerce field becomes more and more mature and competitive, investors need more sophisticated investment strategies to continue profiting from e-commerce investments. In this analysis, we want to see if we can leverage alternative data to improve e-commerce trading strategy assuming no expertise in the area.","1) Both regression and classification models like linear regression, random forest, GBM.

2) The target variable is predicted stock return, representing the projected return for each selected e-commerce stock (both mainstream e-commerce stocks and D2C brands) over a monthly period. The aim is to predict returns and use the resulting rankings for the long-short trading strategy.

3) Yes, we will conduct hypothesis testing to quantify the value added by web traffic data. 
Specifically:
- Null Hypothesis (H0): There is no significant difference in the predictive performance of models using only financial features versus models incorporating web traffic data.
- Alternative Hypothesis (H1): Models that include web traffic data show a statistically significant improvement in predictive performance over models using financial data alone.","- Data Quality: Web traffic data may lack accuracy, granularity, and alignment with stock return timeframes, with potential lags impacting model performance.
  
- Model Overfitting: Combining web traffic with financial data in complex models like GBM risks overfitting, especially with sparse or noisy data.

- Market Sensitivity: Web traffic data’s effectiveness can fluctuate with market conditions, possibly causing model instability.",Jessica Forsstrom
Jadyn Elliott | jadyne@seas.upenn.edu,Joelle Gross | joellegr@sas.upenn.edu,Mitali Kessinger|mitalik@seas.upenn.edu ,"Division of responsibility:

Clean data, perform Exploratory Data Analysis: Everyone

Models
Linear Regression + Hypothesis Testing: Jadyn Elliott
Random Forest: Joelle Gross
Neural Networks: Mitali Kessinger
Compile Notebook & Create Slide Desk: Everyone","For our final project, we will be analyzing data from Airbnb collected by Inside Airbnb. Inside Airbnb is an organization dedicated to collecting Airbnb listings information. The goal of this organization is to provide data aimed at understanding Airbnb's impact and make this data accessible so that communities can navigate short-term rentals and tourism effectively. Inside Airbnb’s data is freely available under Creative Commons license, with proper citation, and can be used as long as the data is not republished. For the purposes of this class, our use of the data falls under the acceptable criteria of the website’s guidelines.

The website consists of listings scraped from the Airbnb website for various cities all over the world. These rentals represent listings that were active on Airbnb within the past 12 months. For our analysis, we decided to focus on a handful of cities in the United States. We chose New York City, Chicago, San Diego, Nashville, Boston, Austin, Seattle, and Washington D.C.. We chose to focus on a diverse set of cities across the U.S, those of which have varying population sizes, geographic locations, and degrees of tourism.

 Above is a screenshot of the listings from New York City (left) and San Diego (right), highlighting two examples of the breadth of data available from this website. The dataset for each city has a wide variety of variables for each listing. For example, the data set includes how many rooms each Airbnb has, how long the rental has been an Airbnb, where it is located, the rating, the number of amenities available, whether the unit is an entire residence or a private room, and most importantly for our analysis, the price of the Airbnb. Each city has a different number of listings, ranging from 5,000 to 40,000 listings. The red dots on the two maps above are listings which are entire homes or apartments, the green dots are listings which are shared rooms. During the exploratory data analysis portion of the assignment, we will be joining together the individual datasets from these cities and removing rows which do not have values for our key variable of interest, price.",70000,"We plan to study and understand how Airbnbs are priced across various American cities. To do this, we plan to build three models where we will predict the price of an Airbnb listing from a set of key properties about the listing. There are many different features that may contribute to an Airbnb’s pricing, such as the physical properties of the listing such as the size or location, details about the host such as ratings and experience, and desirability of the unit through factors like popularity of visiting a given location at any time period. Airbnb hosts have the ability to determine the price of their property for different nights of the year, so our goal is to create a model which can accurately predict listing prices for new Airbnb hosts. We believe that creating a model to do this could help hosts value their properties appropriately and allow for guests to receive a fair rate compared to other listings in the area and across the nation. Our model will help hosts set prices that will result in both the listing being booked and the host profiting off their rental. The model will also help guests ensure they are not overpaying for their stay. 

We are combining multiple cities into one dataset, all of which have vastly different costs of living, Airbnb listing supply, and levels of tourism. Therefore, we want to better assess and understand whether the prices in these cities vary in a statistically significant way. We plan to run hypothesis tests on average prices between the cities (see the hypothesis testing section to understand our two plans for testing).","We plan to perform a regression analysis on our combined, cleaned Airbnb data. Our goal is to predict price from the following features: number of bedrooms (integer), number of bathrooms (integer), rental type (binary: property or single room), rating (integer), quantity of amenities (integer extracted from string array), season (4 binary variables for each season extracted from date), duration of time since property joined Airbnb (integer), and superhost (binary). We plan to train a linear regression model, a random forests model, and a neural network. We plan to hypertune parameters for both random forests and neural networks to achieve the best models we can, using a mean squared error metric to evaluate our success. 

Hypothesis Testing

We plan to utilize simulation-based hypothesis testing to investigate whether Airbnb prices significantly differ across the cities. Below are the two approaches we are considering:

Approach 1: Hypothesis testing for the average prices across cities
In this approach, we will calculate the range of average prices (difference between the highest and lowest average prices across cities) and use it as the test statistic.
Null Hypothesis: The average prices across all cities are the same, meaning the differences observed are due to random chance.
We will then run permutation tests by shuffling city labels across all listings to simulate the null world. 
We will shuffle and calculate the range 10000 times to generate a null distribution of ranges under the assumption that the city has no effect on price. 
If the observed range is statistically significant (i.e. the p-value is < 0.05), we can reject the null hypothesis, suggesting that differences in average prices across cities are unlikely to be due to chance alone. 
 
Approach 2: Hypothesis testing on linear regression model with city as a predictor
In this approach, we will run a multiple linear regression using all features, such as number of bedrooms, amenities, superhost status, and city as a categorical variable, to predict Airbnb prices. We will run a hypothesis test for each city.
Null Hypothesis: The coefficient for a given city is 0, meaning that any observed effect of an Airbnb being in that city on price is due to random chance. 
We will also use permutation testing here by shuffling the city labels to generate a null distribution of the city coefficients, simulating our null world. 
This process will be repeated 10000 times to generate a null distribution for each coefficient.
If we calculate the p-value and we find that our regression’s coefficient occurs less than 0.05, then we should have sufficient evidence to reject the null hypothesis, suggesting that the observed effect of that city on prices is unlikely to be due to random chance. 
","Anticipated Obstacles and Challenges
Unbalanced Quantity of Listings per City
Some cities have significantly more listings than others, which could bias the analysis towards cities with more data
Potential Solution: Use sampling techniques to balance the number of listings per city or apply weighted regression methods to account for the imbalance. 
Missing Data for Desired Features
Some listings are missing information for number of amenities or bedrooms or even price. 
Potential Solution: We could remove incomplete rows from our dataset.
Collinearity between Predictors
Some features like number of bedrooms and number of bathrooms could potentially be highly correlated, which might affect the stability of the regression model and make it difficult to interpret. 
Potential Solution: If we detect collinearity, we could drop or combine correlated features. ",Jessica Forsstrom
Karl Meyer | meyerkn@upenn.edu,Ahan Patani | apatani@wharton.upenn.edu,Sean Zhang | seanzh@seas.upenn.edu,"Karl Meyer - EDA and graphing
Ahan Patani  - Logistic regression, with hypothesis testing and bootstrapping for confidence intervals
Sean Zhang - Random Forest, XGBoost, and K-Nearest Neighbors, with feature selection and hyperparameter tuning",https://www.kaggle.com/datasets/yasserh/loan-default-dataset,98000,"Credit worthiness can be difficult to check for new customers, involving requests to outside companies that may not respond in a timely manner (or may cost money to do so). In some cases, companies offer “pre-approvals” that do not involve hard credit checks. This requires the ability to approximate credit worthiness without querying credit agencies. Our models will attempt to do that, and thus save time and money when a rough approximation of credit-worthiness is required. In our case, we are not looking at credit score, but instead whether individuals will default on their loan.","The overall objective is that given a customer's profile information (such as age, income, etc) we can predict whether they will default on their loan. This lends itself to a classification problem, suitable for models like logistic regression, decision tree, and k-NN. Since a single decision tree is more prone to overfitting with high model variance, we will use Random Forest and XGBoost. We will do a permutation test on our logistic regression model based on selected performance metrics.","The documentation for this data is somewhat limited, so we will need to keep exploring and see if more explanation is given about certain columns. This vagueness of certain potential features combined with the number of columns will make feature selection challenging.",Oscar Eichmann
Eric Cao | ecao22@seas.upenn.edu,Spencer Lee | spendude@sas.upenn.edu,Tianshu Feng | ftianshu@seas.upenn.edu,"Tianshu - Data Processing & Cleaning: Responsible for initially processing the data and cleaning it to address any issues, such as missing values or inconsistencies.

Eric - Exploratory Data Analysis & Modeling: Conducts initial data exploration, visualizations, and develops regression or classification models to analyze the impact of weather on ridership.

Spencer - Results Interpretation & Presentation: Interprets model results, tests hypotheses, and prepares a comprehensive presentation summarizing key findings and actionable insights.
","https://www.kaggle.com/datasets/taweilo/capital-bikeshare-dataset-202005202408 
",16086531 rows,"We intend to study how weather conditions influence ridership patterns for a bike rental company in Washington, DC. Specifically, our goal is to identify how different weather variables, such as temperature, precipitation, and humidity, affect the popularity of certain rental stops and rider behaviors. By understanding these trends, we aim to inform strategic decisions, such as optimizing bike allocations across stops and implementing surge pricing effectively during high-demand periods. This project is interesting because it highlights the intersection of environmental factors and urban mobility, offering practical insights for enhancing operational efficiency and improving the overall user experience in bike-sharing systems.","We are considering regression models to predict the impact of weather variables on bike ridership, as our target variable is the number of bike rentals, a continuous measure. Additionally, we may use classification models to categorize weather conditions and evaluate how these categories affect ridership patterns. Our target variable will primarily be the daily or hourly count of rentals. We also plan to test hypotheses, such as whether higher temperatures or clear weather conditions significantly increase bike rentals, and whether adverse conditions, like heavy rain, result in a notable decrease. This will help us understand and quantify the relationship between weather and rider behavior.
","We anticipate challenges such as data quality issues, including missing or incomplete weather and ridership records. Accurately capturing the impact of weather may also be difficult due to the complex and possibly non-linear relationship between variables. Additionally, accounting for confounding factors like holidays or major events could complicate our analysis. There also may not be strong correlations to make predictions upon.",Oscar Eichmann
Xiaotian Lin | tigerlin@seas.upenn.edu,Jiahua Liao | liao333@seas.upenn.edu,Jiaqi Wan | jiaqiwan@seas.upenn.edu,"Jiahua Liao
Data Collection & Cleaning: Gather, clean, and merge the NBA salary and
advanced stats datasets.
Exploratory Data Analysis: Generate visuals to explore relationships between
metrics and player salaries.
Jiaqi Wan
Hypothesis Testing: Set up and test hypotheses, focusing on metrics like True
Shooting Percentage (TS%).
Feature Engineering: Select key features and reduce multicollinearity for better
model accuracy.
Xiaotian Lin
Modeling & Evaluation: Build and tune regression models to predict player
salaries.
Results Analysis: Identify key factors affecting salaries and evaluate salary
inefficiencies (over/under-paid players).","https://www.kaggle.com/datasets/szymonjwiak/nba-advanced-boxscores-1997-2023?select=advanced.csv
https://www.kaggle.com/datasets/omarsobhy14/nba-players-salaries/data
We will merge and clean data sets Nba Player Salaries.csv and advanced.csv together.
The front set contains around 731k datas, while we only pick 4 years out of 1997 to 2024, so our considered data sets should be around 100k rows.",Approximately 100000 rows,"- Predict player salaries using advanced performance metrics to determine which
statistics most influence compensation.
- Analyze whether certain players are over- or under-paid relative to their advanced
metrics and create a model to identify these inefficiencies.
- Study the relationship between a player’s salary and their contribution to team
success, using metrics that indicate team impact, such as win shares or plus/minus
stats.
- Create a predictive model that could be used by teams to evaluate fair starting
salary offers for players based on their advanced stats and recent performance
trends.","NBA player salaries are measured in continuous numerical values (dollars), making it a
natural fit for regression models, which are specifically designed for continuous
outcomes.
The target variable in this project is NBA player salary. This represents the primary
outcome we aim to predict based on advanced box score statistics. By setting salary as
the target, we can quantify how well different metrics explain salary variations across
players, thereby gaining a more data-driven understanding of player valuation in the
NBA.
Yes, we will consider hypothesis testing.
1)
Objective: To determine if True shooting percentage (TS%) significantly influences
NBA player salaries.
Null Hypothesis (H0): TS has no significant effect on player salary.
Alternative Hypothesis (H1): TS has a significant effect on player salary.
2)
Objective: To identify if there are systematic discrepancies between predicted salaries
(from the model) and actual salaries, indicating potential over- or under-valuation.
Null Hypothesis (H0): There is no significant difference between predicted and actual
player salaries, indicating no systematic salary inefficiency.
Alternative Hypothesis (H1): There is a significant difference between predicted and
actual player salaries, indicating systematic salary inefficiency.","Given the complexity of advanced metrics, more complex models (e.g., neural networks)
might risk overfitting on historical data, leading to poor generalizability.
Many advanced statistics are correlated (e.g., PER, Win Shares, and Box Plus/Minus are
often related), which can lead to multicollinearity in regression models and make
interpretation difficult.",Oscar Eichmann
"Eda Orakci | eorakci@seas.upenn.edu

",Omar Ben Kaddour | omarbk@wharton.upenn.edu,Stefan Zaharia | szaharia@sas.upenn.edu,"Stefan: Data acquisition and pre-processing, ( sourcing social sentiment, mobility, and satellite data; ensures data is cleaned, standardized, and prepared for modeling)
Eda:  Model design and feature engineering; implements LSTM and Random Forests, 
Omar: Baseline modeling with traditional macroeconomic data and comparison analysis; will evaluate performance improvement from alternative data.
We decided to collectively write up the final report. 
","1.	Twitter API for sector-specific sentiment scores. Access via Twitter Developer Portal.
2.	Google Earth Engine for satellite imagery data on traffic near industrial sites. Access via Google Earth Engine.
3.	Google Mobility Data for activity tracking, especially retail and workplace visits. Access via Google COVID-19 Community Mobility Reports. Download region CSVs, open 2022_US_Region_Mobility_Report.csv
4.	FRED for baseline macroeconomic indicators (interest rates, inflation). Access via Federal Reserve Economic Data.

https://developer.twitter.com/en/docs/twitter-api
https://earthengine.google.com/
https://www.google.com/covid19/mobility/
https://fred.stlouisfed.org/
",700 000 rows,"The project seeks to determine if alternative data sources—social sentiment, industrial site traffic, and consumer mobility—provide predictive value for sector-specific stock volatility, complementing traditional macroeconomic data. If proven, the insights could refine short-term volatility forecasting models, providing early signals of market shifts at a sector level. The model produces a forecasted volatility score for each sector, indicating the expected variability in stock prices for specific timeframes. Using the scores, it can also provide an indication of whether volatility is expected to rise or fall (computing a Delta value), helping investors anticipate shifts. This is particularly relevant given the high-frequency nature of alternative data, making it valuable for quick investment decisions.","LSTM Neural Network: To capture sequential patterns over time across daily social sentiment, traffic, and mobility data.
Random Forest: For feature importance analysis, to identify which alternative and macroeconomic variables contribute most to volatility.
Baseline Multiple Regression: Uses only traditional macro indicators to serve as a control for assessing model uplift from alternative data.
Hypotheses:
1.	Increased social sentiment volume or polarity correlates with higher volatility in tech and consumer sectors.
2.	Higher traffic volume at manufacturing sites predicts increased volatility in industrial sector stocks.
3.	Variations in consumer mobility indices (e.g., retail visits) predict retail stock volatility.
","Challenges include integrating disparate data sources at a daily frequency, managing noise from social sentiment data, and computational demands for satellite data processing. LSTM complexity may also require substantial tuning to prevent overfitting given the variety of input features.",Oscar Eichmann
"Angelina Sali | asali@wharton.upenn.edu
","Ipek Obek | iobek@sas.upenn.edu
",Cole Christopher | colec25@sas.upenn.edu,"Angelina - data cleaning and pre processing + respective slides + introduction/background
Ipek - EDA + respective slides + description of challenges / Obstacles Faced
Cole - model building + respective slides + potential next steps / future direction",https://www.kaggle.com/datasets/meirnizri/covid19-dataset,"400,000","The dataset contains features about patients, their health history, COVID-19 test results, outcomes (e.g. hospitalized). We aim to predict, based on the health condition of the patient, if they will be hospitalized after testing positive for COVID-19. This could be useful for healthcare providers and hospitals to better allocate resources and for effective operations management. It can also help health providers identify patient who are seriously at risk for potentially adverse outcomes.  ","We are considering building various classification models including logistic regression, XGBoost, and RandomForest. Our target variable is whether the patient was hospitalized or not. ","Since our dataset is quite large, tuning the hyperparameters of our models may be time consuming so we need to have a good plan for how to tackle this. ",Oscar Eichmann
Evan Canis | ecanis@seas.upenn.edu,Kate Spencer | spkate@wharton.upenn.edu,Lucas Fernandes | flucas@sas.upenn.edu,"Lucas: hypothesis testing on variable significance, scaling/one hot encoding significant variables, random forest model, slides for presentation pertaining to these parts
Evan: visualizations, identifying and handling outliers, other data cleaning, logistic/ridge/lasso regression for classification, slides for presentation pertaining to these parts
Kate: work with correlated variables,  variable background/summary, address variable skew, gradient boosting model, and slides for presentation pertaining to these parts
",https://www.kaggle.com/datasets/gauravduttakiit/loan-defaulter/data,308000,"The goal of our final project is to predict individuals’ loanworthiness based on key personal and financial indicators using machine learning. Our final model should be able to predict if an individual will have difficulties paying or not. This project is interesting not only because it addresses a crucial aspect of creditor decision-making, but also because it aims to empower financial actors by providing insights into their debtor’s default profiles, potentially leading to more informed financial choices and higher recovery rates. It can also allow people with no credit history to access loans and financing.
","We are targeting a classification model as default (our target variable) is a binary outcome. Our target variable is 1 if the client is having difficulties paying their loan (i.e., late payments)
Models targeted:
- Logistic regression for classification
- Lasso/ridge regression
- Random forest regression 
- Gradient boosting regression

We will conduct hypothesis tests to assess the significance of indicators as a whole and the indicators individually in predicting loan default.
Null: There is no significant relationship between the financial indicators and the likelihood of loan default.
Test statistic: We will use the p-value in a permutation test to decide if all indicator coefficients are zero. For individual variable tests, we will use the p-value from a residual test to assess whether the variable is a significant predictor. Both the permutation and residual test p-values will be computed through simulation. 

","Will need to clean the data thoroughly, including scaling and removal of outliers
There are many potentially relevant columns so determining which features are most relevant for loan defaults will be challenging and will require careful analysis, including the removal of many columns
The dataset is imbalanced (2% defaults) which can lead to poor model performance on the minority class
Will need to balance model complexity and interpretability (i.e. random forests/gradient boosting models are accurate & more robust but may be more difficult to understand than a logistic regression which is less accurate but more interpretable)
Risk of overfitting data - due to the size of the dataset, we will need to implement regularization, cross-validating, and hyper-parameter tuning techniques
",Oscar Eichmann
"Jeffrey Li | jeffrli@seas.upenn.edu
",Ziao You | ziaoyou@seas.upenn.edu,Fanchu Zhou | fanchuzh@seas.upenn.edu,"Jeffrey Li
-	Data pre-processing
-	Design the aggressiveness score formula
-	Train model (TBD)
-	Write up report
Jasmine Fanchu Zhou
-	Clean data
-	Train models (TBD）
-	Conduct hypothesis testing
-	Interpret results with background knowledge in tennis tournaments
-	Write up report
-	Communicate(seeking help from) with Professor and TA
Ziao You
-	Data Cleaning
-	Feature Engineering
-	Train models (TBD）
-	Write up report
",https://github.com/JeffSackmann/tennis_atp ,96442,"In tennis, a player uses strategies to attack or defend. An ""attacking"" player actively tries to take control of the point by hitting aggressive shots, often moving forward towards the net to finish points quickly, while a ""defensive"" player prioritizes keeping the ball in play by playing from behind the baseline, waiting for the opponent to make a mistake, and using shots like slices and high lobs to maintain rallies and disrupt the attacker's momentum. 
Our objective is to provide suggestions to players on their aggressiveness of match strategy. More specifically, given a player,  their rivalry of the match, and match environment statistics, our model will output an aggressiveness score to indicate the level of hostility of the target player’s strategy. 
Besides great tennis techniques and philological conditions, tactics are also one of the most crucial aspects of winning a match. Many researchers and analysts are hired by the Top tennis players to create tactics for the player to implement. Usually, these groups of people study the historical data of match training statistics of both sides of the players. As time passes by, the amount of data needed to be studied accumulates fast, which may cause challenges to analysis with intuition. However, with our model, we can not only take players’ objective technical skills into consideration, but also measure their tactics against different opponents under different external circumstances, such as weather, court type, etc.","1. Data pre-process: 
For each tennis match, our dataset provides technical statistics for both the winner and the loser, including metrics such as the number of aces, first-serve points won, and more. Using these statistics, we propose to develop an ""Offensive Aggressiveness"" score for each player, calculated with a manually defined formula of
score= w1 * f1 + w2 * f2 + … + wn * fn + b
Where f represents a single technical statistical feature, and w represents the weight for each feature. It reflects the level of offensive play demonstrated by both the winner and loser. This score will then be standardized to a float between 1 and 100, where values near 1 indicate a conservative playing style and values near 100 indicate high offensive aggressiveness.
The offensive aggressiveness score of the losing player will be combined with additional objective factors—such as physical conditions, venue characteristics, and player rankings—serving as input features. The score of the winning player will serve as the target label. The goal is to design a regression model that predicts the offensive aggressiveness required to achieve victory. This model aims to provide insight into the aggressiveness level necessary for successful performance based on opponent tendencies and match conditions.
2. Baseline Model: 
To kick-start our analysis, we will develop a baseline using a multiple linear regression model. This foundational approach will provide initial insights into relationships within the data and help set a benchmark for evaluating the performance of more complex models.
3. Neural Network Model: 
To capture non-linear relationships within our dataset, we will implement a neural network model. By tuning the architecture, such as the number of layers and neurons per layer, we aim to improve predictive accuracy beyond what linear models can achieve. This model will adaptively learn complex patterns, which may reveal subtle interactions in the data. Additionally, regularization and batch normalization will be used to prevent overfitting and ensure the model’s generalizability.
3. Random Forest Model: 
For further exploration, we will apply a random forest model, an ensemble-based approach well-suited for handling diverse and potentially unstructured data. By constructing multiple decision trees and combining their predictions, the random forest can enhance the model’s accuracy and stability.
4. Loss Functions Choices for Model Eval: 
Mean Squared Error, Mean Absolute Error, R-Squared Score.

Our model is trained based on the historical data of matches played by two players as opponents. The model output will be strategy scores, of which the means and variances are parameters that can be conducted for hypothesis testing. An example of such a two-tailed test can be testing the mean strategy score of Player A when they are playing against Player B. In this case, our null hypothesis would be that the mean of the strategy score is equal to the observed score; the alternative hypothesis would be that the mean of strategy score is not equal to the observed score. Additionally, we expect some players may not have many records of competition in tournaments, so we may implement bagging and boosting to simulate a distribution of their strategy scores.","1.	Aggressiveness Score definition
The aggressiveness score is an essential score for our project. It stands for the players’ aggressiveness of their match strategies. We need to calculate the strategy score for both players on every match basis using their match statistics. The score will later be used for regression models to predict and give suggestions on the optimal strategy score in order to win the game. So calculating the strategy score will be crucial yet challenging, as it must encompass different play styles and translate complex strategies into a meaningful linear formula.
2.	Model Performance
Part of the features, such as the number of Aces or double faults, can only be known after the game is completed, which is dependent on the players’ offensiveness and defensiveness. So it is not logical to include the game statistics features in our training process. So in the classification process, the features we could use will be limited.  So it might bring an issue for us to improve the model performance. 
3.	Model generalizability across players
Since our data is collected over a long period of time and different players around the world, ensuring the models’ effectiveness of players and styles is challenging, particularly if data favors certain skill levels or approaches.
4.	Interpretability and Usability
For practical impacts, the model’s outputs should be interpretable and actionable, allowing players and coaches to understand and leverage the aggressiveness score to inform strategic decisions. We will need to know how different aggressiveness scores actually represent in the real game.
",Aeshon Balasubramanian
Harjasan Singh | harjasan@wharton.upenn.edu,Crystal Lu | cryslu@seas.upenn.edu,Sri Mutukula | mutukula@sas.upenn.edu,"Crystal Lu - Data cleaning and preprocessing. Responsible for ensuring that the dataset is clean, handling any missing values, and preparing it for analysis
Harjasan Singh - Responsible for generating initial visualizations and summaries to understand the dataset, including distributions of review lengths and scores.
Sri Mutukula - Analysis of review length vs. score. Responsible for conducting the main analysis to determine correlations between review length and scores, including creating scatter plots and calculating statistics.
",https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=reviews.csv,130k rows of reviews,"We intend to study the relationship between the length of text reviews and the scores given by users for different animes. The goal is to determine whether longer reviews correlate with higher or lower scores. Understanding this relationship can provide insights into user behavior and review dynamics, potentially helping anime creators and platforms enhance user engagement, and this project is interesting to all of us as anime fans.
","Since we are looking at correlations, we will use regression analysis. The primary target variable will be the score given in the reviews and the independent variable will be the length of the text review (number of words). 
Null Hypothesis (H0): There is no significant correlation between review length and review score.
Test Statistic: We will use Pearson’s correlation coefficient to evaluate the strength and direction of the relationship between review length and score.
Simulations: We may conduct a permutation test to assess the significance of our findings.
","Ensuring that the review text is consistently formatted and free from extraneous information could be challenging. We also have to make sure that we accurately interpret the results of the correlation analysis and ensure that we don’t make incorrect causal inferences. We must make sure to create clear and informative visualizations that accurately represent the data and findings.
",Aeshon Balasubramanian
Shivi Jain | shivij@seas.upenn.edu,Kelly Hu | hukelly@wharton.upenn.edu,Tara Kapoor | tarakap@wharton.upenn.edu,"Shivi Jain: clean up data into a usable file, perform simple linear regression
Tara Kapoor: perform hypothesis testing 
Kelly Hu: perform multilinear regression, compile content onto PowerPoint presentation for final recording
All: interpret and summarize findings, research implications",https://github.com/jamesqo/gun-violence-data,"900,000","We aim to determine how different factors (including gun_type, state, city_or_county, n_guns_involved, etc.) in gun violence incidents determine the total number of people affected, which is calculated from the number of people injured plus the number of people killed.

By examining variables such as gun type, location (state/city), and number of guns involved, we aim to determine which factors correlate most strongly with the total number of people affected (injured + killed) in these incidents. This analysis can help identify geographic patterns and contextual insights that could inform targeted gun legislation in the U.S..","Our target variable is the total number affected: the number of people killed (n_killed) plus the number of people injured (n_injured). We’ll start by analyzing (via hypothesis testing) how different factors (gun type, location, number of guns involved, etc.) correlate with the total number affected. 

To supplement this, we are considering using regression models (single factor and multiple factor regressions) to quantify how each factor that demonstrates a correlation in hypothesis testing impacts the total number affected in gun violence incidents. Some of our factors are categorical, so we may consider multi-logistic regression, Random Forest, decision trees, or other clustering algorithms. ","It may be difficult to conclusively determine correlation given the number of factors involved — existing gun laws varying between states, how fast police were able to respond, the exact location of the event of gun violence, etc.. Additionally, gun policies changed in different states during and after our dataset time frame (2013 - 2018), which may affect the accuracy of our conclusions and the relevance of them today.",Aeshon Balasubramanian
Aidan Crowley | acrowle@upenn.edu,Charlie Chang | chachang@wharton.upenn.edu,Matt Friedrichs | mattfri@seas.upenn.edu,"Aidan: Gather and clean data, create analytic dataset, background knowledge on institutional details of healthcare setting.

Matt: Exploratory analyses, regression and random forest models.

Charlie: Principal component analysis.

All: Interpret findings, create visualizations, prepare the final report and presentation.","National Clinician Downloadable File: https://data.cms.gov/provider-data/dataset/mj5m-pzi6
American Community Survey (Census Bureau): https://www.census.gov/programs-surveys/acs/data.html","2,604,871 rows (doctors) in National Clinician Downloadable File; 239,780 rows (census block groups) in ACS Census data","The objective of this project is to study the relationship between socioeconomic status of a ZIP code and the density of specialist physicians in that ZIP code. We would like to identify area-level predictors of access to various medical specialties (primary care, emergency medicine, etc.), such as income, education, etc. in that ZIP code. Additionally, we would like to predict what factors influence a physician to choose to practice in a low-income ZIP code. This project is interesting because it can contribute information on equitable access to healthcare. For example, if more disadvantaged areas predict lower density of specialists, then this could provide rationale to increase incentives for specialists to move to low-income areas, which could improve health outcomes.","We plan to run 4 models. First, we will train a classification model at the individual level to predict what specialty a person will choose based on gender, graduation year, degree type, and several medical school characteristics. We will also run two multivariate linear regression models: one using area-level data from the ACS (income, education, employment, home values, etc.) to predict density of specialists in that ZIP code, and one using individual-level data from the National Clinician Downloadable File to predict the socioeconomic status of a ZIP code in which a physician will practice. We will also run an alternate specification of the two multivariate linear regression models as random forest models to compare the performance. Finally, we will conduct a principal component analysis of census variables to determine which weigh more heavily onto the density of specialists. We will conduct residual-based simulations to separately test null hypotheses that each coefficient is zero for each of our two multivariate linear regression models.","One concern is not having as many variables as we would like at the individual level to predict specialty choice, as well as outside factors affecting the choice of that specialty (e.g., its salary). However, we can acknowledge that in our limitations. Another challenge is multicollinearity - many of our area-level variables are likely correlated with each other, which is why we plan to conduct the principal component analysis.",Aeshon Balasubramanian
Mengqi Liu |mengqi47@seas.upenn.edu,Yulun Wu | wyulun@seas.upenn.edu,Zuyu Wei | weizeyu@seas.upenn.edu,"Each of us will be responsible for implementing one of the three selected models, along with conducting tuning and visualization for that model. Mengqi is responsible for exploratory data analysis and writeup, Yulun is responsible for data pre-processing and feature engineering. Zeyu is responsible for hypothesis testing. ",https://www.kaggle.com/datasets/kelvinkelue/credit-card-fraud-prediction,"555,719 rows and 22 columns ","The main goal of this project is to develop a model that could identify fraudulent credit card transactions by leveraging demographic data of cardholders and historical transaction data, including temporal, geographic, and merchant information. Our goal is to extract features that indicate whether a transaction is likely to be fraudulent. This model will serve as a proactive tool for detecting potential fraudulent behavior, thereby reducing financial losses for stakeholders and financial institutions. By implementing this fraud prediction model, financial institutions will be empowered to make better data-driven decisions when assessing transactions for fraud. ","For our project, we are using a classification approach to identify fraudulent transactions, with the target variable is_fraud (1 indicating fraud, 0 indicating a legitimate transaction). We plan to implement three models: Logistic Regression as a baseline model for interpretability, Random Forest for capturing feature interactions and handling imbalances, and XGBoost for maximizing predictive power and mitigating false negatives.

Additionally, we will conduct hypothesis testing on features such as transaction amount, transaction time, merchant information and transaction category, geographic distance, cardholder’s age, gender, and city population density to examine their associations with fraudulent behavior. These tests will provide insights into feature selection and enhance model interpretability.","We might encounter challenges when extracting valuable information from columns like streets and jobs, which contain many unique text values. We will also need to address dataset imbalance due to low frequency of fraud. Balancing false positives and false negatives will be essential, as both have significant implications. ",Aeshon Balasubramanian
Duriya Rehan | duriya@wharton.upenn.edu,Claire Zhao | clairezz@wharton.upenn.edu,Vincent Tiu | vincetiu@wharton.upenn.edu,"Claire Zhao: data visualization (mapping, graphs)
Duriya Rehan: data ingestion (cleaning + formatting + conversion to SQL dataset)
Vincent Tiu: data processing (query design, regression analysis)",https://www.rrc.texas.gov/search/?q=production+data,1516269,"We want to visualize the oil and gas industry across Texas. Texas produces the most oil and gas out of any US state, and the continued production of energy is essential for meeting increased US energy demand, especially with the rise of data centers and AI applications running on them. As such, visualization may also indicate potential opportunities for new drilling operations or downstream business strategies.

The technical objective of this project is twofold. Firstly, we would like to visualize existing well data on a map to allow users to better spatially understand the distribution of wells across the state. For an individual well, we would also like to generate graphs displaying production data over time. Secondly, we would like to run regressions on different attributes of the well to understand the state of a given well, as described below. We will not be focusing on forecasting a production time-series, but more on understanding at a high level the current state of a given well and how it compares to other wells in the area.","We plan to use regression analysis to predict the cumulative oil / gas reserves of a well.

Independent variable: total well reserve
Dependent variables: well production in first 3, 6, 9, 12, last 12 months, length of well operation, location

We would like to use hypothesis testing on each of the dependent variables to understand whether they actually impact total well capacity. The null hypothesis is that the independent variable does not depend on the dependent variable, and the test statistic is the correlation between the independent and dependent variables. We will use simulations to generate data in the null world and compare the data to the actual data.","The biggest obstacle with this project will be cleaning and formatting the data. This is a very large dataset with an old API, so extracting and cleaning the data may create significant roadblocks. Once we ingest the data, the data processing may also be challenging (perhaps training a simple model will be better to factor in each input), and finally visualization will be more challenging than usual given the mapping aspect of the project.",William Qi
Michael Li | limich19@seas.upenn.edu,Chaelsey Park | chaelsey@seas.upenn.edu,Hemosoo Woo | hemosoo@seas.upenn.edu,"Michael Li: Working on collecting the data and preprocessing so the data is cleaned for analysis.
Hemosoo Woo: Will work to analyze the data and building the linear regression model to determine a relationship between the data and our objective, the total number of covid-19 cases based on a country’s specific factors. 
Chaelsey Park: Will work on interpreting data using visualizations and lastly determine what limitations we may have run into and adjust our model find improvements based on those observations.",https://www.kaggle.com/datasets/assemelqirsh/covid19-dataset?select=covid_data.csv,429436,"The objective of our project is to analyze global COVID-19 data to uncover patterns and correlations that will provide valuable insights into the pandemic’s progression and the effectiveness of response measures. Specifically, we aim to understand the relationships between various traits and infection or mortality trends across different regions. We hope to be able to predict cases for various countries and apply correlations to different areas. This project is interesting because our findings will contribute to a better understanding of the dynamics of the pandemic and may inform data-driven decisions in similar future health crises.
","We are considering using a regression model to calculate how features like a country’s poverty level, GDP, and diabetes levels would affect the level of covid-19 cases. We are testing if there is a correlation between the two and later our objective is to see whether or not we can supply another country to predict its total covid-19 cases. Our target variable is the total number of covid-19 cases in a particular country. By splitting our training and testing sets, we will determine if these factors impact our result.","We might run into problems with missing data in the csv file or a lack of data for the specific factors mentioned above (gdp per capita, obesity, median age, etc…). If there is a lack of data, then our model won’t have enough information to accurately predict the total cases.
Lastly, normalizing and scaling our data might be challenging because each factor can vary greatly in magnitude, so cleaning our data properly is an extremely important step in this process.
",William Qi
Muyu He | muyuhe@seas.upenn.edu,Yihuai Zhang | yihuaiz@seas.upenn.edu,Shutong Jiang | ryjiang@seas.upenn.edu,"Muyu He: data preprocessing. This includes scaling the data to a desirable range, converting categorical data, and creating stratified datasets for training and validation.

Yihuai Zhang: model training. This includes selecting the models and hyperparameters and fit the models to the dataset.

Shutong Jiang: model evaluation. This includes choosing evaluation metrics, validating the models on the data, and doing ablation studies of various models and configurations.",https://archive.ics.uci.edu/dataset/526/bitcoinheistransomwareaddressdataset,"2,916,697 rows","We choose to study ransomware identification because we want to find underlying patterns of ransomware payments. Since bitcoin transactions are secretive by nature, it is harder to track ransomware attacks than it is for other currencies. Training a classifier on publicly available information like the transaction types and patterns would help accurately identify ransomware attacks. This can in the long term improve security of transactions on crypto platforms.","Since the goal is to identify the type of ransomware operations, e.g., innocent payments or 'montrealAPT' or 'PrincetonCeder', this is a classification problem. We can adopt a wide range of classifiers and use a validation set to compare their performances. Candidate models include K Nearest Neighbors, logistic regression, and decision trees. The target variable would be the type of ransomware operations, which is one of the 29 labels.

There are a few null hypotheses we can test. One of them is that the relation between the number of merging transactions and whether an address is involved in ransomware activities is arbitrary. We will apply residual testing to see the impact of removing that feature. We want to see whether given the null hypothesis, we are likely to observe as strong a correlation as the one we get in training.","The potential obstacle of this project is that the dataset has an uneven distribution of labels. Among the 2,916,697 samples, 2,875,284 are innocent payments. So there is an overwhelming majority of innocent payments and a small portion of 28 ransomware operation types. The challenge would be to incentivize the model to reduce the number of false negatives (considering all ransomware operations as the positive class). Using stratified datasets for training and validation, calculating recall and F1 score, and up-weighting the loss of positive classes are techniques we can use to address this class imbalance problem.",William Qi
Haohai Liu | haohai@seas.upenn.edu,Xingqi Wei | xingqiw@upenn.edu,Pairan Xu | pairan@upenn.edu,"Xingqi Wei would be responsible for data preprocessing and EDA, Haohai Liu would be responsible for doing multilinear regression, and Pairan Xu would be responsible for doing random forest regression. After completing each part, we would combine our outcomes and complete the annotated notebook.",https://www.kaggle.com/datasets/diishasiing/revenue-for-cab-drivers/data,6405008,"We want to do an analysis of factors affecting taxi fare in New York, including location, time, distance, etc. Our ultimate goal is to build an effective regression model to predict the cost of taxi rides in New York. This is interesting because as consumers, we want to understand the factors that affect the price of a taxi so that we can achieve cheaper taxi rides. it is hard to know how much the taxi ride would cost before the ride. If we can build such model, we can know the taxi fare beforehand and compare it with other transportations to save money.","We would build two model. One is multilinear regression model which is simpler, and a random forest regression model which is more complicated. We also want to compare their outcome and see which model have better performance. Our target variable is the total fare amount, so regression model would be better. We plan to test whether each variable has an impact on the fare amount, and we can do a residual test to investigate.","One possible challenge is that the datasize is too large (over 600 millions rows) for model to analyze, so we may need to truncate some of the rows. Another problem is that some columns may be linearly dependent, so we need to study the correlation of variables and find variables that are linearly independent for future analysis.",William Qi
Siyuan Gao | ryangao@seas.upenn.edu,Ruikun Hao | harryhao@seas.upenn.edu,Ziqi Hua | ziqihua@seas.upenn.edu,"Ruikun Hao: Responsible for data wrangling, data cleaning, and feature engineering. Also, in charge of presenting the dataset in the project proposal and initial data analysis.
Ziqi Hua: Conducts exploratory data analysis (EDA) and statistical analysis, leads model selection, and implements baseline model.
Siyuan Gao: Focuses on model training, hyperparameter tuning, and visualization. Responsible for assembling final deliverables, including annotated notebook and presentation.",https://www.kaggle.com/datasets/hugomathien/soccer,187000,"Our project aims to predict a football player's overall rating based on their skill attributes and physical characteristics. By analyzing these metrics, we can identify which attributes are most predictive of a player’s performance. This project could offer valuable insights for football clubs and scouts to identify promising players based on skills, helping streamline recruitment and training strategies.
The analysis will also be useful for fans and fantasy football participants who want to better understand player capabilities, providing a unique perspective on player metrics beyond popular statistics.
","Modeling Plan:
Target Variable: overall_rating
Modeling Approach: This is a regression task. We plan to experiment with various regression models, including:
Baseline: Linear regression model for straightforward interpretability.
Advanced Models: Random forest regression, gradient boosting, and neural networks to capture complex relationships in the data.
Feature Selection: We’ll use techniques such as recursive feature elimination (RFE) or feature importance analysis to identify the most influential attributes on player ratings.

Hypothesis Testing:
Hypothesis: Higher values in acceleration, sprint_speed, and dribbling contribute significantly to a player’s overall_rating.
Null Hypothesis: The attributes acceleration, sprint_speed, and dribbling do not significantly impact a player’s overall rating.
Test Statistic: We will mainly apply residual test to test whether one particular coefficient is zero. We will also use p-values and confidence intervals from regression coefficients, with additional support from permutation tests if needed. ","Missing Data: Some entries might contain missing values, requiring imputation or data cleaning methods to maintain dataset integrity.
Correlated Features: Many player attributes could be highly correlated, potentially affecting model performance. We plan to address this with techniques like PCA or correlation-based feature selection.
Model Complexity: More complex models (e.g., neural networks) may face overfitting issues. We’ll manage this with cross-validation and regularization.",William Qi
Maximilian Chuang | mchuang1@sas.upenn.edu,Max Li | maxli27@seas.upenn.edu,Praneel Varshney | pvarsh@seas.upenn.edu,"Max Li - data cleaning, model training (regression & classification) and interpretation
Max Chuang - exploratory data analysis, model training (regression & classification) and interpretation
Praneel Varshney - visualization, model training (regression & classification) and interpretation",https://www.kaggle.com/datasets/usdot/flight-delays?select=flights.csv,1048576 rows,"We intend to study the relationship between airports, flights, delays, and cancellations by analyzing a dataset of flight delays/cancellations from the US Department of Transportation. The ultimate objective is to determine if certain airports, flights, departure times, seasons, etc. affect the likelihood of a flight being delayed or canceled. This is interesting because in the mainstream media, delays and cancellations are typically discussed as a function of a particular airline, severe weather, or some other significant event, while in reality it may be the case that other less commonly discussed factors may be correlated with delays and cancellations. Of course, we may still investigate more basic relationships such as airline vs flight delays, etc. ","We are mainly interested in classification models, since we would like to determine whether certain factors increase the likelihood of delays or cancellations. There are several models that we could use such as decision trees/random forest or gradient boosting machines (XGBoost). The variables (see dataset) CANCELED and DEPARTURE_DELAY are most likely to be our target variables. If we include DEPARTURE_DELAY as a target variable, we could perform regression analysis on that.

We are considering hypothesis tests, such as:

Is there a relationship between the distance of a flight and the likelihood of delays/cancellations (could be 2 separate tests)
Null: Distance does not affect likelihood of a flight being canceled/delayed
We can use a bootstrapped data set to test whether the coefficient of the distance variable is 0. We will come up with a p value accordingly.

	Does the season affect the likelihood of delays/cancellations?
Null: The season does not affect the likelihood of a flight being delayed or canceled.
We can use the dates of the flights to determine the season (binary encoded, separate columns) for each flight, then we can do a test to see if the coefficients of those variables are 0. We will come up with a p value accordingly.

The direction that we decide to go will also be heavily dependent on findings from our trained models. If there are significant results from a feature we did not expect (perhaps DAY_OF_WEEK), we plan on doing hypothesis testing on the relationship between our target variables and this predictor.","Without knowing all the content of the course, it may be a little difficult to know exactly what things we need to implement (or how) ahead of time, which may affect planning a little bit. Aside from that, I don’t think that we should have too many issues. It will be important to familiarize ourselves with the data and clean properly ahead of time so that we do not need to deal with data-related issues later down the road.",Term Taepaisitphongse
Lisa Xiong | jianjunx@seas.upenn.edu,Maggie Zhang | manzhang@seas.upenn.edu,Taiyu Geng | taiyu@seas.upenn.edu,"Feature engineering - Taiyu
EDA - Maggie
Logistic Regression - Lisa
KNN - Maggie
Random Forest - Taiyu
GBDT - Lisa",https://www.kaggle.com/competitions/playground-series-s3e24/overview,"159,000","Our objective is to predict whether a candidate is a smoker from their basic information, and potentially identify the features that have the highest correlation to smoking. The findings of this model can be used in criminology to identify suspects, and be applied to demographic statistics as well.","We’re doing binary classification to predict whether a candidate is a smoker or not. Some models that we’re interested in are: K-Nearest Neighbors, Random Forest, Gradient-Boosting Decision Trees, and (Weighted) Logistic Regression. If there’s enough time, our current plan is to try all of these models to see which one provides the most accurate predictions on our data. We don’t plan to use hypothesis testing since it does not fit into the scope and goal of the project.","1. It might be difficult to determine which features are correlated to whether a candidate is a smoker.
2. There are a lot of models that we can apply to the data; we would like to experiment with some of them before choosing our final models.
3. Some of the features are clinical measurements, which might be hard for us (without medical background) to understand and interpret.
4. There could be a distributional shift in the dataset since the features in the dataset may not be identical to the original dataset. This may cause the modeling and evaluation to be complex.",Term Taepaisitphongse
"Shruthi Arunkumar|shru235@seas.upenn.edu
","Meera Desikan|meerades@seas.upenn.edu
",Sanskriti Sarkar|	ssans@seas.upenn.edu,"Shruthi Arunkumar - KNN, GBM, Hypothesis Testing
Meera Desikan - Random Forest, GBM, Hypothesis Testing
Sanskriti Sarkar - SVM, GBM, Hypothesis Testing ","Hospital Infections Dataset-

https://www.kaggle.com/datasets/muhammadfaizan65/hospital-infections-dataset?resource=download",222865 rows;16 columns;69.57 MB,"The goal of this project is to study hospital infection data to see what factors make hospitals perform better, worse, or the same compared to national benchmark for infection rates. By looking at how hospitals perform across different types of infections, locations, and other hospital features. We hope to find patterns that could guide better infection prevention strategies.  Infections picked up in hospitals are a major health issue. This makes the project interesting because the analysis performed helps understand infection rates better.","Models- KNN, SVM, Random Forest, Gradient Boosting Machines.
We aim to perform classification on the dataset.
Target Variable-""Compared to National""

1. Effect of Hospital Location on Infection Scores
H0: There is no difference in infection scores between urban and rural hospitals.
H1: There is a significant difference in infection scores between urban and rural hospitals.

2. Impact of County on Infection Rates
H0: Infection rates do not differ by county.
H1: Infection rates differ by county

3. Comparison of Lower and Upper Confidence Limits
H0: The lower and upper confidence limits for infection scores are equal.
H1: The lower and upper confidence limits for infection scores are not equal.","The obstacles we anticipate in this project are significant data quality challenges such as incomplete data, inconsistent formats, and outliers that can distort analyses. Additionally, human errors that were made during data entry and duplicate records can lead to inaccuracies. Data cleaning poses further challenges, including identifying and correcting missing data, transforming formats, and ensuring consistency across sources, all while navigating software limitations. ",Term Taepaisitphongse
Ravi Raghavan | rr1133@seas.upenn.edu,Tricia Lobo | lobotr@seas.upenn.edu,Mattis Dalsaetra Oestby | mdals@seas.upenn.edu,"Ravi Raghavan: 
- Develop Logistic Regression baseline model to determine influence of various factors(e.g. race, location) on the likelihood of an arrest, given a stop and frisk. 
- Geospatial analysis of stop and frisk[e.g. Folium map to display data insights]
- Hypothesis tests to determine correlation between location and stop and frisk rates

Tricia Lobo: 
- Develop XGBoost and Random Forest models to determine influence of various factors(e.g. race, location) on the likelihood of an arrest, given a stop and frisk 
- Hypothesis tests to determine correlation between race and stop and frisk rates
- Hypothesis tests to determine correlation between stop and frisk and crime rates.

Mattis Oestby: 
- Develop Neural Network model to determine influence of various factors(e.g. race, location) on the likelihood of an arrest, given a stop and frisk
- Hypothesis Test to determine correlation between 2013 Floyd v. City of New York ruling and stop and frisk rates. 
-  EDA to develop Data Cleaning approach, Feature Engineering, and Data-Preprocessing. Also will be responsible for merging 2003 - 2023 data as the NYPD changed the column names and data format in 2017 onwards. ","Stop and Frisk Incident Data: https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page

NYC GeoJSON data(i.e. this basically shows each neighborhood in NYC allowing me to develop GeoJson maps): https://www.nyc.gov/site/planning/data-maps/open-data/census-download-metadata.page

Census Data for NYC Neighborhoods(allows me to compute proportion of population subjected to stop and frisk(i.e. stop and frisk rates) ): https://www.nyc.gov/site/planning/planning-level/nyc-population/2020-census.page [This is the Main NYC Census Page]. From Here, click ""Dynamics of Racial/Hispanic Composition in NYC Neighborhoods"". On the bottom of this page, in the section titled ""Resources"", there is a link to the ""Decennial Census data"" used in the report. This data contains census information from 2010 and 2020. Very useful stuff :) 

Note: Census is only done every 10 years. So we are limited to using 2010 and 2020. 

2000 - 2023 Historical New York City Crime Data: https://www.nyc.gov/site/nypd/stats/crime-statistics/historical.page ","Stop and Frisk Incident Data(2003 - 2023): 5,151,808 rows by 82 columns, Census Data: 262 rows(corresponding to 262 neighborhoods in NYC) by 98 columns, NYC Crime Data: Has Crime Data for ~80 Precincts(rows) across NYC from 2000 - 2023(24 columns). Dataset is Aggregated, hence it is very small. However, this is ONLY being used for 1 of our Hypothesis Tests, NOT for the remaining parts of this project","This project aims to study and explore several key objectives related to stop-and-frisk practices in NYC. We will analyze the racial distribution of individuals affected by these practices over time, focusing on the impact of the 2013 Floyd v. City of New York ruling. We will investigate how metrics for stops, frisks, arrests, use of force, etc have shifted across racial groups since this ruling. 

Additionally, we will assess the efficacy of stop-and-frisk in deterring crime by examining its relationship with crime rates, particularly in the context of policy changes after the 2013 ruling. Geospatial visualizations will help us track how stop-and-frisk practices have varied across NYC neighborhoods. Finally, we will develop predictive models to evaluate the likelihood of an arrest following a stop, comparing trends before and after the Floyd decision to understand its broader implications.

This project is particularly interesting to our group because it addresses critical social issues surrounding policing practices and racial equity. Additionally, by examining the implications of the 2013 Floyd v. City of New York ruling, we understand the influence of policy on policing practices across various communities. Finally, our use of predictive modeling and geospatial analysis allows us to provide data-driven insights that can inform future policies and promote more equitable law enforcement practices.","In our project, we will use various classification models to determine whether a stop and frisk leads to an arrest based on individual and situational factors. 

Our target variable is a binary variable(1 if the suspect was arrested and 0 if the suspected was NOT arrested). In our dataset, one of the columns is ""SUSPECT_ARRESTED_FLAG"" which is 1 if the person was arrested following the stop and frisk and the value was 0 if the person was NOT arrested following the stop and frisk

We will start with logistic regression as a baseline model for its interpretability, assessing influences like age, race, and location on the likelihood of arrest, given a stop and frisk. Logistic Regression is typically used in classification tasks. Let me explain why. Logistic regression is a powerful tool for classification because it estimates the probability of a data point belonging to a specific class, allowing for clear decision-making based on probability thresholds. We are working with a binary target variable and we know that logistic regression will create a linear decision boundary between classes, which works well for linearly separable data and provides straightforward interpretability. Additionally, logistic regression can include regularization to prevent overfitting, making it a robust and reliable choice for many classification problems.

We will then use Random Forests for the following reasons: 
1. It provides insights into feature importance, helping to identify which variables most significantly influence the classification outcome 
2. By taking the most ""popular"" classification among multiple decision trees, Random Forest reduces the risk of overfitting to the training data, leading to better generalization on unseen data.
3. Random Forest's tree-based structure can also help us capture complex non-linear relationships in the data. This would be really useful for us!

Additionally, our group wants to use XGBoost due to its residual learning capability, which allows the model to iteratively improve by focusing on the errors made by previous iterations. This approach enhances predictive accuracy and makes XGBoost particularly effective for capturing complex patterns in the data, ensuring robust classification performance. Also, we know that XGBoost implements regularization techniques (L1 and L2) that help prevent overfitting, making it suitable for this dataset which could have some outliers(i.e. due to reporting inaccuracies from NYPD officers). Also, XGBoost’s tree-based structure can capture complex non-linear relationships in the data. This would be really helpful for us!

Finally, we will also explore neural networks to analyze complex relationships in our large dataset. So the interesting part of neural networks is we know that the universal approximation theorem states the following: The Universal Approximation Theorem states that a neural network with at least one hidden layer of a sufficient number of neurons, and a non-linear activation function can approximate any continuous function to an arbitrary level of accuracy. 

So, given a relationship between the outputs and inputs(i.e. y = f(x)), a neural network should, in theory, be able to estimate f. Due to the complexity of our data(i.e. we have more than 80 columns), we are concerned that the aforementioned models just won't be able to fit to our data. Hence, we strongly believe that using a neural network would be beneficial in our case. 


Our hypothesis tests will investigate the correlation between race and stop and frisk rates, the impact of the 2013 Floyd ruling on stop and frisk rates, the correlation between situational factors(e.g. location) and stop and frisk rates, and the correlation between stop and frisk and crime rates.

Overall we aim to identify key predictors of arrests, following stop and frisks, understand the effect of various factors on stop and frisk rates, and overall, inform equitable policing practices. ","1. Personal and Political Biases: Given the political climate around stop and frisk, our group acknowledges that individual biases can influence how we interpret the data. A challenge is to avoid unconsciously favoring results that align with our personal and political beliefs, potentially leading to skewed analyses and conclusions

2. Interpretability of Complex Models: As we explore advanced models such as neural networks, we may face challenges in interpreting the results. While these models can provide high predictive performance, their complexity may make it difficult to actually interpret what the factors are that influence arrest likelihood, following a stop and frisk

3. Feature Selection and Engineering: Identifying the most relevant features that influence arrest outcomes can be challenging. Given the complexity of the dataset, we will need to conduct thorough exploratory data analysis (EDA) to understand relationships between variables and decide which features to include in our models. Ineffective feature selection could lead to suboptimal model performance.

4. Ensuring the quality and completeness of the dataset will be a significant challenge. We may encounter missing values, inconsistencies, or inaccuracies within the data that can hinder our analyses and affect the reliability of our predictions. An important thing to keep in mind is that NYPD officers are not trained for data analysis. Hence, the dataset may have the aforementioned quality and completeness issues we discussed. Addressing these issues will require careful data cleaning and preprocessing",Term Taepaisitphongse
Val Tham | yunxint@wharton.upenn.edu,Pak-whan Kanjanakosit | knicha@seas.upenn.edu,Jean Ee Chua | jeanchua@wharton.upenn.edu,"Pak-whan: Data wrangling and exploratory data analysis (EDA)
Jean: Feature engineering and model implementation
Val: Model evaluation, hyperparameter tuning, and documentation",https://www.kaggle.com/datasets/shriyashjagtap/e-commerce-customer-for-behavior-analysis,250000,"The project aims to analyze customer purchasing behavior in an e-commerce context. 

Objectives:
We intend to identify which customer demographics and product attributes have the most significant impact on total purchase amounts. Understanding these relationships will enable businesses to tailor their offerings more effectively.
Through the analysis, we will segment customers based on their purchasing behavior, allowing for targeted marketing campaigns that resonate with specific demographics. This segmentation aims to foster personalized marketing efforts that increase customer loyalty and engagement.
By uncovering trends and patterns in customer behavior, the project will provide recommendations for optimizing marketing strategies. This may include adjustments in product promotions, pricing strategies, and customer engagement initiatives.

As e-commerce continues to grow and evolve, understanding consumer behavior is crucial for businesses seeking a competitive edge. Thus, by leveraging data analysis, we can derive insights that businesses can use to adapt to dynamic consumer trends and preferences.
","Types of Models: 
Regression Models: Linear Regression, Decision Trees, Random Forests
- Target variable: Total purchase amount
- Linear Regression: Product price, quantity and age as independent variables
- Decision Trees: Product price, quantity and age (continuous); product category, gender and payment method (categorical) as independent variables 
- Random Forests: similar to decision trees

Classification Models: 
Logistic Regression 
Target variable: Categorical variable created from Total Purchase Amount (e.g., classifying customers into high spenders vs. low spenders based on a defined threshold)
Independent variables: 
- Product Price (continuous)
- Quantity (continuous)
- Age (continuous)
- Product Category (categorical, one-hot encoded)
- Payment Method (categorical, one-hot encoded)
- Gender (categorical, one-hot encoded)


To test the hypothesis, we will perform simulations using bootstrapping methods to evaluate to generate confidence intervals for the total purchase amounts for each of the 4 hypotheses.

Hypothesis 1
Null Hypothesis: There is no significant difference in the average total purchase amount across different product categories
Alternative Hypothesis: Customers who purchase products in different categories (e.g., electronics) have a statistically significant different average total purchase amounts.
Test statistic: F-test or ANOVA test to compare the mean total purchase amounts for different product categories.

Hypothesis 2
Null Hypothesis: There is no significant difference in the average total purchase amount based on payment methods.
Alternative Hypothesis: Payment methods have statistically significant different average total purchase amounts, with some payment methods (e.g., credit cards) leading to some payment methods having higher average purchase amounts.
Test Statistic: t-test or ANOVA to compare the mean purchase amounts across different payment methods.

Hypothesis 3
Null Hypothesis: There is no significant difference in the frequency of purchases or average total purchase amount across different age groups.
Alternative Hypothesis: There is a significant difference in the total purchase amount between different age groups.
Test Statistic: Chi-squared test for purchase frequency comparison and ANOVA for comparing average purchase amounts.

Hypothesis 4
Null Hypothesis: There is no significant difference in the average total purchase amount between male and female customers.
Alternative Hypothesis: Male and female customers have a statistically significant average total purchase amount compared to female customers.
Test Statistic: t-test to compare the mean total purchase amounts by gender.
","Feature Engineering with a Small Number of Features:
Challenge: With only a limited number of attributes, improving model performance through feature engineering might be challenging. Adding too many interaction terms or derived features could lead to overfitting.

Handling Imbalanced Data:
Challenge: Certain customer segments or spending categories may be underrepresented, making it challenging to build robust models. This could skew the results and affect the generalizability of findings.
",Term Taepaisitphongse
Wesley Liu | wesliu@seas.upenn.edu,Seohyun Park | sampark@sas.upenn.edu,David Lee | ddavidl@sas.upenn.edu,"We'll split each part of the project pretty evenly. We can each explore different models (baseline model, non-baseline, etc.). For EDA and hypothesis testing we can each explore different directions in the data. Once we figure out more details after exploring the data, we'll split up the tasks across each member.",https://www.kaggle.com/datasets/asaniczka/clash-of-clans-clans-dataset-2023-3-5m-clans?resource=download,"3,500,000","We intend to study the relationships between different clan features (average member levels, required townhall levels, description, etc.) and the performance of those clans in war (the percentage of wins they have compared to total number of wars participated in). The ultimate objective is to create a model that can predict the win percentage, as well as analyze general trends. This project is interesting because it provides insight into how players collaborate and compete in virtual environments, and how these player-decided factors impact competitive play. For example, does being more restrictive in who you let into the clan actually result in better competitive performance?","Some models that we are considering are XGBoost, Random Forest, and a simple linear regression model as a baseline. These models would be regression because we are predicting a win percentage. The target variable is the win percentage in war. We do plan on testing hypotheses, such as whether higher war frequencies correlate with higher win percentages, or whether higher clan requirements (ex. Town Hall level) correlate with higher win percentages. To test these hypotheses, we could use simulation.","One challenge we anticipate with this project is the dataset size, since there are 3.5 million rows. We may need to reduce the dataset size or figure out methods to quickly process this large amount of data. Some other challenges are extracting meaning from the clan descriptions for use in our models, and also figuring out a way to deal with clans that may have a low number of wars participated in (since then the win percentage may be less reliable).",Term Taepaisitphongse
"Jaime Walter Pérez | jwalterp@seas.upenn.edu
",Dhruv Bhargava | dhruvb05@seas.upenn.edu,Benjamin Ham | benjiham@seas.upenn.edu,"Jaime - 
Clean and preprocess data, dividing Chicago into mapped areas with encoded area codes.
Presentation video editing.
Dhruv - 
First null hypothesis
Presentation slides, and script.
Benjamin - 
Create color-coded maps and heatmaps to visualize accident risks by conditions.
Second null hypothesis test.
This plan is very flexible. We will probably work together for many of these things.
For the main model training, which is the most complex part, we will also be working together and discussing while we code.


",https://data.cityofchicago.org/api/views/85ca-t3if/rows.csv?accessType=DOWNLOAD,"(880000, 48)","We have a dataset on car accidents reported to the police in Chicago. First, we will do data cleaning and some preliminary data analysis. The goal is to associate, given weather, time and location (by areas), how much those conditions affect the severity or frequency of car crashes. We will divide the map into squares, and make a color map given different inputs. This means we will convert the latitude and longitude columns into discrete area codes. An area will be more yellow if the input conditions increase the probability of accidents in that area more than in other areas. It could help city planners determine which places have road design problems under what specific conditions (as we predict the accidents happening in those conditions). For example, if we input rain during the day and output the predicted accidents across the map we could highlight locations where the roads are slippery or the speed limit too high, while a clear night as input could highlight locations that are poorly illuminated. It could also highlight what places are made dangerous for cyclists by the snow in which specific regions. ","We will build a predictive machine learning regression model. For each area location within Chicago, we will build a model that, given weather, time of day, speed limit (and maybe other variables), predicts the risk that area has (either by the expected number of crashes or the expected number of injuries) - To output a coloured map of risk of accidents per region in Chicago. Then we will perform two types of null tests. The first one, for every area, we will see if in the null world where there is no relation between the conditions and the risk, we can still ever get risk values as high as the ones we got in reality. The second one, we will shuffle the locations and then perform the training for each area, and see if the distribution of locations we got is significant. Here, the target variable will be the variance of all the values of risk we have found in real life.","The biggest problem we could face is that there is no significance between the input conditions and the outputted risk of accidents in the various regions of Chicago, or that, even though we have 800,000 rows, which seems like a lot, we don’t have enough info to divide into enough areas. Hopefully this is not the case :) . Other issues include data sparsity, interaction effects, and choosing grids. The way accident details are reported can vary, leading to inconsistencies. For example, the accuracy of location data might be poor or overly general in rural areas compared to urban areas.
",Steven Su
Ji Hyun Kim | jhkim10@seas.upenn.edu,Sabrina Weng | tongweii@seas.upenn.edu,Jeongmoon Choi | cjmchoi@seas.upenn.edu,"Jeongmoon will handle background research and EDA. Sabrina and Ji Hyun will collaborate on modeling and hypothesis testing. All members will contribute to the conclusion.
","Jeongmoon’s research data.
Jeongmoon’s lab is as follows.
https://patelgroup.seas.upenn.edu/","The dataset consists of 50,000 rows and 10 columns. ","Surface Ice-philicity, its propensity to nucleate or bind to ice, plays an important role in various biological and technological applications. This project aims to predict ice-philicity based on protein structures by analyzing 50,000 data from protein-heavy atoms. We aim to build a predictive model to identify the ice-philic protein region from its structure. 
Our study would provide insights into protein-ice interactions and inform the design of materials that attract or repel ice. This research has broad potential applications, including improving cryopreservation, advancing antifreeze materials in the food industry, and developing anti-icing materials for the aerospace industry.
","Since ice-philicity is a binary variable(T/F), we will use binary classification models including logistic regression, random forest, gradient boosting machines, and support vector machines. Model performance will be evaluated using accuracy, precision, recall, F1-score, and ROC-AUC.
We will conduct hypothesis testing to assess whether certain structural or chemical features significantly impact ice-philicity:
- Null Hypothesis: Protein structural features do not significantly affect ice-philicity.
- Test Statistic: Differences in mean feature values between ice-philic and ice-phobic atoms or logistic regression coefficients.
- Bootstrapping: May use to construct confidence intervals and assess feature significance by resampling the dataset to create distributions of test statistics.","Imbalance: An imbalance between ice-philic and ice-phobic labels may affect accuracy, requiring SMOTE or class weighting.
Feature Relevance: Given protein structure complexity, techniques like feature importance and dimensionality reduction may be needed.
Prediction Challenges: Accurate prediction requires extensive data. If insufficient, we’ll focus on analyzing structural relationships with descriptive models.",Steven Su
Jason Su | jsu161@seas.upenn.edu ,Hua Huang | hua.huang@pennmedicine.upenn.edu,Xiaoying Zhang | zbzxy@upenn.edu,"Jason Su - project manager:
*planning and organization: Delegate tasks and assign roles to team members, Develop a clear timeline with milestones and deadlines
*coordination and communication: Ensure open communication between team members to keep everyone aligned on project objectives
*tracking and monitoring: Ensure the final report is complete, clearly organized, and ready for presentation or submission
*Documentation & Report:  Summarize key points and prepare any presentation materials

Hua Huang- researcher: 
*Hypothesis Formulation: Develop and refine hypotheses based on project goals. 
MLR Model Evaluation: Build an MLR model using the dataset and interpret coefficients to assess the relationship between features
*Model Selection and Training: Research and select suitable machine learning algorithms for fraud detection, such as logistic regression, decision trees, random forests, or gradient boosting

Xiaoying Zhang: data analyst: 
*Data Cleaning: Handle missing values, incorrect data entries, and outliers. Apply methods such as imputation, outlier removal, or normalization where necessary.
*Data Transformation: Convert data into usable formats to improve consistency and compatibility with analysis and modeling tools.
*Initial Data Analysis: Explore the cleaned dataset to identify distributions, trends, and relationships among variables. Focus on patterns related to fraudulent vs. non-fraudulent transactions.",https://www.kaggle.com/datasets/priyamchoksi/credit-card-transactions-dataset/data,"Row: 1,296,675, Column: 24","Objective:
The objective of this project is to create an effective system for detecting credit card fraud by leveraging data from a large dataset of real-world credit card transactions. With the growing reliance on digital payments, fraud detection has become increasingly important for financial institutions to prevent losses and protect customers from unauthorized transactions. By analyzing patterns in transaction data, our project aims to build a model that can reliably distinguish between legitimate and potentially fraudulent transactions, contributing to the improvement of fraud detection systems in the financial industry.
Value Proposition: 
Fraud detection is critical for financial institutions, as it directly impacts their ability to safeguard customers’ financial assets and mitigate revenue losses. With the increasing prevalence of digital transactions, the frequency and complexity of fraudulent activity are on the rise. An effective fraud detection system not only protects customers from unauthorized transactions but also enhances customer trust and loyalty, which are essential for the long-term success and reputation of financial organizations.
For financial institutions, timely fraud detection can prevent significant monetary losses due to chargebacks, fraud-related fees, and customer compensation. By intercepting fraud early, these institutions can also streamline the management of fraud cases, maintain operational continuity, and reduce costs associated with addressing fraudulent activities. Our project’s approach, which emphasizes accurate fraud detection, aims to build a model capable of adapting to the complexities of real-world fraud scenarios. Through effective modeling and the use of data-balancing techniques, we aim to develop a fraud detection system that benefits both the financial institution and its customers, ultimately enhancing transaction security and reducing fraud-related losses.
","Multiple Linear Regression (MLR) Hypothesis Testing
Hypothesis: All regression coefficients are zero.
Coefficients Tested: Transaction amount, age, gender, job, state, date, time.
Hypothesis: Observing an R2 score as high as the one we observed under the null hypothesis is unlikely.
Test Statistic: Observed R2
Simulation Plan:
1. Shuffle the fraud labels among transactions to create a null distribution.
Calculate the R2 score for each shuffle.
2. Run 10,000 simulations.
3. Calculate the p-value.
4. Decide whether to reject or fail to reject the null hypothesis.

Residual Test for Age as a Predictor
Hypothesis: Age has a significant effect on fraudulent transactions.
Coefficients Tested: Transaction amount, gender, job, state, date, time (excluding age).
Test Statistic: Observed R2 with residuals
Simulation Plan:
1. Predict fraudulent transactions without age, then calculate residuals as the difference between observed and predicted fraud values.
2. Shuffle residuals and add them to predicted values.
3. Run a linear regression on all variables (including age) and calculate R2.
4. Repeat 10,000 times.
5. Decide whether to reject or fail to reject the null hypothesis.

Modeling Plan
-   1. Base model: logistic regression
o   The logistic regression classifier model is an effective and widely used model for binary classification tasks, in our project, fraud detection. We will predict the likelihood of being identified as a fraud transaction.
o   We will apply the LogisticRegression classifier in sklearn weighs our imbalanced fraudulent transaction classes (is fraud or not), with specifying the ‘class_weight’ parameter as 'balanced' to deal with data imbalance and increase penalty for misclassified minority class
-   2. Random Forest:
o   The random forest is an ensemble method which builds multiple independent decision trees on a random subset of the data and features. It usually manages effectively non-linear relationships, high dimensional input features and complex patterns.
o   The algorithm is robust, especially to missing values and outliers, as well as robust to overfitting.
o   The random forest classifier can be less sensitive to imbalanced datasets by adjusting class weights and by resampling with emphasizing learning from minority classes (is fraud), by setting class_weight as 'balanced'.
o   We will tune the hyperparameters, such as ‘n_estimators’, ‘max_depth’, ‘min_samples_split’, ‘min_samples_leaf’ using RandomizedSearchCV, which explores the parameter space more quickly with limited number of iterations. We then can use GridSearchCV to search the parameters in a relatively small range.
-   3. XGBoost
o   XGBoost (eXtreme Gradient Boosting) is a powerful and efficient optimized implementation of gradient boosting algorithm based on decision trees.
o   It creates an ensemble of decision trees in a sequential manner, with each new tree focused on correcting the errors made by the previous trees.
o   The XGBoost algorithm includes regularization to make it robust and prevent overfitting. It also generally handles well large and complex data efficiently, including imbalanced and sparsity data (such our imbalanced fraudulent data).
o   Similar to the random forest, XGBoost provides a way to examine the importance of each feature within the model, making the model more interpretable.
o   We will tune the hyperparameters, such as number of trees ‘n_estimators’, learning rate ‘eta’, ‘max_depth’, ‘min_child_weight’, regularization parameter  ‘gamma’ using efficient RandomizedSearchCV, may combine with GridSearchCV.
-   4. Neural network  
o   We will also try to apply neural networks, autoencoder as unsupervised algorithm for fraud detection 
o   There are two main steps of autoencoder: encoding and decoding
o   We can model normal transaction patterns in the encoding step, and then identify outliers (potential fraud) by analyzing reconstruction errors as indicators in the decoding step. 
o   We will tune hyperparameters, including number of layers, activation function, learning rates and epochs, will also apply regularization to avoid overfitting  

Model evaluation metrics:
Accuracy 
Recall
Measure model’s ability to capture all actual fraud cases. High recall ensures most fraudulent cases are detected, reducing the risk of missed fraud. 
Precision
Ensure the flagged instances are true fraudulent cases. High precision minimizes false positives,which are costly in terms of time, money and customer experience. 
F1 Score 
Useful for evaluating performance when data is imbalanced data, it simultaneously considers both false positives and false negatives. 
AUC - ROC curve ","Here are some potential challenges we might encounter with our fraud detection project, based on the dataset size, imbalance, and project requirements:
Handling Imbalanced Data
The dataset’s extreme imbalance, with very few fraudulent transactions compared to legitimate ones, poses a significant challenge. Models trained on imbalanced data tend to focus on the majority class, leading to poor detection of fraud cases. Techniques like SMOTE, cost-sensitive learning, and class weighting adjustments can help address this, but they may also introduce noise or overfitting, requiring careful tuning to balance precision and recall effectively.
Computational Demands of Large-Scale Data
With over 1.85 million rows and 24 columns, the dataset is large and could require considerable memory and processing power, particularly when using complex algorithms like random forests, XGBoost, or neural networks. Training multiple models or performing hyperparameter tuning on this scale can be computationally expensive and time-consuming, potentially necessitating optimization techniques, more powerful hardware, or cloud-based solutions for efficient processing.
Feature Engineering and Data Quality
The dataset contains features such as transaction location, time, and merchant details, which are likely valuable for distinguishing between fraud and non-fraud cases. However, these features may require extensive preprocessing, such as converting Unix timestamps, handling missing values, and normalizing or encoding categorical data. Ensuring data quality by managing outliers and inconsistencies is also essential, as these could significantly affect model accuracy and robustness.
Avoiding Overfitting with Complex Models
While models like XGBoost and neural networks offer high accuracy, they are also prone to overfitting, especially on complex patterns or noise in the data. Regularization techniques and careful tuning will be essential to prevent the models from capturing random variations that don’t generalize well. Ensuring the model can perform effectively on unseen data (generalizability) is critical for real-world applications.
Class Label and Anomaly Overlap
Fraud detection often deals with nuanced cases where certain “normal” outliers could be mistakenly flagged as fraud. Differentiating genuine anomalies from fraud may require refined thresholds or supplementary techniques beyond binary classification, such as anomaly detection or clustering.
Ensuring Consistency Across Resampling and Validation
Techniques like SMOTE and cross-validation need careful coordination, as over- or under-sampling before splitting the data can lead to data leakage and overoptimistic performance estimates. Implementing these techniques consistently and correctly will be necessary to ensure the model’s true performance on unseen data.
Ensuring Robust Model Evaluation
With fraud detection, a high recall rate is crucial to minimizing missed fraud cases. However, high recall often lowers precision, leading to more false positives, which can frustrate customers and impose costs on the institution. Balancing evaluation metrics like recall, precision, and F1 score, and monitoring the AUC-ROC, will be key to achieving a model that is both effective and practical.",Steven Su
Pulkith Paruchuri | pulkith@wharton.upenn.edu,Eesh Trivedi | etrivedi@wharton.upenn.edu,Rishabh Wuppalapati | rishabhw@sas.upenn.edu,"Pulkith will be in charge of merging the main application data and past application data, and most of the data wrangling and cleaning steps to a usable format, and help with the rest of the EDA.

Eesh will be responsible for the rest of EDA and feature reduction (and row sampling) to a usable amount by the model.

Rishabh will build and test the various Machine Learning models, tune the hyperparameters, and contextualize the results.",https://www.kaggle.com/datasets/adityamishra0708/credit-eda-case-study-data?select=columns_description.csv,"1,978,000","We all find this project largely interesting, since we are all passionate about the finance industry, and think this would be something to further our understanding of both data science and finance. The main thing we hope to study with the project, is if there are any factors from both a single application, and a user's past applications that could predict with relatively high accuracy if a user is likely to default on their loan. A lot of countries have issues with things like non-performing assets, where users deliberately take out loans without the intention of repaying it, and while this issue is less prevalent in the US, it is still a  significant cause. Finding some factor, or series of factors that could predict the risk of a loan could lower national interest rates by reducing lost capital by banks.","We're mainly looking towards classification, perhaps classifying loan applications not only into 'approve'/'reject', but also perhaps into more granular levels of risk if possible. The target value is this level of risk (or just a binary approve/reject). We could perhaps also try regression on the expected (successful) payments, since it may be that there are some factors that lead to default after certain time landmarks after the loan is taken out. We're going to try a lot of different models, like decision trees, gradient boosted trees, etc... from class, and perhaps also clustering if possible. We don't really know what hypothesis we want to test yet, but perhaps during the EDA we might find certain interesting characteristics about some factors, and may want to try some hypothesis tests on those variables to see if they are actually causing some outsized impact.","The main obstacle we are expecting is having too many features / too large of a dataset. We have almost 2 million rows of raw data, which will likely grow after merging, which we need to reduce in a representatitve manner so we don't influence the data. We also need to reduce the features to prevent noisy features from confusing the models, an to reduce training time. As such, we need to figure out what features we want to keep and remove during our EDA.",Steven Su
Xiangxuan Yu | xiangxy@seas.upenn.edu,Yuchen Li | yli02@seas.upenn.edu,Linya Yuan |linyay@seas.upenn.edu,"Xiangxuan Yu: Proposal, EDA; Data Pre-processing; Implementation, assessment, and hyperparameter tuning of Logistic regression;

Yuchen Li : Proposal; Implementation, assessment, and hyperparameter tuning of Neural Network MLP Classifier; conclusion and discussion;

Linya Yuan: Proposal; Implementation, assessment, and hyperparameter tuning of Decision Tree; Introduction and Background;",The data set comes from kaggle (https://www.kaggle.com/datasets/qusaybtoush1990/transactions-data-bank-fraud-detection),1048316,"The objective of this project is to detect fraud of transactions. Moreover, we aim to compare the performance of three models based on several metrics.
The ultimate objective is to find out the best model to detect fraud of transactions and imply it in the future data set.

This project is interesting because it addresses the crucial task of fraud detection in financial transactions, helping to protect both customers and institutions. By analyzing transaction details, account balance changes, and transaction types, we can identify patterns that indicate fraud, ultimately improving security and prevention strategies in financial systems.","We plan to use machine learning models, and classification models specifically. Our target variable is “isFraud”, which takes a value of 0 or 1, indicating whether a transaction is fraudulent. The models we plan to use are: Logistic Regression, Decision Tree, and Neural Network or etc.. We plan to use accuracy, recall, F1 score, and ROC curve to choose the best model. 

We may not plan on test any hypothesis but we will try it in code by doing hypothesis testing using logistic regression to evaluate the importance of the feature “gender” in predicting outcomes. Our null hypothesis is that the coefficient of the feature “gender” in logistics regression is 0. Our test statistic is the accuracy of shuffled data minus the accuracy of original data. We plan to use simulations to test our hypothesis.","Fraudulent transactions typically represent a very small fraction of the total transactions, resulting in highly imbalanced data. This imbalance can cause the model to favor non-fraud predictions, increasing the risk of missing fraudulent cases. Addressing this imbalance is a major challenge.
Also, based on the limitations of computer computing power and time, it is difficult for us to 100% ensure that the hyperparameters we select are the best.",Steven Su
"Ana Bernal | anbernal@seas.upenn.edu

","Sofia Wawrzyniak | fiawawrz@seas.upenn.edu
",Mary Gerhardinger | maryge@sas.upenn.edu,"Cleaning and preprocessing: Mary 


Visualations:
Spatial analysis: Sofia
Class imbalance: Ana
Redshift: Mary 
Exploratory spectral bands within each category: Mary
Quasars, stars, galaxies


Hypothesis testing: Mary, Sofia, and Ana 


Tuning Models and Evaluating Performance:
Decision trees: Mary 
Ensemble models: Ana
Deep learning: Sofia
Neural network: Mary, Sofia, Ana 
",https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17,100k rows,"In this era of high precision cosmology, astrophysicists are gathering observations of our Universe at both a greater quantity and quality than ever before. This data can provide clues about the very evolution of our Universe as well as the objects contained within, like galaxy and star evolution which are open questions today. Therefore, it is imperative that we have highly accurate and precise methods for determining what sort of objects are being observed, so that they can inform new cosmological models. Additionally, as more telescopes are built with larger capabilities, the range of feature and parameter spaces grows. To that end, identifying the important features for classification will help reduce this high dimensional problem into a more manageable issue. Our goal is to create a classification system for classifying stellar observations into three different categories: galaxies, stars, and quasars. We also want to understand what features have the greatest impact on classification, what features are correlated with object class, and create a model to learn and classify various types of galaxies. ","Decision Trees: This will serve as the baseline model. Decision trees are well-suited for this problem due to their ability to capture nonlinear relationships and interactions among features. Given that we are not working on highly dimensional data, the model’s depth will remain manageable, reducing the risk of overfitting. Additionally, the interpretability of the trees will offer valuable insights into feature importance. 

Ensemble Models:
Random Forest: This model will likely improve classification accuracy through ensemble learning. It will also likely reduce variance associated with individual trees, making it more robust against overfitting. This robustness is especially beneficial to this problem given that stellar observational data can be relatively noisy. The light from celestial objects travels immense distances to reach the observer, which can result in interference and distortion of signals as they travel through space. 
XGBoost: This model could yield high accuracy by iteratively correcting errors of weak learners. It’s also relatively interpretable and could help provide insight into which features are most influential in the classification. Its built-in regularization techniques could also mitigate overfitting. 

Deep learning:
Multilayer Perceptron (MLP): For capturing nonlinear relationships in low-dimensional data. Regularization techniques, such as dropout, can help mitigate overfitting.
Autoencoders: For dimensionality reduction and anomaly detection, allowing us to capture nuanced variations in celestial data.

Neural Network: This model is better suited to capture more complex, nonlinear  patterns in the data. This will be especially important if the interactions between the features have complex relationships affecting classification – for example if spatial relationships are significant, a GNN could support spatial distribution of the celestial bodies. However, since neural networks typically work on higher dimensional problems, they will likely overfit in this classification problem. For this reason, we may consider techniques like drop out, regularization, early stopping, or we might conclude that this problem is better answered by simpler models. 

Ultimately, we may need to experiment with different approaches, starting with simpler, baseline models, and gradually introducing more complexity through higher capacity models, while assessing performance and overfitting through cross validation. 




Hypothesis test: determining if the average ‘redshift’ feature is different across stellar observation classes. We expect the average redshift of galaxies to be larger than the average redshift of stars, given galaxies are typically further from Earth because they were formed earlier in time. 
Null hypothesis: assume that there is no difference in the average redshift for galaxies vs stars vs quasars. 
Test statistic: average redshift shift = average redshift in galaxies - average redshift in stars (can extend to quasars too)
Simulations: assume the empirical distribution and shuffle the redshifts across ‘class’ types and calculate the test statistic in each simulation

The data in various spectra bands across the class of ‘stars’ is so varied because of the wide range of stars (from white dwarfs to red giants) that it is not possible to compare the other photometric features to one another easily across class types. This limits our ability to develop hypothesis testing around filter bands and classification.

","Class imbalance: With galaxies comprising about two-thirds of the dataset, stars and quasars are underrepresented. Balancing methods such as resampling, synthetic data generation, or weighting in classification models could be explored to handle this imbalance effectively.

Distance and redshift variance: Redshift is a critical indicator of distance in astronomical datasets, which is important for distinguishing between galaxies and quasars, as quasars are typically at higher redshifts. However, due to overlapping redshift ranges between some galaxies and quasars, further tuning and feature engineering will be essential to avoid misclassifications. The paper notes edge cases including void galaxies, edge galaxies, close mergers

Interference and light distortion: Light from these distant objects can be affected by cosmic dust and interstellar material, leading to data distortions. Techniques like preprocessing and dimensionality reduction (PCA or autoencoders) can help in mitigating the distortions before feeding data into models.

Unusual observations: Observations such as void galaxies, edge galaxies, or objects undergoing mergers might produce anomalies. Anomaly detection approaches like clustering or outlier detection with K-nearest neighbors or autoencoders can be explored to handle these.

Spatial constraints: Given the dataset’s basis in celestial coordinates, spatial positioning (right ascension and declination) might introduce bias when working with different regions of space. Cross-validation techniques like K-Fold, LOOCV, and broader spatial validation should incorporate spatial stratification to account for this spatial bias.
",Henry Sims
Yashvi Mehta | yashvim@seas.upenn.edu,Vinod Ghanchi | gvinod@seas.upenn.edu,Ashay Katre| ashayk@seas.upenn.edu,"Ashay Katre: Build baseline Linear and Random Forest models; evaluate with MAE, RMSE; analyze feature importance.
Vinod Ghanchi: Source, clean, preprocess Mercari data; create features; build baseline models, feature engineering.
Yashvi Mehta: Perform EDA, visualize insights, document findings; develop and tune advanced models (XGBoost, LightGBM); enhance features based on importance.",https://www.kaggle.com/c/mercari-price-suggestion-challenge/data,1482535,"This project aims to build a predictive model to estimate accurate selling prices for products on Mercari. By examining features like condition, category, and brand, it seeks to uncover valuable insights to support sellers in setting competitive and optimal prices. Beyond aiding individual sellers, the model enhances overall market efficiency, fostering fairer pricing practices that benefit consumers. Through detailed analysis, this project strives to bridge the gap between consumer expectations and market dynamics, creating a more balanced e-commerce landscape where sellers gain pricing precision and buyers experience more equitable pricing aligned with market realities.","Our project centers on predicting product prices on Mercari using regression models, ideal for the continuous target variable, price in USD. We’ll explore models including Linear Regression (baseline), Random Forest, XGBoost, LightGBM, and Support Vector Regression to capture complex feature relationships. Key features like item condition, category, and brand will guide model design. Additionally, hypothesis testing will assess the significance of item condition on price. Using ANOVA and t-tests, we’ll analyze feature influence, with simulations offering insights into price variation impacts. This approach aims to ensure robust, interpretable predictions for competitive pricing strategies.","Handling Missing Values: Address missing brand data through imputation or feature engineering to improve model accuracy.
Feature Engineering: Transform categorical features (e.g., item condition, category) using one-hot or target encoding for better interpretability and performance.
Overfitting Concerns: Mitigate overfitting with cross-validation and regularization for robust model generalization.
Data Distribution Anomalies: Manage price skewness with data transformations or robust regression techniques to enhance model accuracy.",Henry Sims
Jackie Liang | jliangus@sas.upenn.edu,Katherine Li | kfli@sas.upenn.edu,Connor Parker | Connor.Parker@pennmedicine.upenn.edu,"Katherine Li
Lead on exploratory data analysis
Assist ML modeling implementation
Generate final results and visuals

Jackie Liang
Project manager
Data collection and preparation
Assist ML modeling implementation
Prepare presentation slides

Connor Parker
Lead on data cleaning
Supervise the ML implementation
Lead ML documentation, tuning, and interpretation
","https://www.census.gov/data/developers/data-sets/cbp-zbp/cbp-api.html

https://data.census.gov/table/ACSDP5Y2022.DP05?q=All%205-digit%20ZIP%20Code%20Tabulation%20Areas%20within%20United%20States%20Populations%20and%20People",59000x20 rows by columns,"Our project aims to use machine learning to predict access to healthful food retailers at the zip code level, identifying areas as food deserts or food swamps. This is novel as the only other ML research uses census tract data, and was published in 2021. While food deserts represent regions with limited healthy food access, food swamps—where unhealthy options are prevalent—are more predictive of obesity rates, especially in urban areas. By focusing on zip codes rather than census tracts, our analysis can potentially be a free and public method of understanding food swamps using a common administrative geographic unit.","Our project builds on a 2021 study, incorporating spatial autocorrelation testing and potential spatial regression corrections. We use the modified Retail Food Environment Index (mRFEI) to classify areas as food deserts, food swamps, or with good food access. The model pipeline includes Random Forest for feature importance ranking, eXtreme Gradient Boost (XGB) with SHAP values for interpretability and feature reduction, and LASSO regression for final feature selection. Model performance will be assessed with accuracy, sensitivity, specificity, and NRMSE, validated by permutation tests.","Given the high number of dimensions we will start with (1000+), we anticipate the feature selection to be the most challenging. Additionally, we are concerned with multicollinearity of predictors that we will try to address using a training test split and other techniques. ",Henry Sims
Yifan Jia | yifanjia@seas.upenn.edu,Lilin Wang | lilinw@seas.upenn.edu,Haochen Lu | haochenl@seas.upenn.edu,"Lilin Wang: Data Pre-processing & Feature Engineering & EDA
Yifan Jia: Model Building & Result analysis
Haochen Lu: Hypothesis Testing & Challenges/limitations/potential future work
",https://www.kaggle.com/datasets/sdolezel/black-friday,"537,577","The primary objective of this project is to develop a predictive analytics framework to forecast new customer purchases during Black Friday using big data. By integrating part of big data's five V's—volume, velocity, value, and variety—we aim to create marketing strategies that drive revenue growth. Black Friday provides a unique chance to analyze vast and diverse consumer data in the real world. Predicting purchases enhances efficiency by revealing consumer trends, increasing sales, and boosting customer loyalty. Furthermore, the insights gained may spur further research in data science and machine learning within the industry and prompt important discussions on data privacy, potentially leading to new industry standards and regulations.
","Modeling:
Our project aims to predict users' purchase amounts based on background information and provide strategies to boost sales revenue, focusing on regression models with strong explainability. We will start with linear regression, incorporating regularization and feature mapping to improve test performance, and also explore models like random forest, XGBoost, and neural networks, tuning hyperparameters through Grid Search. Model performance will be evaluated using MSE and R-squared, with additional analysis on different value clusters to assess performance across segments.
Hypothesis Testing:
a. Testing Model Performance Across Different User Groups
b. Impact of gender on purchase amount
c. Impact of city type on purchase amount","Missing value processing: The data set may contain missing values, and if there is a large number of missing key features, it may reduce the predictive performance of the model.
Data noise: Due to the complexity of user behavior and market factors, there may be noise in the data (outliers) that can affect the predictive effect of the model.
Hyperparameter tuning difficulty: Due to the need to run multiple models (linear regression, random forest, XGBoost) on different user populations, and using Grid Search to tune hyperparameters can lead to increased training time, especially on our large data sets.
 
",Henry Sims
Hannah Lu | hannahlu@seas.upenn.edu,Abigail Harris | avharris@sas.upenn.edu,Sean McGinn | seankey@sas.upenn.edu,"Sean: Cleaning Data
Hannah: Linear Regression 
Abby: Presentation
We're all gonna help in each aspect of the project from the data preprocessing to analyzing our findings and making the presentation.",https://www.kaggle.com/datasets/diishasiing/revenue-for-cab-drivers,1000000 entries (roughly),"We want to study what variables affect the tip given for a taxi ride. Since tipping is completely optional, we hope this will illuminate some interesting psychological factors: If others are with you in the taxi are you more pressured to tip? Are people taking taxis late at night more or less likely to tip? Do certain vendors get less money in tips than others? This project is interesting to us because we are interested in studying human behavior in relation to a service. Furthermore, we live in a society where tipping culture has become very big and changed significantly since COVID, so our project is relevant to the current state of the world. ","We can start with multiple linear regression to establish a baseline, then explore polynomial regression if non-linearity is evident. Target variable is tip_amount, predicting based on columns trip_distance, passenger_count, length of trip, or other factors.","One of the big challenges we are foreseeing for our project is during the data cleaning step just due to the sheer size of our dataset. There are barely any Null entries in our dataset so we will have to decide a method on how to reduce our dataset size if needed that is more sophisticated. Furthermore, our dataset only includes data from rides in New York City. Therefore, our project results might not be predictive of tipping trends in other cities. Our dataset also doesn’t include information on rides taken using rideshare apps which are more popular than taxi hailing. To account for this factor, we are considering only using data entries from before rideshare apps become popular as a step in our data cleaning process. ",Henry Sims
Kalen Truong | kalentru@seas.upenn.edu,Linda Shen | lshen7@sas.upenn.edu,Charlie Wang | cwang8@sas.upenn.edu,"We will all contribute equally to the deliverables of this project. This will include generating ideas, splitting up the work equally during the EDA and Modeling portions of the project, and meeting weekly.",https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022/data?select=Combined_Flights_2018.csv,5.69M,"Potential challenges could be data quality, like missing values, or difficulties understanding the values for each variable. For example, variables like Div4WheelsOff require us to analyze what the data is trying to accomplish. 

Additionally, there could be collinearity within the dataset, so appropriate measures would need to be taken to address this to properly conclude which variables are most significant in contributing to flight delays. 

And lastly, omitted variables could cause bias in our predictions. This is because there are a variety of other factors that could contribute to potential flight delays, like weather, maintenance quality, airport congestion, city, security concerns, etc. that are not in the dataset. 
","We are considering both Classification and Regression models. We can use predictor variables to classify if a flight will be delayed or not and to predict how many minutes a flight will be delayed by. Decision trees and K-Means are possible models we will look into for classification, and we can use PCA to determine the most significant factors that contribute to flight delays and cancellations.

Our target variable will be delay in terms of both classification and prediction, using a variety of our features to predict departure and arrival delays. 

We could perform a variety of hypothesis tests. For example, our null hypothesis could assume that the average delay time t is the same across all airports. By grouping flights by airport and calculating each airport’s average delay, we can identify whether certain airports show statistically significant differences by comparing their average delay times against this threshold, identifying those consistently above or below the overall delay level.","Potential challenges could be data quality, like missing values, or difficulties understanding the values for each variable. For example, variables like Div4WheelsOff require us to analyze what the data is trying to accomplish. 

Additionally, there could be collinearity within the dataset, so appropriate measures would need to be taken to address this to properly conclude which variables are most significant in contributing to flight delays. 

And lastly, omitted variables could cause bias in our predictions. This is because there are a variety of other factors that could contribute to potential flight delays, like weather, maintenance quality, airport congestion, city, security concerns, etc. that are not in the dataset. 
",Hassan Rizwan
Chloe Ng | chloeng@sas.upenn.edu,Yanfu Ou | yanfuou2@seas.upenn.edu,Mike Zhou | mikezhou@seas.upenn.edu,"Chloe Ng, Duties: EDA & modeling, review data pre-processing/feature engineering, code quality, & final presentation 
Mike Zhou, Duties: EDA & modeling, review model implementation & model assessment/hyperparameter tuning
Yanfu Ou, Duties: EDA & modeling, review annotated notebook/sectional summary & model visualization",https://www.kaggle.com/datasets/joebeachcapital/airbnb,495k rows & 89 columns,"We intend to study the factors that impact the success of Airbnb rentals in different cities/neighborhoods in the United States. The objective is to draw some conclusions about which factors related to the host, the property, price, and city have the most impact on the popularity of the rental (as assessed through review scores and amount of availability).
This project is interesting because Airbnb is a very widely used marketplace for short term stays, with operations in over 190 countries. Understanding what factors create a successful Airbnb property can aid people who are planning to open or stay in an Airbnb, or who are interested in the dynamics of housing markets as a whole. The problem is also interesting because there are many potential factors to explore. For example, in the host’s background, we can consider variables such as months of experience as host, host response time, and number of listings per host, while for property factors we could consider property type, room type, amenities, and others. ","Type of Model: We plan on using a multilinear regression. Input variables include number of bedrooms, price per square foot compared to average price of rentals in the city, amenities, number of airbnb rentals in a city, availability of rentals, months of experience as host, host response time, number of listings per host, property type, room type.

Target variables: Average rating, Availability of the airbnb property 

We will be doing hypothesis testing. H0 = All coefficients are zero, there is no significant relationship between each of the input variables and the average rating of Airbnb listings
We can also test H0 where only one of the coefficients is zero, to identify the impact of individual variables and rank them from lowest to highest p-value.

Test statistic: mean of average ratings and mean of Airbnb availability 

We plan to use class topics to test these hypotheses by simulating the null world and testing the probability that the sample mean would be observed in this world. We would also run a multiple linear regression and bootstrap the dataset, by dividing the dataset into a control and a treatment group. We will then do sampling with replacement from the original dataset to get a series of data frames with replacement data, then run a multilinear regression, derive a set of coefficients, then pool those coefficients. 
","Some challenges and obstacles we anticipate are the need to clean the data and separate out the right variables. We will have to analyze qualitative factors quantitatively, which means taking factors like keywords and turning them into binary variables. There will also likely be a significant amount of confounding variables and important factors not captured by our dataset that may influence rating. Another challenge we anticipate is the challenge in converting between the different data formats. One of our datasets is in TSV format and we may need to write a script to convert that into the format we desired. ",Hassan Rizwan
Manav Parikh | manavp@sas.upenn.edu,Alisha Luthra  | aluthra@wharton.upenn.edu,Anushka Levaku | alevaku@seas.upenn.edu,"Manav Parikh
        - Data cleaning/wrangling, feature engineering
        - Ex. Adding a new variable “Time Gap” - difference between Appointment Day and 
            Scheduled Day
         - Help with ML model split
Alisha Luthra 
          - Machine Learning model building (training and testing), preparing presentation
Anushka Levaku
          - Hypothesis Testing, help with model assessment/hypertuning, preparing presentation
","https://www.kaggle.com/datasets/joniarroba/noshowappointments

- The dataset contains demographic information like each patient’s gender, age, neighborhood, attained education, and history of alcoholism. Clinical variables like hypertension and diabetes incidence were also collected. 
","110,527","We want to see which factors affect no-show rates to help optimize healthcare operations. By understanding which variables influence patient attendance, healthcare providers can proactively address issues that lead to missed appointments, improve patient care, and reduce the operational costs associated with no-shows. Our ultimate objective is to provide insights for healthcare providers to identify patients at a high risk of missing appointments and help them create intervention strategies to increase the likelihood of attendance. It’s interesting because it could help enhance resource utilization, provide insights into socio-economic factors that might impact healthcare, and improve health outcomes in underserved communities. ","Modeling

- 3 Models: Multi-linear regression model, Random Forest, XGBoost
- Target variable - “No Show”, Yes or No
- Machine Learning Prediction - Classification
         - Doing a train test split and seeing if the model can predict a given patient being a no-show status or not inputting other variables (i.e. classify patients by “no-show” yes or no)
Features that will be trained on: 
          - Demographic information
                   - Neighborhood, Scholarship Status, Age, Gender
          - Health factors
                   - Hypertension and Diabetes Incidence
          - Appointment Information
                   - Appointment Day and Time, Location

Hypothesis

- Yes, we are doing a hypothesis test. Our null hypothesis is that in the proportion of people who are no-shows, there is no difference between those who receive scholarships and those who do not. 
- We are considering doing a Z-test for the proportions of no-show rates between those who receive scholarship and those who do not.  The target variable would be the proportion of no-shows (use 0 for someone show-up and 1 for no-shows). ","Some challenges and obstacles we might encounter are:
      - Data quality is poor - patients might have recorded or falsified their socioeconomic status  yielding skewed results and an ML model that is trained with incorrect weights
      - Dataset outdated - dataset not accurate of current trends 
      - Machine learning models chosen (i.e. random forest, XGBoost) do not accurately predict on the training set and we will have to investigate another method
",Hassan Rizwan
"Fadel Batal | fadelbt@seas.upenn.edu
",Majd Ayyad | mayyad@seas.upenn.edu,Thomas Zeuthen | tzeuthen@wharton.upenn.edu,"Majd Ayyad: Exploring the data set/EDA and extracting relevant features
Fadel Batal : visualize data by plot different graphs and writing initial insights 
Thomas Zuethen: Testing different models and getting model accuracy",https://archive.ics.uci.edu/dataset/553/clickstream+data+for+online+shopping,"165,474 rows,  and 14 columns","We are looking to understand how different features affect online shopping. We are aiming  to build a report targeting marketing agencies to advise them on how to build more effective websites  increasing  their profit. Although the data set is focused on pregnant women clothing, we are expecting the model and the conclusions to be applicable to other online stores focusing on women clothing.
	Our objective is to understand how the display of a given product on an online shopping site will determine how many clicks it gets (given by the order variable). We will include variables such as price in USD, position on the page, color, how many pages you have to go to to get to the product, type of product, time of year, and others to see how these variables affect the number of clicks a product gets. ","The project focuses on predicting the number of clicks we will have on a product (given by the order variable), we are considering multiple machine learning models for regression including ridge regression, random forest, XGBoost, and SVM. Additionally we can consider deep learning techniques since the dataset is of a size where those techniques may be effective. 

We are currently thinking of these tests:

Null Hypothesis 1: There is no difference in the number of user clicks on a product among counties. The test statistic will be the normalized average number of clicks of  a product by country.
Null Hypothesis 2: There is no difference between average number of clicks on a product based on the month of the year. The test statistic will be the average number of clicks of a product separated by months of the year. 
Null Hypothesis 3: There is no difference between average number of clicks on a product based on the position in the page. The test statistic will be the average number of clicks of a product separated by position in the page. 

In order to test the hypothesis, we will perform the random sampling method we went over in class. ",Part of the issue could come in what model we should use since the number of clicks is an integer value but it is constrained (since it cannot be too large). Additionally we may have class imbalance in certain features such as country of origin or clothing type that might make prediction for these classes more difficult.,Hassan Rizwan
"Nimay Kumar | nimay512@seas.upenn.edu
","Kim Fung | kimfung@sas.upenn.edu
",Lee Ru Ye Laura | leeruye@wharton.upenn.edu,"Kim Fung - Data Wrangling and Final Slides Presentation. Data pre-processing and cleaning, formatting and combining tables. 
Nimay Kumar - Machine Learning model experimentation & hyperparameter tuning and Final Slides Presentation. Creating target variables and training various models, experimenting with param tuning.
Lee Ru Ye Laura -  Hypothesis Testing and Analysis of Data and Final Slides Presentation. Calculating test statistics and interpreting results of the hypothesis test.






",https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce,"100k rows, 52 columns total","The ultimate objective of our study is to train a model that can predict whether an order will be on time or not, given information about the order. In addition we plan to explore the distributions of order values across different geographic regions of Brazil. This project is interesting because it will potentially shed light on e-commerce purchasing trends in Brazil, and also demonstrate what variables, such as time of year, are more likely to affect delivery times. ","We are planning to build classification models that given an order and its features (products in order, seller, etc) will make a prediction whether the order will arrive after its predicted delivery date (late) or on-time or early. In olist_orders_dataset we have columns order_delivered_customer_date and order_estimate_delivery_date. From these we can construct our target variable delivered_on_time that will be 1 if the delivery was early or on-time, and 0 if it was delivered late. We suspect that decision trees will perform the best here, so we will of course be investigating Random Forest and XGBoost models. In addition we’d like to use Logistic Regression as our baseline, and potentially a neural network as a special model; the number of layers is something we could experiment with.

We have two null hypotheses that we are potentially interested in investigating. First, that there is no significant difference in on-time delivery rates across different geographic regions and states. Second, that distance between customer and seller has no significant effect on whether an order is delivered on-time. Of these two we believe that given our data the first hypothesis would be better to investigate. We plan to use an Analysis of Variance (ANOVA) test to compare the mean on-time delivery rates between different geographical groups and distances. 

To test this hypothesis, we will use the following concepts we have learnt in class, including hypothesis testing to compare geographic influences on order value, cross-validation to ensure robust model performance by testing on different data subsets, decision trees to classify customer reviews or predict product ratings, and ensemble methods for predictive modeling of customer behavior based on geographic or demographic features.
","We anticipate challenges in dealing with missing data, ensuring zip code prefixes are comparable (potential zip code inconsistencies), controlling for different population densities across regions, and normalizing different city populations across regions. This may impact order size and frequency.",Hassan Rizwan
"Zihan Wu | zihanw@seas.upenn.edu
",Yunqi Teng | yunqit@seas.upenn.edu,Junga Youn | norayoun@sas.upenn.edu,"All group members will participate in the discussions for each phase of the project. The
specific responsibilities for team member are as follows:
● Zihan (Hans) Wu：
○ Objectives, value propositions, and data explanations
○ Data wrangling and learnings from exploratory data analysis
● Yunqi (Alicia) Teng:
○ Modeling
○ Challenges, limitations, and potential future works
● Nora Youn:
○ Hypothesis tests
○ Implications & insights
","Our main source of data will be the US_flights_2023.csv dataset, which is a part of the US 2023 Civil Flights, delays, meteorology and aircrafts series of datasets on Kaggle. It is a table with columns as flight attributes and rows as individual flights. Flight attributes include information such as departure time, arrival airport, flight duration, and delay. The raw dataset contains 24 columns and 6.74 million rows. We will reduce this dataset (both rows and columns) for cleanliness and efficiency, focusing only on the Big 3 US Airlines – American, United, and Delta. We’ll also remove unnecessary columns such as “Tail_Number.”

We will consider merging this dataset with another one in the same collection called weather_meteo_by_airport.csv which provides the weather information for each airport in 2023. Weather attributes include information such as minimum temperature, maximum temperature, and total precipitation. We can merge the two datasets by date and airport. This will allow us to incorporate more independent variables that may affect our chosen target variable - the total delay in minutes. 

Link: https://www.kaggle.com/datasets/bordanova/2023-us-civil-flights-delay-meteo-and-aircraft/data?select=US_flights_2023.csv",620680,"Our main objective is to analyze the departure delays of the Big 3 US Airlines: United, American, and Delta. In addition to investigating the worst airline in terms of delays, we would like to investigate factors that are correlated with departure delays, such as flights leaving from a certain region, a flight of a certain length, and/or a flight under certain weather conditions. 

This project is interesting because we would like to know what airlines are the best choice for our flights. More specifically, the results can help us identify the best flight options (plane type, airline, day of week, weather, etc.) if we want to avoid delays. This is important especially since there is currently not a lot of specific information on which flights to choose in order to avoid delays.
","Our target variable is departure delay. As it is a continuous variable, we are considering regression models, including Multiple Linear Regression, Random Forest, and XGBoost. The Multiple Linear Regression model is straightforward and easy to interpret, so we choose to use it as a baseline model. We also plan to use a Random Forest model, which is robust against overfitting and can capture complex, non-linear relationships. Lastly, we will try XGBoost. While it provides excellent predictive power, it is more complex to tune and less interpretable than simpler models, requiring careful hyperparameter adjustment.

We plan to conduct hypothesis testing to further analyze our regression results. Specifically, when “Delays” is used as a dependent variable and “Airline,” “Dep_Airport,” “Manufacturer,” “Departure_Time,” “Flight_Duration,” and “Day_Of_Week” are used as independent variables, our null hypothesis would be: “the coefficients of these variables are zero.” We plan to shuffle these variables and perform 1,000,000 simulations.

More importantly, we want to investigate whether there is a statistically significant difference in delays among the Big 3 US airlines. As an example, our null hypothesis here would be that there is no difference in the average delays (in minutes) between American Airlines and United Airlines, and we will conduct 1,000,000 simulations to examine this. Given the large dataset, we plan to use vectorization to optimize processing speed.","1. Identifying Relevant Independent Variables: Determining which independent variables significantly impact our target variables will be a challenge. As there are so many features in this dataset, effective feature selection methods will be necessary to enhance model performance.
2. Handling Outliers and Large Variance: Preserving or appropriately managing outliers is crucial, as they can significantly affect the results of our analyses. In our project, the delay time can vary a lot. We will need to decide whether to retain, modify, or exclude these data points based on their influence on the model.
",Alan Wu
Yiting Li | ytingli@seas.upenn.edu,Kris Zhang | krisz@seas.upenn.edu,Jason Pan | jp2286@seas.upenn.edu,"Jason is responsible for EDA and feature engineering.
Kris is responsible for modeling, feature selection, and tuning. 
Yiting is responsible for visualization and model assessment. 
All of us will make the slides together.",https://www.kaggle.com/datasets/ruiqurm/lianjia/data,"279,107","We aim to predict housing prices in Beijing by analyzing various combinations of features such as longitude, latitude, floor, and number of rooms. 

Our ultimate objective is: 
1. to determine the most influential variables affecting house prices;
2. to compare different machine learning models to identify the most effective one for accurate predictions. 

The project is interesting because it provides insights into the dynamic real estate market in Beijing, helping stakeholders make informed decisions. Also, it explores the performance of multiple predictive models, showing the strengths as well as weakness in real world situation. ","We will use Regression Model. 
The target variable is price. 
We do not plan to test any hypothesis at this time. ","1. The dataset is pretty big, which might need longer time to proceed. 
2. Selecting the right features and tuning model parameters could be challenging due to the complex nature of the housing market.",Alan Wu
Soojin Lee | soojinl@upenn.edu,Nicole (Wanting) Li | wtl731@seas.upenn.edu ,Henry Corkran | hcorkran@seas.upenn.edu,"Shared duties:
Formulating final project research plan (searching for data, designing modeling, research hypothesis etc)
Preparing presentation and creating slides

Tentative work distribution (tentative)
Henry Corkran: Unsupervised learning (Clustering), Machine Learning
Wanting Li:  Variance Inflation Factor (VIF) Classification Model
Soojin Lee: Initial data cleaning , Hypothesis testing, MLR Residual analysis
","Centers for Disease Control and Prevention (CDC) Behavioral Risk Factor Surveillance System (BRFSS) - 2023：https://www.cdc.gov/brfss/annual_data/annual_2023.html

Introduction
The 2023 BRFSS data continue to reflect the changes initially made in 2011 for weighting methodology (raking) and adding cell-phone-only respondents. The aggregate BRFSS combined landline and cell phone data set is built from the landline and cell phone data submitted for 2023 and includes data from 48 states.

Dataset size
Total number of samples: 433,323 records
Variables: 30+ 
","433,323","Objective: 
We plan to analyze the CDC’s 2023 Behavioral Risk Factor Surveillance System (BRFSS) dataset to understand the influence of demographic, socio-economic, behavioral (lifestyle), and healthcare access factors on overall health outcomes. By modeling relationships between these variables and self-reported health levels, we seek to identify significant predictors of both positive and negative health outcomes. This will allow us to determine which categories (e.g., socioeconomic status vs. lifestyle) and specific factors (e.g., insurance status, physical activity, income level) most strongly impact self-reported health levels.

Value Proposition: 
Through developing and analyzing multiple models that control for various factors, we aim to draw actionable insights into the drivers of health outcomes. By identifying key socioeconomic and behavioral influences, we can support targeted strategies for improving health within specific communities and demographics. This tailored insight allows public health initiatives to go beyond one-size-fits-all recommendations, perhaps providing an opportunity to advocate for improved health outcomes for diverse communities that may have previously been underserved.  
","Modeling Plan: 

We will develop a classification model to categorize individuals into health levels (Excellent, Good, Fair, Poor) which is a calculated variable in the data set derived from self-reporting of overall health status and number of healthy days in the previous 30 days.  

Nearly all of the data in our dataset is either binary or categorical, which is well suited to a classification model.  Encoding and other preprocessing will be a significant step of the modeling process.

We anticipate using a Random Forest Classifier since there will likely be a lot of noise among the many features of the data set, and we need a model less prone to overfitting.  It would be great to discuss model selection with our TA, as our group does not have extensive experience with machine learning and may not be aware of better candidate models.  To make training more efficient, we’ll likely go through several iterations of our model, stripping away noisy or irrelevant variables that confound the results each time.  

Target Variable: The Combined Health Indicator (categories: Excellent, Good, Fair, Poor) derived from _RFHLTH (self-reported health status) 

In addition, we anticipate needing to do significant cleaning of the data and only work with a useful subset of the hundreds of features in the raw set.  We will use our judgment and preliminary model results to do this, focusing on the most interpretable variables such as those corresponding to specific chronic conditions, behaviors, socioeconomic circumstances, and demographics of the respondents.  

Hypothesis Testing: 
	
(Null Hypothesis)

Given the large number of variables in the BRFSS dataset, we first plan to use null hypothesis testing to filter out variables that we can ignore, focusing on those with the stronger statistical relationships.

(Example Hypothesis )
H0 (Access to Insurance): There is no statistically significant difference in self-reported health status between individuals who have access to insurance and have not. 

(H1): Individuals who have access to health insurance have significantly different health status compared to those who do not.

Once we have identified 7-10 features/variables we want to focus on, we plan to conduct Multiple Linear Regression (MLR) residual analysis to identify and control confounding variables. 
- Initial MLR model fitting (using 7-10 selected features)
- Conduct residual analysis 
- Identify potential confounders
- Re-evaluate predictors","Missing Data: 
The survey allowed participants to choose to skip certain questions (e.g “refuse to answer” options), leading to incomplete responses in some factors. These will be dropped and may significantly limit sample size. 

Inherent Bias in data:
Our target variable is self-reported health status, not a scientific or clinical measurement. This distinction is essential, as self-reported data introduces inherent biases and relies on individuals’ subjective interpretations of health. Such biases may affect the accuracy of our analysis and must be acknowledged in interpreting the results.

Multicollinearity: 
Many variables, such as age and chronic health conditions, may have high correlation, potentially impacting the interpretability of the mode. We plan to do VIF/multiple significance test to identify and regularize multicollinearity. 


Complex Interactions: 
Many health determinants, such as education, income, and health behaviors, are likely interdependent. Ensuring the robustness of our model will require exploring these complex interactions and validating feature importance during model selection.
",Alan Wu
Grace Liu | gyliu@seas.upenn.edu,Leah Ning | leahning@seas.upenn.edu,Yucheng Shao | yuchengs@seas.upenn.edu,"We will first collaboratively clean the dataset. This includes filtering out data based on relevance (ie. a lot of the columns related to information about the host is not necessary), ensuring data is in 1NF by exploding fields with multiple values, and verifying consistent data formats.
After this initial cleaning, we will each focus on our own model and tailor/clean the dataset to include only the relevant variables for that model. We will also each be testing a null hypothesis of our choosing based on correlations within the dataset (potential null hypotheses listed under 5).
Grace Liu - Random Forest
Null hypothesis: Location has no significant impact on Airbnb listing prices. 
Leah Ning - Multiple Linear Regression
Null hypothesis: Listing type has no significant impact on Airbnb listing prices
Yucheng Shao - XGBoost
Null hypothesis: Amenities provided have no significant impact on Airbnb listing prices
","https://www.kaggle.com/datasets/mysarahmadbhat/airbnb-listings-reviews

Note that we will only be using Listings.csv",265864,"Our objective is to analyze the key factors influencing Airbnb listing prices. By studying the relationship between various listing characteristics (e.g., location, amenities, and listing type) and price, we aim to develop predictive models that can accurately forecast listing prices. 
Our project is interesting since it can help Airbnb hosts competitively price their listings as well as offer insights for potential Airbnb guests to understand pricing trends and book the most suitable listing
","We will be using regression models to try to predict the listing price which is our target variable since it is continuous. 

We plan on using hypothesis testing. 

Null Hypotheses: features like location, listing type, amenities, etc. have no significant impact on Airbnb listing prices
We will use regression testing and permutation testing on the MLR model with R^2 as the test statistic and just permutation testing with the accuracy variable as the test statistic for the other 2 models. 

","Because our dataset contains more than 200,000 individual rows, we may run into issues when loading and cleaning the data. We may also run into issues involving processing speed when we run our machine learning models on the cleaned data. 
Airbnb pricing is quite dynamic, and can be affected greatly by variables outside of the ones we are considering like seasonality and short-term events. These changes won’t be accounted for in static historical data, so we could run into issues with the accuracy of predictions given that we cannot account for those variables.
Some features, like latitude/longitude and neighborhood/location/city will be highly correlated, which means that their multicollinearity may inaccurately inflate the variance of estimates and make it difficult to discern the effect of each individual variable. It may be the case that we only keep 1 geographical variable to avoid this.
",Alan Wu
Kaiyuan Wang | kaiywang@seas.upenn.edu,Yuqiao Su | yuqiaosu@seas.upenn.edu,Wangshu Hong | winshong@seas.upenn.edu,"Kaiyuan Wang: Model Assessment, Neural Network implementation 
Wangshu Hong: Data Pre-Processing, Decision tree and model tuning.
Yuqiao Su: Data cleaning, baseline model and Visualization
All together: Concept proving and Presentation
",The dataset we choose is the Airline Passenger Satisfaction dataset from Kaggle. https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction/data,"100,000 rows","Our project focuses on predicting airline passenger satisfaction based on certain variables, such as travel class (Business, Eco), food and drink, and so on. The ultimate objective is to help airlines identify the key factors that influence passenger satisfaction the most, allowing airlines to prioritize specific areas that can enhance the customer experience. This project is interesting because customer satisfaction is essential for airlines to provide better service so that more customers would prefer to choose those airlines. ","Models:
Logistic Regression on categorical variables: logistic regression can be a baseline model since it provides clearer interpretable results of the influence of each feature on the target variable (satisfaction), we can know how much each variable would affect the satisfaction  
Random Forests/Decision tree: capture non-linear relationships and provide more complex interactions among features which simpler models usually miss, and verify whether the accuracy is improved or not by using these models
Neural Network: since the dataset (100,000) is quite large and there are many features (25), complex feature interactions may be better explained by neural networks, while it would make the variable less interpretable  
Target variable: Satisfaction column, a categorical variable with three values, Satisfaction, neutral or dissatisfaction
","We can expect challenges from different aspects. 
One kind of obstacle is feature-related challenge, such as feature correlation. For example, there are ""On-board service,"" ""Seat comfort,"" and ""Inflight entertainment,"" which given the description of these columns might lead to multicollinearity. Meanwhile, some features such as ""Flight distance"" and ""Age,"" are on a different scale than satisfaction levels. Thus we anticipate that we need to do regularization and normalization to address these questions.

Another possible challenge is the meanings of categorical variables. For example, the column ""Customer Type,” which states whether customers are loyal or not, may greatly influence satisfaction. The same effect may apply to the Type of Travel (Personal Travel, Business Travel) and Class (Business, eco, eco plus), which may cause models to be biased towards certain situations

In conclusion, we anticipate obstacles in many aspects and we will do our best in data cleaning, model training, and tuning processes to eliminate potential biased outcomes.
",Alan Wu
Zixuan Cheng | zc888@seas.upenn.edu,Zhiyu Liu | liuzhiyu@seas.upenn.edu,Weicong Liu | weicongl@seas.upenn.edu,"Zixuan Cheng:  

  

Conducted exploratory data analysis, data preprocessing, and wrangling, ensuring alignment with data integrity standards 

Led development of both interim and final presentations, with a primary focus on data visualization and insightful interpretation 

Designed hypothesis tests through simulations to enhance the robustness of the project’s findings 

Validated the application of course concepts at the project’s outset 

Assisted in all other project areas 

  

  

Zhiyu Liu & Weicong Liu: 

Led the model selection and implementation model assessment and tuning  

Ensure the accuracy and readability of codes 

Designed hypothesis tests through simulations to enhance the robustness of the project’s findings 

Assis in all other project areas ","Tweets about the Top Companies from 2015 to 2020 

(https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020): 

contains over 3 million unique tweets with their information such as tweet id, author of the tweet, postdate, the text body of the tweet, and the number of comments, likes, and retweets of tweets matched with the related company. 

 

 

Self-created stock data table for the ""Magnificent Seven"" companies from 2015 to 2020, generated using the quantmod package in R. 

 

 

Fortune 500 Companies 1955-2021 

(https://www.kaggle.com/datasets/darinhawley/fortune-500-companies-19552021/data): 

Fortune magazine has published the top 500 companies in the US based on annual revenue. ",3000000,"Our project aims to explore the application of course topics, including data processing and machine learning algorithms, to forecast stock market trends. 

The primary objective is to uncover meaningful correlations between tweets and the stock prices of the ""Magnificent Seven"" companies in the U.S. market, providing investors with insights for more informed decision-making. 

Our project stands out by examining both quantitative factors—such as the number of comments, likes, and retweets—and qualitative factors, including the sentiment score of each tweet derived through NLP models, alongside stock metrics like price, volume, and P/E ratio.","Model for Filling Missing Values in Time Series Data 

 

For our first model, we aim to address missing values in our time series data. Given the temporal nature of the dataset and the presence of gaps, we plan to use imputation models like interpolation, ARIMA, or LSTM-based approaches. This model functions as a regression model since it predicts continuous values based on historical data patterns. The target variable here is the missing values within the time-dependent columns, which we aim to complete by leveraging these patterns. 

Model for Matching Variations in Key Names Across Tables 

 

The second model tackles the challenge of matching variations in entity names across data tables, where names may appear in different formats or abbreviations (e.g., ""apple inc"" vs. ""APL""). To solve this, we plan to employ NLP-based matching or entity resolution models, such as fuzzy matching, BERT embeddings, or Word2Vec, which help in identifying textual similarities. This is a classification task, as the model will determine whether two names are a “match” or “non-match,” with the target variable being the match status. 

 

 

Model for Sentiment Analysis of Comments 

 

Lastly, we are developing a sentiment analysis model to quantify the tone of comments, helping us gauge whether they are positive or negative. For this, we’re considering transformer-based models like BERT or RoBERTa, or traditional sentiment analysis methods that assign a sentiment score. This model functions as a regression, outputting a continuous sentiment score that ranges from -1 to 1. The target variable is the sentiment score, which captures the polarity of each comment, allowing us to assess its sentiment intensity. 

 ","Handle null and missing values systematically during data wrangling to maintain data quality. 

Generate a table in R that provides detailed stock data for the ""Magnificent Seven"" companies, aligned with the tweet data for the period 2015–2020. 

Apply NLP models to generate sentiment scores for each tweet, enabling quantitative analysis. 

Coordinate the integration of multiple machine learning algorithms to enhance predictive accuracy. 

Identify significant correlations between tweet metrics and stock market trends. ",Ming Cao
Shenyi Tang | tshenyi@seas.upenn.edu,Yuxiao Li | liyuxiao@seas.upenn.edu,Guocheng Wang | rilk@seas.upenn.edu,"We have three members in our team, Shenyi Tang, Yuxiao Li and Guocheng Wang. There are three models we are going to explore, each of us will be responsible for the whole process of one model. Yuxiao contributed to finding the dataset we want to use, while all of us will be participating in data processing. The breakdown	 duty of each team member will be:
Shenyi Tang: Data processing; Logistic regression code realization; Report on the result of Logistic regression, also Exploratory Data Analysis for the final report. She will elaborate the challenges and obstacles she encountered in her part as well.
Yuxiao Li: Data processing; Decision Tree and Random Forest code realization; Introduction part of the final report and corresponding presentation part; Challenges and obstacles she encountered in her part.
Guocheng Wang: Data Processing; Neural Network code realization; Potential Next Steps and Future Direction; Challenges and obstacles encountered in his part.
The final presentation will be done by the whole team.",https://wildlife.faa.gov/search,304854,"The objective of this project is to explore predictive modeling to determine whether an aircraft is likely to sustain damage given specific features of a wildlife strike incident. This insight can contribute to proactive measures within the aviation system, potentially enabling interventions when a high likelihood of damage is identified. The project thus holds practical value in enhancing safety by mitigating risks associated with bird strikes.

","We plan to develop a classification model where the target variable is to predict whether a bird strike event will cause damage to an aircraft, with 1 indicating damage and 0 indicating no damage. The primary models we intend to use for classification include:

Logistic Regression: To estimate the probability of damage occurring based on input features.
Decision Trees: To create a model that predicts damage through a series of decisions based on input variables.
Random Forest: An ensemble method using multiple decision trees to improve prediction accuracy and reduce overfitting.
Gradient Boosting: A technique that builds models sequentially, where each new model corrects the errors of the previous ones.
Neural Networks: To capture complex patterns in the data for improved classification performance.

We plan to conduct hypothesis testing to evaluate the impact of specific variables on aircraft damage. Our null hypothesis is that certain variables do not have an effect on whether an aircraft will be damaged. Under this null hypothesis, the probability of damage would be 0.5. We can use the proportion of aircraft damage as our test statistic and perform simulations to assess the validity of our hypotheses.
","1. There are over 100 variables, it can be difficult to screen out the ones that are really meaningful. 
2. More than 300,000 pieces of data with high dimension can be time consuming and computational complex.
3. Data processing can be hard, it can be tricky to handle the missing value for the dataset we use, several feature selection and dimension reduction methods may be needed to improve the efficiency of the models.
4. The quality of the dataset is not promised, oversampling or undersampling, or using a weighted loss function may be needed to resolve the imbalance in data.
5. It can be challenging to find out the best hyper-parameters for random forest and neural network.",Ming Cao
"Sophia (Ruxi) Xu | sophiaxu@seas.upenn.edu
",Jiani Wang | wjiani@seas.upenn.edu,Xinyi Cheng | xinyich@seas.upenn.edu,"EDA & Model Implementation (Xinyi Cheng)
Leads EDA to analyze variables and identify patterns in credit risk, creating interactive visualizations. Collaborates on implementing and tuning models like Boosting, aligning with project goals and documenting insights.

Labeling & Tuning (Sophia Xu)
Defines labels using vintage analysis, manages class imbalance, and engineers features. Leads hyperparameter tuning and visualizes performance for interpretability.

Preprocessing & Implementation (Jiani Wang)
Prepares data through scaling, encoding, and cleaning. Implements and tunes models, contributing to evaluation documentation and visual summaries of findings.",https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction,69.71 MB,"This project aims to build a machine learning model that predicts whether a credit card applicant poses a high risk of default. By predicting applicant risk accurately, this model can support banks and financial institutions in making informed lending decisions, optimizing risk management, and enhancing regulatory compliance. The project is valuable as it addresses the dynamic challenge of credit scoring, where newer predictive models can improve the resilience of financial institutions amid changing economic conditions.","Modeling Plan:
Model Types: We will explore classification models, for example, Logistic Regression (for interpretability and transparency), Boosting (for high predictive power), or Random Forest. Linear Regression will serve as a baseline due to its history in credit scoring, while Boosting and Random Forest can handle complex, non-linear relationships.
Target Variable: We will label applicants as “good” or “bad” clients based on their credit repayment history using vintage analysis on credit_record.csv. This involves analyzing the STATUS field across monthly records to define default risk profiles.


Hypothesis Testing:
We will conduct hypothesis testing to determine the impact of specific features (such as income, occupation, and family size) on credit risk.
Null Hypothesis (H0): Features such as income and occupation type have no significant impact on credit risk.
Alternative Hypothesis (H1): There is a significant impact of certain demographic and financial factors on credit risk.
Test Statistic: We’ll use chi-square tests for categorical variables like occupation and t-tests for numerical features such as income to identify key factors.","Label Definition: Defining “good” vs. “bad” clients from raw status data in the credit_record.csv table requires careful vintage analysis and may lead to ambiguity in some cases.
Data Imbalance: The “bad” client group is expected to be much smaller than the “good” client group, which could impact model accuracy and require resampling techniques.
Model Interpretability: Although models like Boosting and Random Forest may provide higher accuracy, they lack transparency, which may be an issue for explaining decisions to stakeholders.
Economic Sensitivity: Credit scoring models may be sensitive to external economic factors, which could affect generalizability, especially if economic conditions shift.",Ming Cao
Shashank Kambhatla | skamb@seas.upenn.edu,Eileen (Yilin) Feng | eilfeng@seas.upenn.edu,Siddhant Kapoor | kapsid@seas.upenn.edu,"Siddhant Kapoor: Data Sourcing, Model creation
Shashank Kambhatla: Data Cleaning, Hypothesis testing
Eileen Feng: Model Evaluation, Presentation",https://www.kaggle.com/datasets/aparnashastry/building-permit-applications-data?resource=download ,200000,"As we walk around the city’s bustling streets, we see scaffolding wraps around the building and sometimes even become symbols of the city. But what is exactly happening behind those projects—specifically, the permit approval process? 

For our final project, we plan to explore the predictive factors that determine whether a permit will be approved and what factors determine the costs of a proposed project. For example, understanding how factors like project costs, zoning regulations, or building types influence outcomes. Based on the dataset provided, which includes key variables such as permit types, costs, zoning information, construction details, and neighborhood data, this project will study various aspects of building permits to identify the most significant factors affecting the approval status and cost estimations. The ultimate objective is to provide insights that can help a real estate company better navigate the complexities of the permit process, improving their chances of success. This topic is interesting to us because it’s like uncovering the hidden mechanics behind urban development, turning complex data into practical strategies that can reshape how projects take shape in the city.
","There are two aspects to this project - we need to investigate what factors determine the permit approval process + what factors determine the cost of a proposed project. 

To find out more about the factors that determine permit approval and in what ways they affect the permit approval, we will be building a binary classifier that predicts whether a permit will be approved or not based on selected features. We will be experimenting with linear regression models as well as decision tree models (Random Forest, XG Boost etc) to see what best fits our data and use a train-test split of our dataset to build the classifier using the above-mentioned models.

To find out more about the ways in which certain factors determine the cost of a proposed project, we will be building a multivariate regression model that predicts the cost of a project. We will be experimenting with regression models discussed in lectures. We will also be exploring individual relationships between each factor and the cost of a project, for which we will use simple linear regression to explore each individual relationship. 

Our plan for hypothesis testing is to conduct tests like that in lecture 10. For our Linear regression models, we will be performing tests with the null hypothesis that all coefficients are 0, to demonstrate that there is a significant relationship between the different variables and permit approval or project cost. We will also identify certain features of interest and perform hypothesis tests on these where the null hypothesis is that a particular feature has a coefficient of 0. This is to show the significance of a particular feature in determining permit approval or project cost. 
",The biggest challenge would be to select key features in both aspects of the project. The dataset has over 20 features (columns) so we will need to do some feature engineering to decide which are the most impactful features in each aspect.,Ming Cao
Anbang Chen | frdchen@upenn.edu,Yushan Yan | yyan07@seas.upenn.edu,Zihan Wang | wangz2@seas.upenn.edu,"Anbang Chen: Data Collection, Data Aggregation, Feature Engineering
Yushan Yan: Feature Engineering, Model Training
Zihan Wang: Model Evaluation, Model Comparisons, Reflections","Iowa Liquor Sales: This data is publicly available through Iowa Open Data. The original data has 30.1m rows in total, using a random seed: 1817 sampled 1.5m rows for the project. https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data

Iowa Weather Data: Data from the Iowa Environmental Mesonet cover daily weather data (temperature, precipitation, snow, etc.)
https://mesonet.agron.iastate.edu/request/daily.phtml

Iowa ASOS Weather Station Info: Data for joining location between weather and sales data.
https://mesonet.agron.iastate.edu/sites/networks.php
",1500000,"Predicting Alcohol Sales in Response to Weather Conditions
Main Question: Can we predict liquor sales by category based on weather patterns?
Past research has shown that weather can influence consumer behavior, including the purchase of various goods. In this study, we are particularly interested in understanding how liquor sales fluctuate under different weather conditions—a topic that moves beyond traditional economic models and explores the impact of external environmental factors.","We want to use regression models to predict liquor sales. We aim to use OLS, tree-based models, and possibly neural networks. The target variable is liquor sales in Iowa by city. We will consider conducting hypothesis tests on whether weather type (sunny, rainy, etc.) impacts liquor consumption. Depending on the weather details, we will conduct different tests. One possible null hypothesis is that sales would be higher on sunny days than on rainy days. We will perform simulations to test the hypotheses.","The dataset is large, and we may have challenges cleaning/extracting useful datasets. 
We need to align two datasets together (sales and weather) and use graph algorithms to match the counties/liquor shops with the nearest weather stations. 
The current liquor category’s scope is too small, so we will try to use NLP to group them into larger groups.",Ming Cao
"Jiayu Zhang | zjiayu@seas.upenn.edu
",Jeslyn Guo | jeslyng@wharton.upenn.edu,"Rachel Doman | rdoman@wharton.upenn.edu
","Jiayu Zhang: In charge of data cleaning, exploratory data analysis (EDA), and initial data wrangling.
Jeslyn Guo: In charge of model development and testing using initial data, including building baseline and advanced models.
Rachel Doman: In charge of hypothesis testing, statistical analysis, and report generation. Will also manage the final presentation and deliverable formatting.",https://www.kaggle.com/datasets/thedevastator/imdb-movie-and-crew-data,45.59 MB,"We intend to study the features and performance of movies. The objective of this project is to predict movie popularity. We will do so by examining factors such as genre, runtime, and the influence of directors, writers, or actors. We find understanding these relationships to be interesting as they can provide insights for us as consumers to evaluate various factors when deciding whether to see a new movie, as well as to understand why a movie has garnered popularity or started trending. Moreover, this project could help production studios forecast box office performance and optimize marketing strategies, creating a valuable tool in entertainment analytics.","We’ll use a multiple regression linear model to determine how each feature affects the target variable. Our target variable is popularity, which we’ll define based on IMDb ratings or a score derived by revenue or ratings data. We’ll test two hypothesis: 

(1) Genre Impact Hypothesis: Certain genres are more popular than others. Null Hypothesis: Genre has no significant effect on movie popularity. Test Statistic: Test to assess if genre distributions differ significantly between popular and non-popular movies.

(2) Director/Writer Influence Hypothesis: Well-known directors or writers correlate with higher popularity. Null Hypothesis: Crew reputation has no impact on movie popularity. Test Statistic: Regression analysis to see if crew reputation and previous movie involvement predicts popularity.","Data cleaning/handling missing values: Since we’re comparing all movies on a like-for-like basis, we’ll need to carefully drop any movies where there are null/indeterminate values and avoid extreme outliers. 

Quantifying “success” of a movie: movie success can be subjective, depending on which variable we look at it from. We’ll have to find a combination of these metrics. ",Bekzat Amirbay
Drew Krumenacker | drewkrum@sas.upenn.edu,Matthew Kulubya | mak23@wharton.upenn.edu,Max Mercado | maxmerc@seas.upenn.edu,"Drew - EDA / data cleaning
Max - Modeling and hypothesis testing, Group leader organizing meetings
Matthew - Modeling and hypothesis testing, presentation lead.
Everyone - responsible for making comments and annotations to collab and markdown files when they make a change, Creating a presentation and rehearsing their part of the presentation.
These are loose outlines of what roles we plan to have",https://www.kaggle.com/datasets/stefanoleone992/ea-sports-fc-24-complete-player-dataset?select=male_players.csv,"Pre-cleaning: roughly 180,000 rows by 131 columns. Post cleaning: 65,000 rows by 40 columns","Our project aims to analyze FIFA player ratings to determine which attributes most significantly contribute to these ratings, identifying potential patterns and insights that link game data to real-world factors. We’ll investigate correlations between FIFA ratings and players' market values, performance metrics, and other real-world data to assess if in-game ratings align with actual player abilities and market worth. This project is interesting because it bridges virtual representations with real-world dynamics, revealing how accurately FIFA captures player performance and market trends, and it may provide insights for gamers, analysts, and football enthusiasts interested in the intersection of data, sports, and gaming.","
We plan to use both classification and regression models. For classification, we aim to group players by their positions to understand how positional roles influence different attributes and ratings. For regression, our target variable will be the overall player rating, as we want to predict a player’s rating based on individual attributes (e.g., pace, shooting, passing) and determine the impact of these features on overall ratings. Additionally, we’ll test hypotheses such as whether higher ratings correlate with higher real-world market values, if certain attributes are more predictive for specific positions, and whether there are any significant deviations in ratings accuracy across player demographics.","Key challenges in our project may include handling null values within the dataset, which could affect model accuracy if not addressed properly. Additionally, goalkeeper data presents a unique challenge, as their core attributes differ significantly from other positions, potentially requiring separate modeling or feature engineering to ensure accurate predictions across all player roles.",Bekzat Amirbay
Sihan Yan | sihanyan@seas.upenn.edu,Junyang Wang | jyw3@seas.upenn.edu,Mengyue Huang | mengyue4@seas.upenn.edu,"Sihan Yan: Data Pre-processing and Cleaning, Exploratory Data Analysis (EDA)

Mengyue Huang: Model Development and Evaluation

Junyang Wang: Hypothesis Testing, Statistical Analysis, and Documentation
",https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand,102894 × 31,"Primary Objective: This project aims to investigate patterns within hotel booking data that affect some key performance metrics such as cancellation rates, lead time, and special requests. Specifically, we aim to build predictive models that estimate the likelihood of cancellations and variations in average daily rates, enabling hotels to optimize operational decisions.

Value Proposition: Findings can help hotels identify high-risk, optimize room pricing strategies by analyzing ADR (Average Daily Rates) patterns, and explore peak demand periods on rooms. The predictions and insights can further support decision-making on overbooking policies, special offer timing, and marketing strategies based on customer behavior trends.","Classification Task: Predicting is_canceled (whether a booking is canceled).
Regression Task: Predicting adr (ADR: average daily rate).
Baseline Models: Logistic regression for cancellation prediction (classification) and linear regression for ADR (regression).
Advanced Models: XGBoost, Gradient Boosting Machines/LightGBM to improve accuracy and capture non-linear relationships in the data.

Hypothesis 1: Higher lead time results in a higher cancellation rate.
(correlation test / ANOVA)
Hypothesis 2: Bookings made through different channels (e.g., online travel agencies vs. direct) have significantly different cancellation rates.
(Chi-square tests /ANOVA)","Data Imbalance: We anticipate class imbalance in the is_canceled variable, as cancels may naturally be fewer than not. To address this, we may use techniques like SMOTE or class weighting.

Modeling Efficiency: Given the dataset size (119k rows and 32 columns), some advanced models may require considerable computational resource. Downsizing features with data pre-processing will be necessary to avoid unaffordable runtime.",Bekzat Amirbay
Zachery Lim | zlim3@wharton.upenn.edu,Immanuel Anaborne | anaborne@seas.upenn.edu,Samuel Tausner | stausner@sas.upenn.edu,"We intend to split up each step of this process evenly, meaning we will work together during the EDA, pre-processing, model implementation, and the presentation. As the project progresses, we will likely become more specialized in the types tasks we delegate to each other. ","https://www.kaggle.com/competitions/nfl-big-data-bowl-2024

Note in the NFL competition's data set legal terms and conditions: 
“A. Data Access and Use. You may access and use the Competition Data for non-commercial purposes only, including for participating in the Competition and on Kaggle.com forums, and for academic research and education.”

","roughly 350,000 rows in player_play.csv",We intend to study if it is possible to use pre-snap information from NFL games to predict plays and results on the offensive and defensive end. The ultimate objective is to gain insight beyond intuition into how plays will unfold. As football fans this is an interesting opportunity to view and understand the game from a new lens. ,"Our primary focus is to analyze play-by-play data with classification models, targeting features like receiver alignment, defensive formation, previous play attempts, etc. We might also look to do hypothesis testing, potentially looking to see if some characteristics of a given player have a statistically significant impact on their decision to run or pass the ball.","This is quite a large dataset, so it might take us some time to narrow down on what specific features we would like to filter on for our prediction models. ",Bekzat Amirbay
Yuntian Gan | ganyt@seas.upenn.edu,"Yiqi Qiu | yiqiqiu@seas.upenn.edu
",Kexin Wang | kexinw5@seas.upenn.edu,"Yuntian Gan: Modeling Training, Tuning 
Yiqi Qiu: Feature Engineering, Hypothesis test
Kexin Wang: Data Cleaning & Preprocessing (encoding)  ",https://www.meps.ahrq.gov/mepsweb/index.jsp,900000,"The objective of this research is to look for cases where some patient sets (similar condition and patient identity) are using different drugs before and after COVID-19. This research considers the potential impact of COVID-19 pandemic on the attitude of medical providers of patients, focusing on the possibility that human attitudes toward health and preventive care may have evolved, and impact treatment choice as objective factors. Treatments are chosen by medical providers and the patients, based on their human decision. Knowing how objective factors (social context, mentality etc.) influence medical decisions can help the field of science and the governments better design research and drug management policy, thereby ensuring the safety and efficacy against the odds of scientific unknowns. ","This research’s method would use a classification model (random forest) to map condition information to drugs used for each condition. The target variable is drug prescribed for each medical event. The null hypothesis would be: similar medical cases (similar patient identity and condition) should be receiving consistent treatment. 
The test statistic is the difference between model error (the predicted drug is different from actual drug prescription). A significant difference in prediction errors may indicate that the treatment patterns have changed between the two periods.","The major challenge of this research is the complexity of both model output and input features. 
The condition will be used as an input feature, but it is a nominal feature with a huge number of potential values. So a correlation clustering will be applied to transform the nominal conditions into a 1D numerical space: disease frequently occurring together will be mapped closer together  numerically, vice versa. 
Each person might receive multiple drugs for some conditions. To reduce the complexity, ensemble based multi-label methods RAkEL will be used to group frequently co-prescribed drugs together. A sub-model will be trained on each drug group and predict multiple drugs prescribed jointly. ",Bekzat Amirbay
Anant Aggarwal | anant24@seas.upenn.edu,Pradipta Kinasih | pkinasih@seas.upenn.edu,Ankita Diwan | ankita9@seas.upenn.edu,"Anant Aggarwal: Data cleaning, data preprocessing, and hypothesis testing.

Pradipta Kinship: Exploratory Data Analysis (EDA), feature engineering, and presentation slides.

Ankita Diwan: Modeling, hyperparameter tuning, creating annotated notebook, and presentation recording.",https://www.kaggle.com/datasets/gauthamp10/google-playstore-apps,2300000,"The main objective is to analyze factors influencing app popularity. We aim to identify the features that correlate with higher ratings and download counts, providing insights that could be valuable to developers and marketers in optimizing their app strategies. For developers, understanding which features impact app success can guide design and content improvements based on user preferences. For marketers, the project helps them optimize promotional strategies and target the right audience. Additionally, our project aims to predict app success based on attributes like developer reputation, app updates, and user reviews, allowing for strategic improvements for every app release.","We plan to use both classification and regression models for this analysis. Specifically, we will start with linear regression to predict app ratings and installation counts as our baseline model. We will also use classification models like Logistic Regression and Random Forest to classify apps based on their popularity (e.g., high vs. low downloads). Additionally, ensemble models such as Gradient Boosting and XGBoost will be explored to improve predictive performance. By comparing these models, we will identify which features contribute most to app success and achieve the best model performance. The target variables for our regression models are app ratings and installation counts, while for classification, the target variable will be app popularity categories based on installation counts. The modeling will include hyperparameter tuning and cross-validation to optimize model performance.

We are considering several hypothesis tests to explore relationships within the data. ● Effect of App Type on Ratings: We will test whether the type of app (free vs. paid) has a significant effect on average ratings. The null hypothesis (H0) is that the type of app does not significantly affect the average rating, while the alternative hypothesis (H1) is that there is a significant difference. We will use a t-test to assess statistical significance. ● Effect of App Category on Download Counts: We will conduct an ANOVA test to determine if there are statistically significant differences in download counts across different app categories. The null hypothesis (H0) is that all app categories have similar download counts, while the alternative hypothesis (H1) is that at least one category differs. ● Simulation-Based Testing: We will use simulation techniques to validate our test statistics, similar to methods discussed in class. We’re considering generating a null distribution by randomizing app types and comparing the observed test statistic to this distribution. This will allow us to estimate empirical p-values and determine the likelihood of observing the effect by chance. ● Chi-Square Test for Independence: We will also use a chi-square test to determine if there is an association between app category and app type (free vs. paid). The null hypothesis (H0) is that there is no association between these variables, while the alternative hypothesis (H1) is that there is a significant association","● DataQuality: Handling missing or inconsistent data, especially in user reviews, might be challenging and could affect model accuracy. Data imputation techniques will need to be carefully chosen. ● Imbalanced Data: The target variables, such as app popularity, may be imbalanced, with a large number of apps having low ratings or few downloads. We plan to address this using techniques like SMOTE (Synthetic Minority Over-sampling Technique) or downsampling. ● ModelComplexity: Balancing model interpretability with performance may be difficult. More complex models like XGBoost may perform better but will be harder to interpret compared to simpler models like linear regression. We will need to consider trade-offs when selecting our final models. ● Computational Efficiency: With over 2 million rows, traditional single-machine processing might be inefficient. We might consider distributing the data across multiple processes to increase efficiency.",Jon Wu
Allan Zhang | allzhang@upenn.edu,Alan Wu | alanlwu@upenn.edu,Cameron Shaw | cshaw1@upenn.edu,"We will initially divide the work of preprocessing, feature engineering, and creating a baseline model evenly between us. We will then each choose a different type of more advanced model and tune its hyperparameters. We will work on the EDA and presentation together. We will each take a section of the final annotated notebook to create and combine it together at the end.",https://www.kaggle.com/competitions/nyc-taxi-trip-duration/data,"1,458,644 rows x 11 columns","We plan to study how the duration of taxi rides can be predicted from the other features in the dataset, such as the pickup time, pickup location, and dropoff location. This information is all known before the trip begins, so being able to predict how long the journey will take would be quite useful. Taxi companies and drivers can better optimize their fleets as well as plan the routes they want to take that day to maximize fares. From the passenger’s perspective, trip time is useful for providing an estimate of how much the journey will cost, since taxi fares are calculated using the ride distance and time. Analyzing taxi trip data will be quite interesting at a higher level since we can see how much each feature impacts ride duration and draw larger conclusions about high-congestion areas and peak traffic times.","Since the target variable (trip_duration, duration of the trip in seconds) is continuous, we plan to use regression models. Our baseline model could be a multiple linear regression (unregularized or regularized, e.g. LASSO and Ridge) or a decision tree. We also plan to use more advanced models, particularly ensemble models like Random Forest and XGBoost. Time permitting, we could also try deep learning techniques like neural networks.

We do not plan on testing any hypotheses.","If we choose to include inter-borough taxi rides it could be challenging to deal with the fact that routes will most likely be less direct since they will require going over a bridge or through a tunnel, so the distance and thus time traveled could be quite different to the trip distance in a single borough.",Jon Wu
Victor Wang | vwang04@sas.upenn.edu,Boris Chen | chenbor@wharton.upenn.edu,Nicholas Wu | nickwu@seas.upenn.edu,"Victor Wang
Victor will focus on data collection and cleaning, specifically with regards to the reviews data. He will also lead the initial exploratory data analysis (EDA), identifying patterns and trends within the larger dataset. He will also help create the final project report in the slidedeck format, taking charge of visualizations and other factors for the presentation.

Nicholas Wu
Nicholas will lead the model development and hypothesis testing. He will also focus on selecting which tables and location from the sources will be used. He was responsible for finding the initial dataset as well, and will build the regression models to predict pricing and classification. He will also contribute to model interpretation and help with selecting relevant features to avoid overfitting.

Boris Chen
Boris will analyze the neighborhood data and other listing information to examine the impact of outside factors on booking frequency and price. He will also be responsible for making sure that the final notebook is presentable and readable for the TA, and focus on communicating insights effectively. He will also help with the hypothesis testing and writing the interpretations of the various models.
",https://insideairbnb.com/get-the-data/ ,"94,000+ rows","Our project explores factors that influence Airbnb pricing and guest ratings. We aim to identify key features, such as neighborhood, listing type, host status, and availability, that correlate with higher pricing and booking frequency. By analyzing review scores and comments, we also aim to understand the impact of guest experience on host success and price. This project holds practical value for potential hosts looking to optimize their listings and for travelers seeking cost-effective options. Moreover, insights from this analysis could help local policymakers understand the short-term rental market's influence on housing affordability.
","Our primary model will use regression analysis to predict listing prices, with features like neighborhood, property type, availability, minimum days, etc. For classification, we may group listings into tiers (e.g., budget, mid-range, premium) to segment price ranges. Hypotheses include “Superhost status correlates with higher prices” and “Properties with higher review scores have greater booking frequency.” Cross-validation and feature engineering will be applied to improve model accuracy.
","Challenges include handling missing values in review scores and diverse formats for pricing data. Geographic and seasonal variations could affect pricing, requiring careful interpretation. Additionally, high-dimensionality may demand feature selection techniques to improve model interpretability and avoid overfitting.
",Jon Wu
Dai Wang |dwan@seas.upenn.edu,Changbang Li | lcb0105@seas.upenn.edu,Bowen Zhang |zboen@seas.upenn.edu ,Bowen Zhang will be responsible for introduction and EDA and Changbang Li will be responsible for modeling. Dai Wang will be responsible for description of challenges and future direction.,"https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease
",320000,"In this project, we aim to study risk prediction for heart disease. Our goal is to build an accurate heart disease prediction model by analyzing a range of health and lifestyle factors (such as BMI, smoking, alcohol consumption, etc.). The ultimate goal of the project is to help identify high-risk groups so that preventive intervention can occur at an early stage and reduce the chance of heart disease.This research is meanful because heart disease is one of the leading causes of death and disability worldwide, and early prediction can significantly reduce medical costs and improve patients' quality of life. ","In this project we will use a classification model focusing on predicting the presence of heart disease (our target variable: ""HeartDisease""). The main models considered include logistic regression, random forests, and decision trees. We will test hypotheses such as whether smoking and BMI significantly affect heart disease risk. Hypothesis testing will include chi-square tests for categorical variables (such as smoking) and t-tests for continuous variables (such as BMI) to provide in-depth analysis of key health factors that influence heart disease.","We might meet challenges such as potential data imbalance for heart disease cases, which can affect model accuracy. Additionally, selecting relevant features to prevent overfitting and finding optimal hyperparameters can require a lot of experimentation. Finally, balancing interpretability and accuracy when selecting a model is also a challenge.",Jon Wu
"Dylan Marchlinski | dmarchli@sas.upenn.edu
",Jake Murphy | jakemur@sas.upenn.edu,Molly Bradley | mollykb@sas.upenn.edu,"Dylan:
Data cleaning (main dataset)
Regression model (population growth)
Geographic visualizations of the data
	
Jake
Data cleaning (QoL datasets; health)
Regression model (health)
Graph visualizations of data

Molly
Data cleaning (QoL datasets; crime)
Regression model (crime)
","Main Data (mortgages):
https://www.consumerfinance.gov/data-research/hmda/historic-data/

Supplementary Data (will gather more as needed):
Crime rates by county in the US in 2016
https://www.kaggle.com/datasets/mikejohnsonjr/united-states-crime-rates-by-county?resource=download
",1048576,"In this project, we aim to use national data on mortgages from the Consumer Financial Protection Bureau to explore the effects of housing prices on quality of life metrics. Specifically, we are interested in understanding if changes in housing prices over time correspond to changes in QoL variables of interests and population growth. We will then use the results of this analysis to construct a predictive model for 1. future housing prices and 2. how those future predictions will affect QoL variables. We think this project will be interesting as we believe housing prices are correlated with many of the target variables, but also know there are some (like population) that are not perfectly correlated.","How do changes in housing prices influence various QoL metrics (crime rate, health indicators, etc.). 

Comparing how housing price per county has grown compared to national average. 

Regression problem which compares housing price increase with the QoL variables, plus a predictive model which extrapolates price increase per county over next 5-10 years.

Our hypothesis tests will be if our regressions are statistically significant or not.",We could lose information aggregating at the county level. Another challenge could be compiling data from various sources with differing years or metrics. ,Jon Wu
